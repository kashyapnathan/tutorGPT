{"prompt":"05_section.txt\n\n###\n\n","completion":" –1–\nCS109 May9,2024\nConditional Expectation, Introductory Inference\nBeforeyouleavelab,makesureyouclickheresothatyou’remarkedashavingattendedthis\nweek’ssection.TheCAleadingyourdiscussionsectioncanenterthepasswordneededonce\nyou’vesubmitted.\nWarm-ups\n1 Why Multiple Random Variables?\nWhatisaprobabilisticmodelwithmultiplerandomvariables?Whatdoestheterminference\nmean?Whatdoyoucalltheprobabilityofanassignmenttoallvariablesinaprobabilisticmodel?\nWhyisthatuseful?Whycanitbehardtorepresent?\n2 Joint Random Variables Statistics\nTrueorFalse?Thesymbol𝐶𝑜𝑣 iscovariance,thesymbol∧islogical-and,thesymbol 𝜌 is\nPearsoncorrelation,thesymbol =⇒ islogicalimplication,and 𝑋 ⊥𝑌 isjustafancywaytosay\nthat 𝑋 and𝑌 areindependent.\nAstatementlike”𝐴 ∼ 𝐵𝑖𝑛(10,0.5) ∧ 𝐵 ∼ 𝐵𝑖𝑛(10,0.5) ∧ 𝐴 ⊥ 𝐵 =⇒ 𝐴+ 𝐵 ∼ 𝐵𝑖𝑛(20,0.5)”\nreads”If 𝐴 and 𝐵 arebothdistributedasBinomialswiththesameparameters,then 𝐴+ 𝐵 isa\nBinomialaswellwiththesame 𝑝 parameterandan 𝑛 parameterthat’sthesumofthethosefor 𝐴\nand 𝐵.”\n𝑋 ⊥𝑌 =⇒ 𝐶𝑜𝑣(𝑋,𝑌) = 0 𝑉𝑎𝑟(𝑋 + 𝑋) = 2𝑉𝑎𝑟(𝑋)\n𝐶𝑜𝑣(𝑋,𝑌) = 0 =⇒ 𝑋 ⊥𝑌 𝑋 ∼ N(0,1) ∧𝑌 ∼ N(0,1) =⇒ 𝜌(𝑋,𝑌) = 1\n𝑌 = 𝑋2 =⇒ 𝜌(𝑋,𝑌) = 1 𝑌 = 3𝑋 =⇒ 𝜌(𝑋,𝑌) = 3\n3 Random Number of Random Variables\nLet 𝑁 beanon-negativeinteger-valuedrandomvariable—thatis,arandomvariablethattakeson\nvaluesin {0,1,2,...}.Let 𝑋 ,𝑋 ,𝑋 ,... beaninfinitesequenceofindependentandidentically\n1 2 3\ndistributedrandomvariables(independentof 𝑁),eachwithmean 𝜇,and 𝑋 = (cid:205) 𝑖𝑁\n=1\n𝑋 𝑖 bethesum\nofthefirst 𝑁 ofthem.\nBeforedoinganywork,whatdoyouthink 𝐸[𝑋] willturnouttobe?Thenshowitmathematically\ntoseeifyourintuitioniscorrect.\n–2–\nProblems\n1 CS106A Is Popular\nCS106AisStanford’sintroductoryprogrammingcourseandlargelyconsideredtheprimary\ngatewaytoourundergraduatemajor.Assumenextquarter’sofferingofCS106Aisexactly600\npeople,thateachofthefourundergraduateclassesiscompromisedof1750students,andthatnext\nquarter’sCS106Arosterisjustsomerandomsampleofthe7000undergraduates.Let 𝐴, 𝐵,𝐶,and\n𝐷 countthenumberoffreshman,sophomores,juniors,andseniorsintheclassof600.\n• Presentthejointprobabilitymassfunctionof 𝐴,𝐵,𝐶,𝐷?Restated,presentanexpression\nfor 𝑃(𝐴 = 𝑎,𝐵 = 𝑏,𝐶 = 𝑐,𝐷 = 𝑑).\n• DoesyourPMFfromparta)describeamultinomialrandomvariable?Intuitivelyjustify\nyouranswer.\n• Whatistheconditionalprobabilitydistributionof 𝐴 giventhat 𝐵+𝐶 = 300?Restated,what\nis 𝑃(𝐴 = 𝑎|𝐵+𝐶 = 300)?\n• Doyouexpect𝐶𝑜𝑣(𝐴,𝐵) tobepositive,zero,ornegative?Justifyyouranswer.\n2 Managing Screen Time\nPushnotificationslightupourphonesataratethat’sguidedbyaPoissonprocesswithanconstant\naveragerateof5notificationsperhouratallhours,nightandday.Inanefforttomaximize\nproductivity,youputyourphonedownandignoreitasmuchaspossible.Youdo,however,\nperiodicallycheckittoclearallnotifications.Youcheckat7amwhenyouwakeup,noonwhenyou\ngrablunch,5pmasyouwrapupclassesfortheday,andthenagainat10pmbeforeyougotosleep.\nLetW,X,Y,andZbePoissonrandomvariablesthatcountthenumberofpushnotificationsthat\nhaveaccumulatedfrom10pmto7am,7amtonoon,noonto5pm,and5pmto10pm,respectively.\nWeassumethatthenumberofpushnotificationsthatarrivewithineachintervalareallmutually\nindependentofallotherintervals.\n• ComputethejointPMFonW,X,Y,andZ.\n• ComputetheconditionaljointPMFonW,X,Y,andZgiventhatW+X+Y+Z=150.\n• ComputetheconditionalPMFofX+Y+Z—that’sthenumberofnotificationsthatarrive\nwhileyou’reawake—giventhatW+X+Y+Z=150.\n• Compute 𝐸[𝑋 +𝑌 + 𝑍|𝑊 + 𝑋 +𝑌 + 𝑍 = 150] and𝑉𝑎𝑟(𝑋 +𝑌 + 𝑍|𝑊 + 𝑋 +𝑌 + 𝑍 = 150).\n–3–\nA=0 A=1\nB=0 B=1 B=0 B=1\nC=0 0.36 0.20 0.00 0.00\nC=1 0.04 0.20 0.10 0.10\n3 Understanding Bayes Nets\nThejointprobabilitytable(above)forrandomvariables 𝐴, 𝐵 and𝐶 isequivalenttotheBayes\nnetwork(below).Bothgivetheprobabilityofanycombinationoftherandomvariables.Inthe\nBayesnetworktheprobabilityofeachrandomvariableisprovidedgivenitscausalparents.\n• UsetheBayesnetworktoexplainwhy 𝑃(𝐴 = 0,𝐵 = 1,𝐶 = 1) = 0.20\n• Whatis 𝑃(𝐴 = 1|𝐶 = 1)?\n• Is 𝐴 independentof 𝐵?Explainyouranswer.\n• Is 𝐴 independentof 𝐵 given𝐶 = 1?Explainyouranswer.\n4 Fish Sticks [courtesy of Lisa Yan]\nFishSticks,theonlineplatformdesignedtomeetallofyourfishstickneeds,wantstomodeltheir\nhourlyhomepagetrafficfromStanford.Thecompanydecidestomodeltwodifferentbehaviorsfor\nhomepagevisitsaccordingtotheBayesianNetworkbelow:\n𝐴 and 𝐵 arethenumbersofStanfordstudentsandfaculty,respectively,whovisittheFishSticks\nhomepageinanhour.SinceFishSticksdoesnotknowwhenStanfordpeopleeat,thecompany\n–4–\nmodelsdemandasa”hidden”Bernoullirandomvariable 𝐷,whichdeterminesthedistributionof\n𝐴and 𝐵.RecallthatinaBayesianNetwork,randomvariablesareconditionallyindependentgiven\ntheirparents.Forexample,given 𝐷 = 0, 𝐴 ∼ Poi(5) and 𝐵 ∼ Poi(3),twoindependentrandom\nvariables.\na. Giventhat6usersfromgroup 𝐴 visitthehomepageinthenexthour,whatistheprobability\nthat 𝐷 = 0?\nb. Whatistheprobabilitythatinthenexthour,thetotal numberofuserswhovisitthe\nhomepagefromgroups 𝐴 and 𝐵 isequalto12,i.e.,whatis 𝑃(𝐴+ 𝐵 = 12)?\nc. Nowsimulate 𝑃(𝐴+ 𝐵 = total),wheretotal = 12,byimplementingthe\ninfer prob total(total, ntrials)functionbelowusingrejectionsampling.\n• totalisthetotalnumberofusersfromgroups 𝐴 and 𝐵 intheevent 𝐴+ 𝐵 = total.\n• ntrialsisthenumberofobservationstogenerateforrejectionsampling.\n• probisthereturnvaluetothefunction,whereprob ≈ 𝑃(𝐴+ 𝐵 = total).\n• Thefunctioncallisimplementedforyouatthebottomofthecodeblock.\nYoucancallthefollowingfunctionsfromthescipypackage:\n• stats.bernoulli.rvs(𝑝),whichrandomlygeneratesa1withprobability 𝑝,and\ngeneratesa0otherwise.\n• stats.poisson.rvs(𝜆),whichrandomlygeneratesavalueaccordingtoaPoisson\ndistributionwithparameter𝜆\nYouarenotrequiredtouselistsornumpyarraysinthisquestion(butyoucanifyouwant).\nPseudo-codeisfineaslongasyourcodeaccuratelyconveysyourapproach.\nimport numpy as np\nfrom scipy import stats\ndef infer_prob_total(total, ntrials):\n# here's where your implementation belongs\nreturn prob\ntotal = 12\nntrials = 50000\nprint('Simulated% P(A + B)=', infer_prob_total(total, ntrials))\n5 ChatGPT, Watermarking, and Bayesian Inference\nChatGPTisagenerativeAItechnologythatcanbecoarselysummarizedtobeachatbotwitha\nseeminglyboundlessabilitytodiscussanytopic—history,computerscience,art,nuclearphysics,\nprobability,andeventheethicsofusingChatGPT—inanyoneofseveralwrittenlanguages,\nincludingEnglish,French,Spanish,C++,JavaScript,Python,andsome100others.\n–5–\nUnsurprisingly,wewillsoonbelefttowonderwhetherapoem,aTweet,aC++function,ora\ncollegethesisiswrittenbyChatGPTorahumanbeing.Questionsaboutauthorship,accuracy,and\nattributionhavepromptedOpenAI,thecompanybehindChatGPT,toaddresstheseconcernsby\nimplementingChatGPTtoemploywhat’stermedwatermarkingandinsertcertainwordsmoreor\nlessoftenthaniscustomaryineventhebestofhuman-authoredwriting.\nToillustrate,let’sassumethatmosthumansusethewordtheanaverageof4.8timesper100\nwords,whereasChatGPTmightgenerateprosewheretheappearsonaverage6.5timesper100\nwords.Similarly,humansusethewordofonaverageabout3.9timesper100words,whereas\nChatGPTmightleverageofabout6.2timesper100words.Conversely,ChatGPTmightgenerate\nthewordbyonly1.6timesper100words,whereashumansusethewordbyabout2.7timesper\n100words.\na. YouelecttomodelwordfrequencyofallwordsusingeitheraPoissonforparagraphsof200\norsowords—butasaGaussianforlargerdocuments—say10000wordsormore.Explain\nwhytheGaussianmightbethebetterchoiceforlargerdocumentsthanthePoisson,whereas\nPoissonismoreeasilydefendedforsmallerdocuments.\nb. Adeeperstatisticalanalysisofmanyhuman-writtendocumentsstronglysuggeststhat\n𝐻 ∼ N(5,1) and 𝐻 ∼ N(4,1),whereasaseparatebutequallydeepanalysisstrongly\nthe by\nsuggeststhat𝐶 ∼ N(3,1) and𝐶 ∼ N(2,1).(Theparametersareroundedvaluesfor\nthe by\nsimplicityandassumedtobeper-100-words.)\nAssumingapriorbeliefthataverylongdocumentwaswrittenbyahumanis0.99,whatis\nyourposteriorbeliefthatthedocumentwaswrittenbyahumanifthedocumentcontainsan\naverageof5the’severy100wordsbutonly1byevery100words.Youmayassumeall\nGaussiandistributionsofinterestareindependent.\nc. Inreality,theGaussiansherearenotindependent,sincethepresenceofonewordimplies\ntheabsenceofanother.Assumingacorrelationvalue 𝜌 thatisslightlynegative,wouldyou\nexpecttheobservationsstatedinpartb)toresultinalargerposteriorprobabilityora\nsmallerone?Brieflyexplainwhy. <END>"}
{"prompt":"cs109_lec08_conceptcheck.txt\n\n###\n\n","completion":" Lecture 8: Concept Check  Graded\nStudent\nNeetish Sharma\nTotal Points\n5 \/ 5 pts\nQuestion 1\nAll About Poisson 3 \/ 3 pts\n1.1 Poisson: When Applicable? 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\n1.2 Poisson: In the Limit 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\n1.3 Poisson: Support 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 2\nManaging Email 2 \/ 2 pts\n + 2 pts Correct\n+ 0 pts Incorrect\nQ1 All About Poisson\n3 Points\nSelect the appropriate responses to each of the short answer questions about\nthe Poisson distribution.\nQ1.1 Poisson: When Applicable?\n1 Point\nWhat is one general problem that the Poisson RV models?\nAmount of time until next event occurs.\nProbability of k requests in the next 1 minute.\nNumber of ways to place k balls in n bins.\nAmount of time until first success.\nQ1.2 Poisson: In the Limit\n1 Point\nIt can be proven that the Poisson distribution is the limit of the ____ distribution.\nbinomial\ngeometric\nexponential\nQ1.3 Poisson: Support\n1 Point\nPick all of the following values that a Poisson RV can take on:\n-1\n0\n0.5\n1\n1.5\n10000\nQ2 Managing Email\n2 Points\nSuppose you receive a steady-state average of 5 emails every hour during all\nhours of the day. What is the probability you'll receive at least one email in the\nnext five minutes? You may assume email arrival can be modeled as a traditional\nPoisson distribution.\nExpress your probability with a leading zero and four decimal places.\n0.3408 <END>"}
{"prompt":"02_section.txt\n\n###\n\n","completion":" CS109 April18,2024\nSection 2: Conditional Probability and Bayes\nChrisPiech,MehranSahami,JerryCain,LisaYan,andnumerousCS109CA’s.\nOverview of Section Materials\nThewarm-upquestionsprovidedwillhelpstudentspracticeconceptsintroducedinlectures.Thesectionprob-\nlemsaremeanttoapplytheseconceptsinmorecomplexscenariossimilartowhatyouwillseeinproblemsets\nandexams.Infact,manyofthemareoldexamquestions.\nBeforeyouleavelab,makesureyouclickheresothatyou’remarkedashavingattendedthisweek’ssection.The\nCAleadingyourdiscussionsectioncanenterthepasswordneededonceyou’vesubmitted.\nWarm-ups\n1. Definitions:CiteBayes’Theorem.Canyouexplainwhy 𝑃(𝐴|𝐵) isdifferentthan 𝑃(𝐵|𝐴)?\n2. TrueorFalse.NotethattruemeanstrueforALLcases.\n(a) Ingeneral, 𝑃(𝐴𝐵|𝐶) = 𝑃(𝐵|𝐶)𝑃(𝐴|𝐵𝐶)\n(b) If 𝐴 and 𝐵 areindependent,soare 𝐴 and 𝐵𝐶 .\n1 Taking Expectation: Breaking Vegas\nPreamble:Whenarandomvariablefitsneatlyintoafamilywe’veseenbefore(e.g.Binomial),wegetitsexpec-\ntationforfree.Whenitdoesnot,wehavetousethedefinitionofexpectation.\nProblem:Ifyoubeton“Red”inRoulette,thereis 𝑝 = 18\/38thatyouwithwin$Yanda (1−𝑝) probabilitythat\nyoulose$Y.Considerthisalgorithmforaseriesofbets:\nLetY=$1.FirstyoubetY.Ifyouwin,thenstop.Ifyoulose,thensetYtobe2Yandrepeat.\nWhatareyourexpectedwinningswhenyoustop?Itwillhelptorecallthatthesumofageometricseries 𝑎0+𝑎1+\n𝑎2 +··· = 1 if0 < 𝑎 < 1.Vegasbreaksyou:Whydoesn’teveryonedothis?\n1−𝑎\n2 Conditional Probabilities: Missing Not at Random\nPreamble:Wehavethreebigtoolsformanipulatingconditionalprobabilities:\n• Definitionofconditionalprobability: 𝑃(𝐸𝐹) = 𝑃(𝐸|𝐹)𝑃(𝐹)\n• LawofTotalProbability: 𝑃(𝐸) = 𝑃(𝐸𝐹) + 𝑃(𝐸𝐹𝐶) = 𝑃(𝐸|𝐹)𝑃(𝐹) + 𝑃(𝐸|𝐹𝐶)𝑃(𝐹𝐶)\n• BayesRule: 𝑃(𝐸|𝐹) = 𝑃(𝐹|𝐸)𝑃(𝐸) = 𝑃(𝐹|𝐸)𝑃(𝐸)\n𝑃(𝐹) 𝑃(𝐹|𝐸)𝑃(𝐸)+𝑃(𝐹|𝐸𝐶)𝑃(𝐸𝐶)\nThisisagoodtimetocommitthesethreetomemoryandstartthinkingaboutwheneachofthemisuseful.\nProblem:YoucollectdataonwhetherornotpeopleintendtovoteforAyesha,acandidateinanupcomingelec-\ntion.Yousendanelectronicpollto100randomlychosenpeople.Youassumeall100responsesareindependent\nandidenticallydistributed.\nUserResponse Count\nRespondedthattheywillvoteforAyesha 40\nRespondedthattheywillnotvoteforAyesha 45\nDidnotrespond 15\nLet 𝐴 betheeventthatapersonwillvoteforAyesha.Let 𝑀 betheeventthatauserdidnotrespondtothepoll.\nWeareinterestedinestimating 𝑃(𝐴),thoughcomputingthatestimateisdifficult,giventhat15usersdidn’tactu-\nallyrespond.\na. WhatistheprobabilitythatausersaidtheywillvoteforAyeshaandthattheyrespondedtothepoll 𝑃(𝐴 and 𝑀𝐶)?\nb. Whichformulafromclasswouldyouusetocalculate 𝑃(𝐴)?Yourformulashouldrelyonthecontextthat\nvotersforAyeshaareinoneoftwo(mutuallyexclusive)groups:thosethatmissedthepoll,andthosethat\ndidnot.\nc. Calculatethe 𝑃(𝐴).Youestimatethattheprobabilitythatavoterismissing,giventhattheyweregoingto\nvoteforAyeshais 𝑃(𝑀|𝐴) = 1.\n5\n3 Sending Bits to Space\nPreamble: Whensendingbinarydatatosatellites(orreallyoveranynoisychannel),thebitscanbeflippedwith\nhighprobability.In1947,RichardHammingdevelopedasystemtomorereliablysenddata.ByusingErrorCor-\nrectingHammingCodes,youcansendastreamof4bitsalongwith3redundantbits.Ifzerooroneoftheseven\nbitsarecorrupted,usingerrorcorrectingcodes,areceivercanidentifytheoriginal4bits.\nProblem: Letsconsiderthecaseofsendingasignaltoasatellitewhereeachbitisindependentlyflippedwith\nprobability 𝑝 = 0.1.\na. Ifyousend4bits,whatistheprobabilitythatthecorrectmessagewasreceived(i.e.noneofthebitsare\nflipped).\nb. Ifyousend4bits,with3Hammingerrorcorrectingbits,whatistheprobabilitythataninterpretablemes-\nsage(i.e.amessagewithzerooroneerrors)wasreceived?\nc. InsteadofusingHammingcodes,youdecidetosend100copiesofeachofthefourbits.Ifforeverysingle\nbit,morethan50ofthecopiesarenotflipped,thesignalwillbecorrectable.Whatistheprobabilitythata\ncorrectablemessagewasreceived?\nExtra:Explanationofthe”Hamming(7,4)”technique\nIfwearetryingtotransmit4bits,wecansendanadditional3”parity”bitsthatwecanusetocorrectourorig-\ninalmessageifabitgetsflippedduetoanerrorintransmission.Considerthediagram.Thedatabitsare 𝑑\n1\nthrough 𝑑 .The”parity”bitsare 𝑝 through 𝑝 .Aparitybitissettowhatevervaluewouldmakeit’slargecir-\n4 1 3\nclehaveanevennumberofbits.Forexample,thegreencircleconsistsof 𝑝 , 𝑑 , 𝑑 ,and 𝑑 .If 𝑑 = 1, 𝑑 = 1,\n1 1 2 4 1 2\nand 𝑑 = 1,then 𝑝 wouldbesetto1inordertoensurethereareanevennumberofbitsinthatcircle(inthis\n4 1\ncase,4bits).\nConvinceyourselfthatasingleerrorwhichappearedinanybitcouldbeidentifiedandcorrected!Forexample,\nif 𝑑 isflipped,itwouldthrowofftheparityforthegreenandredcircles.Therefore,flipping 𝑑 backistheonly\n2 2\nwaytocorrecttheparity.Asanotherexample,if 𝑝 isflipped,thenonlythebluecirclewouldhaveaparityis-\n2\nsue,andflipping 𝑝 backistheuniquesolutiontofixingtheparity.\n2 <END>"}
{"prompt":"cs109_lec07_conceptcheck.txt\n\n###\n\n","completion":" Lecture 7: Concept Check  Graded\nStudent\nNeetish Sharma\nTotal Points\n4 \/ 4 pts\nQuestion 1\nUnderstanding Variance 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 2\nCoin Flips 3 \/ 3 pts\n2.1 Coin Flips: Seven Up 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\n2.2 Coin Flips: Which Coin? 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\n2.3 Coin Flips: Expectation 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQ1 Understanding Variance\n1 Point\nTrue or False: The variance of a probability distribution be negative?\nTrue\nFalse\nQ2 Coin Flips\n3 Points\nYou have one biased coin (where the probability of getting heads is 0.7 on any\nflip) and one unbiased coin. You randomly select one of these two coins and\nproceed to flip it exactly 10 times.\nQ2.1 Coin Flips: Seven Up\n1 Point\nWhat is the probability that you get 7 heads in all 10 flips? (to 3 decimal places,\nformatted as 0.abc)\n0.192\nQ2.2 Coin Flips: Which Coin?\n1 Point\nGiven that I see 7 heads in 10 flips, what is the probability that I chose the biased\ncoin? (to 3 decimal places, formatted as 0.abc))\n0.695\nQ2.3 Coin Flips: Expectation\n1 Point\nSuppose that your friend now tells you that you indeed chose the biased coin.\nWhat is the expected number of heads you will flip on 10 trials using the biased\ncoin?\n7 <END>"}
{"prompt":"04_section.txt\n\n###\n\n","completion":" –1–\nCS109 May2nd,2024\nContinuous Random Variables, Joint Distributions\nBeforeyouleavelab,makesureyouclickheresothatyou’remarkedashavingattendedthis\nweek’ssection.TheCAleadingyourdiscussionsectioncanenterthepasswordneededonce\nyou’vesubmitted.\n1 Warmups\n1.1 Reviewing the Basics\na. GivenaNormalRV 𝑋 ∼ 𝑁(𝜇,𝜎2),howcanwecompute 𝑃(𝑋 ≤ 𝑥) fromthestandard\nNormaldistributionZwithCDF 𝜙?\nb. Whatisacontinuitycorrectionandwhenshouldweuseit?\nc. IfwehaveajointPMFfordiscreterandomvariables 𝑝 𝑋,𝑌(𝑥,𝑦),howcanwecomputethe\nmarginalPMF 𝑝 𝑋(𝑥)?\n1.2 Independent Random Variables\na. WhatdistributiondoesthesumoftwoindependentbinomialRVs 𝑋 +𝑌 have,where 𝑋 ∼\n𝐵𝑖𝑛(𝑛 , 𝑝) and𝑌 ∼ 𝐵𝑖𝑛(𝑛 , 𝑝)?Includeanyparameterswithyouranswer.\n1 2\nb. WhatdistributiondoesthesumoftwoindependentPoissonRVs 𝑋 +𝑌 have,where 𝑋 ∼\nPoi(𝜆 ) and𝑌 ∼ Poi(𝜆 )?Includeanyparameterswithyouranswer.\n1 2\n2 Marguerite Gets Some Competition\nInthelate1880s,Stanfordbeganrunningahorseandtwelve-personbuggyservicefromtheStan-\nfordQuadtothetrainstationjustoffcampus.Thenameofthisshuttlingservicewaschosentobe\nMarguerite,whichwasthenameofthefavoritehorseofsomeStanfordbigwigofthetime.The\nhorse-and-carriageoperationwasretiredaround1910andreplacedwithelectricstreetcars,which\nthemselveswerereplacedwithbusesaround1930.Theservicehasgrownsubstantiallysince,and\nthebuseshavebeenupgradedseveraltimes.Theservice,however,hasretaineditsnamesincethe\nverybeginning.\nSeveralStanfordhorseenthusiastshaverecentlyrevivedthehorse-and-buggyservicetocompete\nwithMarguerite,andthey’vegivenitthenameHildegard.Now,whenyouneedaridefromthe\nQuadtothetrainstation,youhavetwooptions!\nYouarriveattheQuad,headedtothetrainstation,andyou’reequallyhappytotakeeitherofthe\ntwoindependentservices.Youarrivepreciselyat8:00am,whichisthetimethatbothservices\nstartfortheday.ThenumberofminutesyouneedtowaitforaMargueritebusismodeledbya\ndiscreteUniformrandomvariable 𝑀 ∼ 𝑈𝑛𝑖(0,20),whereasthenumberofminutesyouneed\ntowaitforaHildegardhorse-and-buggyismodeledbyadiscretePoissonrandomvariable 𝐻 ∼\n𝑃𝑜𝑖(10).(Yes,it’stechnicallypossiblethatHildegardneverarrives.)\n–2–\na. WhatistheprobabilitythatMargueriteandHildegardbotharriveat𝑡 = 6minutes?\nb. Whatistheconditionalprobabilitythat 𝐻 < 𝑀,given 𝑀 = 𝑚—thatis,whatis 𝑃(𝐻 <\n𝑀|𝑀 = 𝑚)?Expressyouranswerasasum.\nc. Whatistheunconditionalprobabilitythat 𝐻 < 𝑀,i.e.,whatis 𝑃(𝐻 < 𝑀)?Expressyour\nanswerasadoublesumthatleveragesyouranswertopartb.\nd. WhatistheCDFofyourwaitingtimeforthefirstofthetwotoarrive?Youshouldleave\nyouranswerinsummationform.\n3 Burrow Smoke Detectors and Joint Probability Distributions\nBurrowLabshastakenonotherstartupsinthehomesafetyandsecurityspaceandhasrecently\nstartedmarketinganewsmokedetector.Burrow’ssmokedetectorsrelyon𝐶𝑂 sensorsthat\n2\neventuallyfail,andthatfailuretimedictatestheaverageproductlifetimeofthesmokedetector.\nBurrowmanufacturesthreequartersofitssmokedetectorsincentralIdaho,andtherestareman-\nufacturedinsuburbanMaine.Anysinglesmokedetector’sproductlifetimecanbemodeledasa\nExponentialrandomvariable.\nEachofthetwolocationssourcesits𝐶𝑂 sensorsfromdifferentsuppliers,sothesmokedetec-\n2\ntorsmanufacturedinMainehaveanaverageproductlifetimeof7yearsandthesmokedetectors\nmanufacturedinIdahohaveanaverageproductlifetimeof6years.Allsmokedetectorsaresold\nonline,soasidefromthefactthatasmokedetectoristhreetimesmorelikelytoshipfromthe\nIdahofacility,youcan’ttellbylookingatasinglesmokedetectorwhereitwasmanufactured.\nLet𝑇 modeltheamountoftimethatpassesuntilthe𝐶𝑂 sensor(andthereforethesmokedetec-\n2\ntor)fails,andlet 𝑀 beadiscreterandomvariablethattakesonthevalueof1forasmokedetec-\ntormanufacturedinMaine,and0otherwise.\na. Presentthecumulativedistributionandprobabilitydensityfunctionsfortherandomvari-\nable𝑇.BothyourCDFandyourPDFshouldbeanalyticfunctionson𝑡.\nb. ComputetheprobabilitythatasmokedetectorwasmanufacturedinMaine,giventhatit\nlastsmorethan15years.Ifneeded,youcankeepyouranswerintermsof 𝐹 𝑇(15) or 𝑓 𝑇(15)\nfrompart(a).However,anyconditionalexpressionoftheform 𝑃(·|·) or 𝑓(·|·) mustbe\nevaluated.\n4 Elections\nWewouldliketoseehowwecouldpredictanelectionbetweentwocandidatesinFrance(Aand\nB),givendatafrom10polls.Foreachofthe10polls,wereportbelowtheirsamplesize,how\nmanypeoplesaidtheywouldvoteforcandidateA,andhowmanypeoplesaidtheywouldvote\nforcandidateB.Notallpollsarecreatedequal,soforeachpollwealsoreportavalue”weight”\nwhichrepresentshowaccuratewebelievethepollwas.Thedataforthisproblemcanbefoundon\ntheclasswebsiteinpolls.csv:\n–3–\na. First,assumethateachsampleineachpollisanindependentexperimentofwhetherornot\narandompersoninFrancewouldvoteforcandidateA(disregardweights).\n• CalculatetheprobabilitythatarandompersoninFrancevotesforcandidateA.\n• AssumeeachpersonvotesforcandidateAwiththeprobabilityyou’vecalculatedand\notherwisevotesforcandidateB.IfthepopulationofFranceis64,888,792,whatisthe\nprobabilitythatcandidateAgetsmorethanhalfofthevotes?\nb. NateSilveratfivethirtyeightpioneeredanapproachcalledthe”PollofPolls”topredict\nelections.ForeachcandidateAorB,wehavearandomvariable 𝑆 𝐴 or 𝑆 𝐵 whichrepresents\ntheirstrengthonelectionnight(likeELOscores).TheprobabilitythatAwinsis 𝑃(𝑆 𝐴 >\n𝑆 𝐵).\n• Identifytheparametersfortherandomvariables 𝑆 𝐴 and 𝑆 𝐵.Both 𝑆 𝐴 and 𝑆 𝐵 arede-\nfinedtobenormalwiththefollowingparameters:\n• Wewillcalculate 𝑃(𝑆 𝐴 > 𝑆 𝐵) bysimulating100,000fakeelections.Ineachfakeelec-\ntion,wedrawarandomsampleforthestrengthofAfrom 𝑆 𝐴 andarandomsample\nforthestrengthofBfrom 𝑆 𝐵.If 𝑆 𝐴 isgreaterthan 𝑆 𝐵,candidateAwins.Whatdowe\nexpecttoseeifwesimulatesomanytimes?Whatdoweactuallysee?\n–4–\nc. Whichmodel,theonefrom(a)orthemodelfrom(b)seemsmoreappropriate?Whymight\nthatbethecase?OnelectionnightcandidateAwins.Wasyourpredictionfrompart(b)\n”correct”? <END>"}
{"prompt":"cs109_lec21_conceptcheck.txt\n\n###\n\n","completion":" Lecture 21: Concept Check  Graded\nStudent\nNeetish Sharma\nTotal Points\n5 \/ 5 pts\nQuestion 1\nExponential RV and MLE 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 2\nMystery RV and MLE 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 3\nBeta Basics 2 \/ 2 pts\n3.1 Backpatching Betas from Expectations 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\n3.2 Bootstrapping Beta 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 4\nThumbtacks 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQ1 Exponential RV and MLE\n1 Point\nSuppose that 8 batteries are needed to power an digital thermometer, and that\neach of the 8 batteries has a lifetime that's exponentially distributed, as with\nf(T i∣λ) = λe−λT i . The thermometer works brilliantly for a while, but stops\nworking after 112 days.\nWhat is the likelihood function, L(λ)?\nL(λ) = λ∏8 e−λT i\ni=1\nL(λ) = λ∏112 e−λT i\ni=1\nL(λ) = λ8 ∏ i8\n=1\ne−λT i\nL(λ) = λ112 ∏8 e−λT i\ni=1\nL(λ) = λ112 ∏112 e−λT i\ni=1\nQ2 Mystery RV and MLE\n1 Point\nConsider an iid sample of continuous random variables X 1, X 2, ..., X n with a\ndensity function of f(X\ni\n∣γ) = 21\nγ\ne−∣X γi ∣ .\nWhat is the maximum likelihood estimate of γ, or γ MLE?\nHere, the RV is presumably influenced by a single parameter γ.\nn\nγ MLE = ∑ i=1 X i.\nγ MLE = Xˉ = n1 ∑ in =1 X i.\nγ\nMLE\n= n1 ∣∑ in\n=1\nX\ni\n∣.\nγ\nMLE\n= n1 ∑ in\n=1\n∣X\ni\n∣.\nQ3 Beta Basics\n2 Points\nLet's make sure you're familiar with the Beta function!\nQ3.1 Backpatching Betas from Expectations\n1 Point\nWhich of the following RVs have E[X] = 0.5? Select all that apply.\nX ∼ Beta(1,1)\nX ∼ Beta(5,10)\nX ∼ Beta(100,100)\nX ∼ Beta(3,5)\nX ∼ Beta(7,1)\nQ3.2 Bootstrapping Beta\n1 Point\nWhich of the following is a uniform distribution between 0 and 1? Select all that\napply.\nX ∼ Beta(1,1)\nX ∼ Beta(0,0)\nX ∼ Beta(100,100)\nX ∼ Beta(3,5)\nX ∼ Beta(7,1)\nQ4 Thumbtacks\n1 Point\nIf you flip a thumbtack up into the air, it can either land on its base (with the\npoint sticking straight up) or it can rest on the point and a portion of its circular\nbase. You have a hunch that the probability the thumbtack lands on its base is\n0.2. You then toss the that thumbtack 18 times and see that it never lands on its\nbase. Which of the following RVs could model your posterior belief about of\nwhat the probability really is? Assume a Bayesian approach as introduced in\nlecture, and assume that the prior belief distribution is modeled as a Beta.\nΘ ∼ Beta(3,9)\nΘ ∼ Beta(3,18)\nΘ ∼ Beta(3,27)\nΘ ∼ Beta(21,27)\nΘ ∼ Beta(21,99) <END>"}
{"prompt":"cs109_lec12_conceptcheck.txt\n\n###\n\n","completion":" Lecture 12: Concept Check  Graded\nStudent\nNeetish Sharma\nTotal Points\n5 \/ 5 pts\nQuestion 1\nIndependence 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 2\nSum of Binomials 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 3\nSum of Poissons 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 4\nSum of Binomial and Poisson 2 \/ 2 pts\n4.1 Small Sum 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\n4.2 Probability that sum is large 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQ1 Independence\n1 Point\nAssume you roll two fair dice and D is the outcome of the first die, D is the\n1 2\noutcome of the second die and S = D +D . Select all that are true.\n1 2\nP(D = 1,S = 7) = P(D = 1) ⋅ P(S = 7)\n1 1\nP(D = 6,S = 6) = P(D = 6) ⋅ P(S = 6)\n1 1\nRandom variables D and D are independent\n1 2\nRandom variables D and S are independent\n1\nEvents D = 1 and S = 7 are independent\n1\nQ2 Sum of Binomials\n1 Point\nLet X and Y be two independent random variables where\nX ∼ Bin(25,0.5) and Y ∼ Bin(50,0.5). Let Z = X + Y . What is the\ndistribution of Z?\nZ ∼ Bin(75,0.25)\nZ ∼ Bin(75,0.5)\nZ ∼ Bin(75,1.0)\nZ ∼ Bin(2500,0.25)\nQ3 Sum of Poissons\n1 Point\nLet X and Y be two independent random variables where X ∼ Poi(7) and\nY ∼ Poi(3). What is the distribution of X + Y ?\nX + Y ∼ Poi(10)\nX + Y ∼ Poi(21)\nX + Y ∼ Poi(4)\nQ4 Sum of Binomial and Poisson\n2 Points\nLet X and Y be two independent random variables where X ∼ Bin(5,0.5) and\nY ∼ Poi(1).\nQ4.1 Small Sum\n1 Point\nWhat is the exactly probability that X + Y ≤ 1? Express your answer to three\ndecimal places. Hint: Rely on Python and scipy to quickly compute the relevant\nprobabilities, as with:\n>>> from scipy.stats import poisson, binom\n>>> n, p, lamb = 5, 0.5, 1\n>>> poisson(lamb).pmf(0)\n0.36787944117144233\n>>> binom(n, p).pmf(1)\n0.15624999999999994\n>>>\n0.0805\nQ4.2 Probability that sum is large\n1 Point\nYou're interested in the exact probability, however small it might be, that X + Y\nis precisely 50. Recall that P(X + Y = 50) is calculated as the sum of many\nterms of the form P(X = k)P(Y = 50 − k). How many such terms—each of\nthe form P(X = k)P(Y = 50 − k)—contribute a nonzero amount to the\noverall probability?\n6 <END>"}
{"prompt":"cs109_lec06_conceptcheck.txt\n\n###\n\n","completion":" Lecture 6: Concept Check  Graded\nStudent\nNeetish Sharma\nTotal Points\n5 \/ 5 pts\nQuestion 1\nRandom Variables: Definitions 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 2\nRandom Variables 3 \/ 3 pts\n2.1 Random Variables: Probability Mass Functions 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\n2.2 Random Variables: Cumulative Distribution Functions 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\n2.3 Random Variables: Expectation 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 3\nExpected Number of Volleyball Games 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQ1 Random Variables: Definitions\n1 Point\nWhich of the following are random variables? Select all that apply.\nIf n coins are flipped, the number of coins that land heads.\nIf n coins are flipped, the probability that all coins land on heads.\nThe number of ways to order {A,B,C,D,E} so that A and B are not next to\neach other and C and D are not next to each other.\nA boolean which denotes whether it will rain today (1) or not (0).\nQ2 Random Variables\n3 Points\nLet X be a random variable whose support is {0, 1, 2}.\nQ2.1 Random Variables: Probability Mass Functions\n1 Point\nWhich of the following is a valid probability mass function for X? Assume that for\neach answer below, P(X = x) = 0 for all unspecified values of x.\nP(X = 0) = 0, P(X = 1) = 2, and P(X = 2) = 0.5\nP(X = 0) = 0.1, P(X = 1) = 0.8, and P(X = 2) = 0.1\nP(X = 0) = −0.1, P(X = 1) = 0.9, and P(X = 2) = 0.2\nP(X = 0) = 0.2, P(X = 1) = 0.6, and P(X = 2) = 0.11\nQ2.2 Random Variables: Cumulative Distribution Functions\n1 Point\nWhat is the CDF (Continuous Distribution Function), associated with the valid\nPMF you found in Q2.1?\nP(X < 0) = 0, P(X ≤ 0) = 0, P(X ≤ 1) = 2, P(X ≤ 2) = 0.5\nP(X < 0) = 0, P(X ≤ 0) = 0.1, P(X ≤ 1) = 0.9, P(X ≤ 2) = 1\nP(X < 0) = 0, P(X ≤ 0) = −0.1, P(X ≥ 1) = 0.8, P(X ≤ 2) = 1\nP(X < 0) = 0, P(X ≤ 0) = 0.2, P(X ≤ 1) = 0.8, P(X ≤ 2) = 0.91\nQ2.3 Random Variables: Expectation\n1 Point\nGiven the same PMF you found in Q2.1, what is the expectation, E[X]?\n(Remember expectation is defined as E[X] = ∑ xP(x)) Hint: see if you can\nx\nuse symmetry to cut down on algebra!\n1\n0.5\n0.63\n1.34\nQ3 Expected Number of Volleyball Games\n1 Point\nTwo volleyball teams—let's say Nebraska and Stanford, which according to one\npoll were the top two Division I teams last autumn—play a best-of-seven match,\nand the match ends as soon as either team wins four games. Each game results\nin a win for one team and a loss for the other, so there are no ties or draws.\nAssume each team is equally likely to win each game, and that each game is\nindependent of all others. Compute the expected number of games played, and\nexpress your answer out to four decimal places.\n5.8125 <END>"}
{"prompt":"Important Links.txt\n\n###\n\n","completion":" Course Website (you can view schedule here)\n\nEd Forum (for real-time Q&A data from students, TAs, and instructors - need access via an authorized Stanford login, so ask Neetish Sharma) <END>"}
{"prompt":"04_section_soln.txt\n\n###\n\n","completion":" –1–\nCS109 May2nd,2024\nContinuous Random Variables, Joint Distributions\nBeforeyouleavelab,makesureyouclickheresothatyou’remarkedashavingattendedthis\nweek’ssection.TheCAleadingyourdiscussionsectioncanenterthepasswordneededonce\nyou’vesubmitted.\n1 Warmups\n1.1 Reviewing the Basics\na. GivenaNormalRV 𝑋 ∼ 𝑁(𝜇,𝜎2),howcanwecompute 𝑃(𝑋 ≤ 𝑥) fromthestandard\nNormaldistributionZwithCDF 𝜙?\nb. Whatisacontinuitycorrectionandwhenshouldweuseit?\nc. IfwehaveajointPMFfordiscreterandomvariables 𝑝 𝑋,𝑌(𝑥,𝑦),howcanwecomputethe\nmarginalPMF 𝑝 𝑋(𝑥)?\na. First,weapplyalineartransformationtoarriveatΦ((𝑥 − 𝜇)\/𝜎).Wethenlookup\nthevaluewe’vecomputedintheStandardNormalTable(orwerelyonPythonto\ncomputetheprobabilityforus).\nb. ContinuitycorrectionisusedwhenaNormaldistributionisusedtoapproximatea\nBinomial.SinceaNormaliscontinuousandBinomialisdiscrete,wehavetousea\ncontinuitycorrectiontodiscretizetheNormal.Thecontinuitycorrectionmakesitso\nthatthenormalvariableisevaluatedfrom+or-0.5incrementsfromthedesired 𝑘\nvalue.\nc. Themarginaldistributionis 𝑝 𝑋(𝑥) = (cid:205)\n𝑦\n𝑝 𝑋,𝑌(𝑥,𝑦)\n1.2 Independent Random Variables\na. WhatdistributiondoesthesumoftwoindependentbinomialRVs 𝑋 +𝑌 have,where 𝑋 ∼\n𝐵𝑖𝑛(𝑛 , 𝑝) and𝑌 ∼ 𝐵𝑖𝑛(𝑛 , 𝑝)?Includeanyparameterswithyouranswer.\n1 2\nb. WhatdistributiondoesthesumoftwoindependentPoissonRVs 𝑋 +𝑌 have,where 𝑋 ∼\nPoi(𝜆 ) and𝑌 ∼ Poi(𝜆 )?Includeanyparameterswithyouranswer.\n1 2\na. Binomial: 𝑋 +𝑌 ∼ 𝐵𝑖𝑛(𝑛 +𝑛 , 𝑝)\n1 2\nb. Poisson: 𝑋 +𝑌 ∼ Poi(𝜆 +𝜆 )\n1 2\n–2–\n2 Marguerite Gets Some Competition\nInthelate1880s,Stanfordbeganrunningahorseandtwelve-personbuggyservicefromtheStan-\nfordQuadtothetrainstationjustoffcampus.Thenameofthisshuttlingservicewaschosentobe\nMarguerite,whichwasthenameofthefavoritehorseofsomeStanfordbigwigofthetime.The\nhorse-and-carriageoperationwasretiredaround1910andreplacedwithelectricstreetcars,which\nthemselveswerereplacedwithbusesaround1930.Theservicehasgrownsubstantiallysince,and\nthebuseshavebeenupgradedseveraltimes.Theservice,however,hasretaineditsnamesincethe\nverybeginning.\nSeveralStanfordhorseenthusiastshaverecentlyrevivedthehorse-and-buggyservicetocompete\nwithMarguerite,andthey’vegivenitthenameHildegard.Now,whenyouneedaridefromthe\nQuadtothetrainstation,youhavetwooptions!\nYouarriveattheQuad,headedtothetrainstation,andyou’reequallyhappytotakeeitherofthe\ntwoindependentservices.Youarrivepreciselyat8:00am,whichisthetimethatbothservices\nstartfortheday.ThenumberofminutesyouneedtowaitforaMargueritebusismodeledbya\ndiscreteUniformrandomvariable 𝑀 ∼ 𝑈𝑛𝑖(0,20),whereasthenumberofminutesyouneed\ntowaitforaHildegardhorse-and-buggyismodeledbyadiscretePoissonrandomvariable 𝐻 ∼\n𝑃𝑜𝑖(10).(Yes,it’stechnicallypossiblethatHildegardneverarrives.)\na. WhatistheprobabilitythatMargueriteandHildegardbotharriveat𝑡 = 6minutes?\nWecanrepresenttheeventsthattheMargueriteandHildegardarriveat𝑡 = 6minutesas\n𝑀 = 6and 𝐻 = 6,respectively.Write\n𝑃(𝑀 = 6,𝐻 = 6) = 𝑃(𝑀 = 6)𝑃(𝐻 = 6) (𝑀 ⊥ 𝐻)\n1 106𝑒−10\n= · .\n21 6!\nb. Whatistheconditionalprobabilitythat 𝐻 < 𝑀,given 𝑀 = 𝑚—thatis,whatis 𝑃(𝐻 <\n𝑀|𝑀 = 𝑚)?Expressyouranswerasasum.\nWewerelookingforsomethingalongthelinesofthis:\n𝑃(𝐻 < 𝑀|𝑀 = 𝑚) = 𝑃(𝐻 < 𝑚) (specifyingvalueof 𝑀)\n𝑚−1\n∑︁\n= 𝑃(𝐻 = ℎ)\nℎ=0\n𝑚 ∑︁−1 10ℎ𝑒−10\n= .\nℎ!\nℎ=0\nc. Whatistheunconditionalprobabilitythat 𝐻 < 𝑀,i.e.,whatis 𝑃(𝐻 < 𝑀)?Expressyour\nanswerasadoublesumthatleveragesyouranswertopartb.\n–3–\nWewerelookingforsomethinglikethis:\n∑︁\n𝑃(𝐻 < 𝑀) = 𝑃(𝐻 < 𝑀,𝑀 = 𝑚) (LawofTotalProbability)\n𝑚\n∑︁\n= 𝑃(𝐻 < 𝑀|𝑀 = 𝑚)𝑃(𝑀 = 𝑚) (ChainRule)\n𝑚\n20\n∑︁ 1\n= · 𝑃(𝐻 < 𝑀|𝑀 = 𝑚)\n21\n𝑚=0\n1\n∑︁20 𝑚 ∑︁−1 10ℎ𝑒−10\n= (frompartb)\n21 ℎ!\n𝑚=0 ℎ=0\nd. WhatistheCDFofyourwaitingtimeforthefirstofthetwotoarrive?Youshouldleave\nyouranswerinsummationform.\nLet 𝑆 betherandomvariablerepresentingyourwaitingtimeforthefirstofthetwotoar-\nrive.Thenwehave 𝑆 = min{𝑀,𝐻},andaccordinglythat 𝑆 ∈ {0,1,...,20}.Letting 𝐹 𝑆 be\ntheCDFof 𝑆 and 𝑠 ∈ {0,1,...,20},write\n𝐹 𝑆(𝑠) = 𝑃(𝑆 ≤ 𝑠) (definitionofCDF)\n= 𝑃(min{𝑀,𝐻} ≤ 𝑠) (definitionof 𝑆)\n= 𝑃(𝑀 ≤ 𝑠∪𝐻 ≤ 𝑠)\n(cid:18) (cid:19)\n= 1− 𝑃 (cid:0)𝑀 ≤ 𝑠∪𝐻 ≤ 𝑠(cid:1)𝐶\n= 1− 𝑃(𝑀 > 𝑠,𝐻 > 𝑠) (DeMorgan’sLaw)\n= 1− 𝑃(𝑀 > 𝑠)𝑃(𝐻 > 𝑠) (𝑀 ⊥ 𝐻)\n(cid:18) 20 (cid:19)(cid:18) ∞ (cid:19)\n∑︁ ∑︁\n= 1− 𝑃(𝑀 = 𝑚) 𝑃(𝐻 = ℎ)\n𝑚=𝑠+1 ℎ=𝑠+1\n(cid:18) ∑︁20\n1\n(cid:19)(cid:18) ∑︁∞ 10ℎ𝑒−10(cid:19)\n= 1−\n21 ℎ!\n𝑚=𝑠+1 ℎ=𝑠+1\n(cid:18) 20−𝑠(cid:19)(cid:18) ∑︁𝑠 10ℎ𝑒−10(cid:19)\n= 1− 1− .\n21 ℎ!\nℎ=0\n3 Burrow Smoke Detectors and Joint Probability Distributions\nBurrowLabshastakenonotherstartupsinthehomesafetyandsecurityspaceandhasrecently\nstartedmarketinganewsmokedetector.Burrow’ssmokedetectorsrelyon𝐶𝑂 sensorsthat\n2\neventuallyfail,andthatfailuretimedictatestheaverageproductlifetimeofthesmokedetector.\n–4–\nBurrowmanufacturesthreequartersofitssmokedetectorsincentralIdaho,andtherestareman-\nufacturedinsuburbanMaine.Anysinglesmokedetector’sproductlifetimecanbemodeledasa\nExponentialrandomvariable.\nEachofthetwolocationssourcesits𝐶𝑂 sensorsfromdifferentsuppliers,sothesmokedetec-\n2\ntorsmanufacturedinMainehaveanaverageproductlifetimeof7yearsandthesmokedetectors\nmanufacturedinIdahohaveanaverageproductlifetimeof6years.Allsmokedetectorsaresold\nonline,soasidefromthefactthatasmokedetectoristhreetimesmorelikelytoshipfromthe\nIdahofacility,youcan’ttellbylookingatasinglesmokedetectorwhereitwasmanufactured.\nLet𝑇 modeltheamountoftimethatpassesuntilthe𝐶𝑂 sensor(andthereforethesmokedetec-\n2\ntor)fails,andlet 𝑀 beadiscreterandomvariablethattakesonthevalueof1forasmokedetec-\ntormanufacturedinMaine,and0otherwise.\na. Presentthecumulativedistributionandprobabilitydensityfunctionsfortherandomvari-\nable𝑇.BothyourCDFandyourPDFshouldbeanalyticfunctionson𝑡.\nTheLawofTotalProbabilityappliestoallprobabilities,includingcumulativeonesrele-\nvanttocontinuousdistributions.Thatmeansthat:\n𝐹 𝑇(𝑡) = 𝑃(𝑇 ≤ 𝑡) = 𝑃(𝑇 ≤ 𝑡|𝑀 = 1) · 𝑃(𝑀 = 1) + 𝑃(𝑇 ≤ 𝑡|𝑀 = 0) · 𝑃(𝑀 = 0)\n1 3\n=\n(1−𝑒−𝑡\/7)\n· +\n(1−𝑒−𝑡\/6)\n·\n4 4\n1 3\n= 1−\n𝑒−𝑡\/7\n−\n𝑒−𝑡\/6\n4 4\n1 3\n𝑓 𝑇(𝑡) = 𝐹 𝑇′(𝑡) = 𝑒−𝑡\/7 + 𝑒−𝑡\/6\n28 24\nOfcourse,thesearealldefinedfornon-negativevaluesof𝑡.\nb. ComputetheprobabilitythatasmokedetectorwasmanufacturedinMaine,giventhatit\nlastsmorethan15years.Ifneeded,youcankeepyouranswerintermsof 𝐹 𝑇(15) or 𝑓 𝑇(15)\nfrompart(a).However,anyconditionalexpressionoftheform 𝑃(·|·) or 𝑓(·|·) mustbe\nevaluated.\nWeonceagainrelyonahybridformofBayes’sTheorem,althoughthistimetheprobabili-\ntiesrequireweintegrateanaccumulationofprobabilitydensitiesonTfortgreaterthan15\n–5–\nhours.\n𝑃(𝑇 > 15|𝑀 = 1)𝑃(𝑀 = 1)\n𝑃(𝑀 = 1|𝑇 > 15) =\n𝑃(𝑇 > 15)\n1 1− 𝑃(𝑇 ≤ 15|𝑀 = 1)\n= ∗\n4 1− 𝑃(𝑇 ≤ 15)\n1 1− (1−𝑒−15\/7)\n= ∗\n4 1− 𝐹 𝑇(15)\n1 𝑒−15\/7\n= ∗\n4 1− 𝐹 𝑇(15)\n= 0.32268\n4 Elections\nWewouldliketoseehowwecouldpredictanelectionbetweentwocandidatesinFrance(Aand\nB),givendatafrom10polls.Foreachofthe10polls,wereportbelowtheirsamplesize,how\nmanypeoplesaidtheywouldvoteforcandidateA,andhowmanypeoplesaidtheywouldvote\nforcandidateB.Notallpollsarecreatedequal,soforeachpollwealsoreportavalue”weight”\nwhichrepresentshowaccuratewebelievethepollwas.Thedataforthisproblemcanbefoundon\ntheclasswebsiteinpolls.csv:\na. First,assumethateachsampleineachpollisanindependentexperimentofwhetherornot\narandompersoninFrancewouldvoteforcandidateA(disregardweights).\n• CalculatetheprobabilitythatarandompersoninFrancevotesforcandidateA.\n• AssumeeachpersonvotesforcandidateAwiththeprobabilityyou’vecalculatedand\notherwisevotesforcandidateB.IfthepopulationofFranceis64,888,792,whatisthe\nprobabilitythatcandidateAgetsmorethanhalfofthevotes?\n–6–\nb. NateSilveratfivethirtyeightpioneeredanapproachcalledthe”PollofPolls”topredict\nelections.ForeachcandidateAorB,wehavearandomvariable 𝑆 𝐴 or 𝑆 𝐵 whichrepresents\ntheirstrengthonelectionnight(likeELOscores).TheprobabilitythatAwinsis 𝑃(𝑆 𝐴 >\n𝑆 𝐵).\n• Identifytheparametersfortherandomvariables 𝑆 𝐴 and 𝑆 𝐵.Both 𝑆 𝐴 and 𝑆 𝐵 arede-\nfinedtobenormalwiththefollowingparameters:\n• Wewillcalculate 𝑃(𝑆 𝐴 > 𝑆 𝐵) bysimulating100,000fakeelections.Ineachfakeelec-\ntion,wedrawarandomsampleforthestrengthofAfrom 𝑆 𝐴 andarandomsample\nforthestrengthofBfrom 𝑆 𝐵.If 𝑆 𝐴 isgreaterthan 𝑆 𝐵,candidateAwins.Whatdowe\nexpecttoseeifwesimulatesomanytimes?Whatdoweactuallysee?\nc. Whichmodel,theonefrom(a)orthemodelfrom(b)seemsmoreappropriate?Whymight\nthatbethecase?OnelectionnightcandidateAwins.Wasyourpredictionfrompart(b)\n”correct”?\na. 𝑃(randompersonvotesforA) = 𝑣𝑜𝑡𝑒𝑠𝑓𝑜𝑟𝐴 = 4881 = 0.655\n𝑡𝑜𝑡𝑎𝑙𝑣𝑜𝑡𝑒𝑠 7453\nNow,letXbethenumberofvotesforcandidateA.WeassumethatX\n˜𝐵𝑖𝑛(64888792,0.655).\n• Sincenissolarge,wecanapproximateXusinganormalY˜𝑁(𝑛𝑝,𝑛𝑝(1− 𝑝)).\n• 𝜇 = 𝑛𝑝 = 42502158.76,Variance= 𝑛𝑝(1− 𝑝) = 14663244.77StdDev=3829.26\n• Votestowin= 64888792 = 32444396\n2\n• 𝑃(Agetsenoughvotes) = 𝑃(𝑋 > 32444396) ≈ 𝑃(𝑌 > 32444396.5) = 1.00\nb. 𝑆 𝐴 ˜𝑁(5.324,16.436)\n𝑆\n𝐵\n˜𝑁(2.926,16.436)\n𝑃(𝑆\n𝐴\n> 𝑆 𝐵) ≈ 0.66\nWecanfigurethisoutthroughsimulationbydrawingfrom 𝑆 𝐴 and 𝑆 𝐵 100,000timesand\nseeinghowoftenthe 𝑆 𝐴 valueisgreaterthanthe 𝑆 𝐵 value.Laterinthequarter,when\nwelearntheconvolutionofindependentGaussians,youwillbeabletofigurethisout\nmathematically,withoutsampling.\nc. Algorithm(a)makesveryfewassumptions,andsimplicitycanbeuseful,butitdoes\nassumethateachvoterisindependent,whichwedefinitelyknowisn’tthecaseinreal\n–7–\nelections.Algorithm(b)allowsustomodelbias(usingtheweightsweincorporated),and\ndoesn’tthinkofeachvoterasnecessarilyindependent. <END>"}
{"prompt":"netflix-readme.txt\n\n###\n\n","completion":" Task:\nYour task is to predict if a user would rate Love Actually with 5 five stars based on their ratings for the 19 other movies. \n\nValues:\nEach row in the train and test set represents one user. Each column represents one movie. All users in the dataset rated all movies in the dataset. Each entry in this dataset is binary. A value of 1 indicates a rating of 5 stars. A value of 0 indicates a rating of 1, 2, 3 or 4 stars. \n\nColumn meaning:\nEach column represents ratings for a particular movie.\n\nPrediction:\nThe variable you are predicting is the binary value for the user's rating of the movie Love Actually.\n\nCredit:\nThis dataset is based on data originally made for the \"Netflix Prize\". The Netflix Prize data was initially retracted because of concerns over user privacy. Reed Hastings, the CEO of Netflix, gave the official thumbs up for CS109 to release this anonymized subsample of data. Thanks to Matt Chen for his help in getting the Netflix Prize data. <END>"}
{"prompt":"cs109_cheat_cheat.txt\n\n###\n\n","completion":" CS109 LaTeX Cheat Sheet\nCreated by Derek Chong for CS109 during Spring 2020.\nThis cheat sheet assumes you have done an introductory tutorial and have a basic level of\ngeneral knowledge. It focuses on giving you a quick reference for language features you will\nencounter in CS109, in order to make your life easier when working on problem sets.\nFrequently-Used Markup\nBuilding Blocks\n\\geq \\leq\n\\neq \\approx\n\\sim \\Rightarrow\n\\sqrt{42} \\infty\n\\lambda \\mu, \\sigma\n\\Phi(0) \\Sigma\n\\theta \\bar{X}, \\hat{X}\nA,B,\\dots,Z 1,2,\\cdots,n\n\\verb|my_function() \\frac{42 \\textrm{\n| units} \\times 42\n\\textrm{ units}}{42\n\\textrm{ units}\n\\times 42 \\textrm{\nunits}}\n\\sum_{i=0}^{n} \\prod_{i=0}^{n}\n\\frac{a}{b} \\frac{a}{b}\nProbability\nP(A \\cap B) P(A \\cup B)\n\\binom{n}{k} P(A_1|B^C)\nP(\\textrm{text } | P(\\textrm{cond}\n\\textrm{ text}) \\leq 5.0)\n\\frac{P(\\textrm{a } \\frac{P(\\textrm{a }\n| \\textrm{ b}) | \\textrm{ b})\n\\times \\times\nP(\\textrm{b})}{P(\\t P(\\textrm{b})}{P(\\t\nextrm{a})} extrm{a } |\n\\textrm{\nb})P(\\textrm{b}) +\nP(\\textrm{a } |\n\\textrm{\nb}^C)P(\\textrm{b}^C\n)}\nRandom Variables\nX \\sim X \\sim\n\\textrm{Ber}(p) \\textrm{Bin}(n,p)\nX \\sim X \\sim\n\\textrm{Poi}(\\lambd \\textrm{Exp}(\\lambd\na=0) a=0)\nX \\sim \\theta \\sim\n\\mathcal{N}(\\mu = \\textrm{Beta}(a,b)\n0, \\sigma^2 = 1)\nCalculus\n\\int_{-1}^{1} x^2 - \\left[ x -\n2x + 1 dx \\frac{1}{2}x^2\n\\right]_{-1}^1\n\\iint_{0<y<x<1} \\left. \\frac{2}{3}y\n\\frac{x}{y} dy dx - \\frac{3}{4}y^2\n\\right|_{-1}^{x}\nVariance and Covariance\n\\textrm{Var}(X) \\textrm{Cov}(X,Y)\n\\begin{bmatrix} a & \\rho\nb \\\\ c & d\n\\end{bmatrix}\n\nUseful Structures\nGroups of Equations\n\\begin{align*} chunks can be used to organise multiple lines of equations.\n​\n\\begin{align*}\nP(X=x|Y=1,W=P_1)\n&= \\frac{P(X=x,Y=1 |\nW=P_1)}{P(Y=1|W=P_1)} \\\\\n&= P(X=x|W=P_1) \\\\\n&= \\binom{5}{x}(0.1)^x(0.9)^{5-x} \\\\\nP(X=x|Y=1,W=P_2)\n&= \\frac{P(X=x,Y=1 |\nW=P_2)}{P(Y=1|W=P_2)} \\\\\n&= P(X=x|W=P_2) \\\\\n&= \\binom{5}{x}(0.1)^x(0.9)^{5-x}\n\\end{align*}\nVenn Diagrams\nYou may have to install the venndiagram package (and include \\usepackage{venndiagram})\n​ ​ ​ ​\n\\begin{venndiagram3sets}[labelA={Foo},labelB={Bar},labelC\n={Baz},\nlabelOnlyA={1},labelOnlyB={2},labelOnlyC={3},\nlabelOnlyAB={4},labelOnlyAC={5},labelOnlyBC={6},labelABC=\n{7},\nlabelNotABC={8}]\n\\begin{center}\n\\begin{venndiagram3sets}[labelA={Foo},labelB={Bar},labelC\n={Baz},\nlabelOnlyA={1},labelOnlyB={2},labelOnlyC={3},\nlabelOnlyAB={4},labelOnlyAC={5},labelOnlyBC={6},labelABC=\n{7},labelNotABC={8},shade={yellow}]\n\\fillOnlyB\\end{venndiagram3sets}\n\\end{center}\n\\begin{center}\n\\begin{venndiagram2sets}[labelA={Foo},labelB={Bar},\nlabelOnlyA={$x$},labelOnlyB={$z$}, labelAB={$y$},\nshade={white}]\n\\fillAll\n\\end{venndiagram2sets}\n\\end{center}\nFull package documentation is available here:\nhttps:\/\/ctan.math.illinois.edu\/macros\/latex\/contrib\/venndiagram\/venndiagram.pdf\nPlotting Graphs\n\\begin{tikzpicture}\n\\begin{axis}[\naxis lines = center,\nxlabel = {$p$},\n]\n\\addplot [domain=-0.05:1.5, samples=100,\ncolor=blue]{x};\n\\addlegendentry{$p$}\n\\addplot [domain=-0.05:1.5, samples=100,\ncolor=red]{x*x};\n\\addlegendentry{$q = p^2$}\n\\end{axis}\n\\end{tikzpicture}\nProbability Trees\n\\tikzstyle{level 1}=[level distance=3.5cm, sibling\ndistance=3cm]\n\\tikzstyle{level 2}=[level distance=3.5cm, sibling\ndistance=3.3cm]\n\\tikzstyle{bag} = [text width=4em, text centered]\n\\tikzstyle{end} = [circle, minimum width=3pt,fill, inner\nsep=0pt]\n\\begin{tikzpicture}[grow=right]\n\\node[bag] {Foo}\nchild {\nnode[bag] {Bar}\nchild {\nnode[end, label=right:{Baz}] {}\nedge from parent\nnode[above] {$X$}\nnode[below] {$x$}\n}\nchild {\nnode[end, label=right:{Xyzzy}] {}\nedge from parent\nnode[above] {$X$}\nnode[below] {$x$}\n}\nedge from parent\nnode[above] {$X$}\nnode[below] {$x$}\n}\nchild {\nnode[end, label=right:{Quux}] {}\nedge from parent\nnode[above] {$X$}\nnode[below] {$x$}\n};\n\\end{tikzpicture}\nTables\n\\setlength{\\tabcolsep}{0.75em} % horizontal padding\n\\def\\arraystretch{1.25} % vertical padding\n\\begin{tabular}{ |c|c|c|c|c|c| }\n\\hline\nFoo & $0$ & $1$ & $2$ & $3$ & $4$ \\\\\n\\hline\nBar & $0.0000$ & $0.0000$ & $0.0000$ & $0.0000$ & $0.0000$ \\\\\n\\hline\nBaz & $0.0000$ & $0.0000$ & $0.0000$ & $0.0000$ & $0.0000$ \\\\\n\\hline\nQuux & $0.0000$ & $0.0000$ & $0.0000$ & $0.0000$ & $0.0000$ \\\\\n\\hline\nXyzzy & $0.00\\%$ & $0.00\\%$ & $0.00\\%$ & $0.00\\%$ & $0.00\\%$ \\\\\n\\hline\n\\end{tabular}\nPython Code\nThis code requires you to install the pythonhiglight package and include\n​ ​\n\\usepackage{pythonhighlight} in your header.\n​\nhttps:\/\/github.com\/olivierverdier\/python-latex-highlighting\n\\begin{python}\nsome_var = 42 # example comment\nfor n in range(5):\nprint(\"Hello world!\")\n\\end{python}\nGeneral Tips\n● Local LaTeX: Running LaTeX locally can help you learn faster than Overleaf - a shorter\n​\nfeedback loop is really helpful!\n● Wolfram Alpha: You can paste LaTeX snippets directly into Wolfram Alpha and most of\n​\nthe time it'll understand them correctly and do something useful.\n○ You can even add Wolfram Alpha as a custom search engine in Chrome. This\nlets you type \"w [your LaTeX equation]\" into your location bar, and it'll take you\nstraight to the answer in Wolfram. Just go to this link, and add this as an entry.\n​ ​ ​ ​ ​ ​\n● Half-LaTeX environments: Using a half-LaTeX environment like MS Word, Powerpoint,\n​\nor Google Docs with the Auto-LaTeX Equations Addon is super useful while you’re doing\n​ ​\nyour rough work.\n○ Save up snippets for typesetting and test them in Wolfram Alpha as you go!\n○ Auto-LaTeX may throw an error if you’re signed into >1 Google Account at once\n○ MS Office LaTeX support was added in 2016, is not yet available on macOS\n● Keyboard shortcuts:\n​\n○ On TeXShop, you can type \\bali and press Esc twice, and it'll set up a\n\\begin{align*} block. Or \\b[xyz] for any \\begin{xyz} block.\n○ It can be nice to define custom shortcuts for things you find yourself typing\nfrequently, such as \\textrm{} or Bayes’ Theorem.\n​ ​\n● Expectations: You don’t have to use any of the above structures if you don’t want to!\n​\nThe teaching team will accept and grade PSets in any format you submit.\n● Motivation: Strong LaTeX skills will make you more effective in all future courses. Goals\n​ ​ ​ ​ <END>"}
{"prompt":"06_section_soln_clt.txt\n\n###\n\n","completion":" –1–\nCS109 May16,2024\nContinuous Joint Distributions, Central Limit Theorem\nBeforeyouleavelab,makesureyouclickheresothatyou’remarkedashavingattendedthis\nweek’ssection.TheCAleadingyourdiscussionsectioncanenterthepasswordneededonce\nyou’vesubmitted.\n1 Warmups\n1.1 Food for Thought\nKarelthedogeatsanunpredictableamountoffood.Everyday,thedogisequallylikelytoeat\nbetweenacontinuousamountintherange100to300g.HowmuchKareleatsisindependentofall\notherdays.Youonlyhave6.5kgoffoodforthenext30days.Whatistheprobabilitythat6.5kg\nwillbeenoughforthenext30days?\nThe distribution of the sum is given by the central limit theorem. Let 𝑋 𝑖 ∼ Uni(100,300)\nwhere 𝐸[𝑋 𝑖] = 200and𝑉𝑎𝑟(𝑋 𝑖) = 1 (200)2 ≈ 3333.\n12\n∑︁\n𝑌 = 𝑋 𝑖\n𝑖\nLet’sapproximate𝑌 withanormalR.V.\n∼ N(6000,316.2122)\n𝑃(𝑌 < 6500)\n(cid:18)𝑌\n−6000\n6500−6000(cid:19)\n𝑃 <\n316.212 316.212\nLet 𝑌−6000 = 𝑍 ∼ N(0,1)\n316.212\n(cid:18) (cid:19)\n6500−6000\n𝑃 𝑍 <\n316.212\n𝑃(𝑍 < 1.58)\nΦ(1.58)\n–2–\n1.2 Sample and Population Mean\nComputingthesamplemeanissimilartothepopulationmean:sumallavailablepointsanddivide\nbythenumberofpoints.However,samplevarianceisslightlydifferentfrompopulationvariance.\n1. Considertheequationforpopulationvariance,andananalogousequationforsample\nvariance.\n𝑁\n1 ∑︁\n𝜎2 = (𝑥 𝑖 − 𝜇)2\n𝑁\n𝑖=1\n𝑛\n1 ∑︁\n𝑆2 𝑏𝑖𝑎𝑠𝑒𝑑 = 𝑛 (𝑋 𝑖 − 𝑋¯)2\n𝑖=1\n𝑆2 isarandomvariabletoestimatetheconstant 𝜎2.Becauseitisbiased,\n𝑏𝑖𝑎𝑠𝑒𝑑\n𝐸[𝑆2 ] ≠ 𝜎2.Is 𝐸[𝑆2 ] greaterorlessthan 𝜎2?\n𝑏𝑖𝑎𝑠𝑒𝑑 𝑏𝑖𝑎𝑠𝑒𝑑\n2. ConsideranalternativeRandomVariable, 𝑆2 (knownsimplyas 𝑆2 inclass).The\n𝑢𝑛𝑏𝑖𝑎𝑠𝑒𝑑\ntechniqueofun-biasingvarianceisknownasBessel’scorrection.Writethe 𝑆2\n𝑢𝑛𝑏𝑖𝑎𝑠𝑒𝑑\nequation.\na. 𝐸[𝑆2 ] < 𝜎2. The intuition is that the spread of a sample of points is generally\nbiased\nsmallerthanthespreadofallthepointsconsideredtogether.Thisbecomesmoreclear\nwhenweconsidertheunbiasedversionandhowitmakestheexpressionevaluatetoa\nlargernumber.\nb. 𝑆 u2\nnbiased\n= 𝑆2 = 𝑛−1\n1\n(cid:205) 𝑖𝑛 =1(𝑋 𝑖 − 𝑋¯)2\n2 Problems\n2.1 Sum of Two Exponentials\nConsidertwoindependentrandomvariables 𝑋 and𝑌,eachExponentialswithdifferent\nparameters—specifically,let 𝑋 ∼ 𝐸𝑥𝑝(1) and𝑌 ∼ 𝐸𝑥𝑝(1).Assuming𝑇 = 𝑋 +𝑌,deriveand\n2 3\npresenttheprobabilitydensityfunction 𝑓 𝑇(𝑡) byevaluatingtherelevantconvolution.Onceyou\narriveatyour 𝑓 𝑇(𝑡),verifyyouranswerbycalculating 𝑓 𝑇(2) outtothreedecimalplaces.\nIf we let 𝑋 and𝑌 be continuous random variables with probability density functions 𝑓 𝑋(𝑡)\nand 𝑓 𝑌(𝑡),thentheprobabilitydensityfunctionand 𝑓 𝑇(𝑡) of𝑇 = 𝑋+𝑌 istheconvolutionof\n𝑓 𝑋(𝑡) and 𝑓 𝑌(𝑡)—thatis:\n∫ ∞\n𝑓 𝑇(𝑡) = 𝑓 𝑋(𝑥)𝑓 𝑌(𝑡 −𝑥)𝑑𝑥\n−∞\nInthecaseofthisproblem,both 𝑋 and𝑌 areExponentialswithsupportsofallnonnegative\nreal numbers, so the bounds of the integral can be compressed to include just those values\n–3–\nwhere both 𝑥 and 𝑡 −𝑥 are greater than 0 (which is [0,𝑡]). Of course, the density function\nforageneralExponentialis 𝑓 𝑋(𝑥) = 𝜆𝑒−𝜆𝑥 ,sotheconvolutiontobeevaluatedshouldbe:\n∫ 𝑡\n𝑓 𝑇(𝑡) = 𝑓 𝑋(𝑥)𝑓 𝑌(𝑡 −𝑥)𝑑𝑥\n0\n∫ 𝑡\n1 1\n=\n𝑒−1 2𝑥\n·\n𝑒−1 3(𝑡−𝑥)𝑑𝑥\n2 3\n0\n∫ 𝑡\n1\n=\n𝑒−1 3𝑡 𝑒−1 6𝑥𝑑𝑥\n6\n0\n(cid:12)𝑡\n= −𝑒− 31𝑡𝑒−1 6𝑥(cid:12)\n(cid:12)\n0\n=\n𝑒−1 3𝑡 (1−𝑒−1 6𝑡\n)\n=\n𝑒−1 3𝑡 −𝑒−1 2𝑡\nThat’stheprobabilitydensityfunctionofinterest,anditsvalueat𝑡 = 2is 𝑓 𝑇(2) = 0.145537.\n2.2 Grading Exams\nJacobandKathleenareplanningtogradeProblem1onyourWeek7exam,andthey’lleachgrade\ntheirhalfindependentlyoftheother.Jacobtakes 𝑋 ∼ 𝐸𝑥𝑝(1) hourstofinishhishalfwhile\n3\nKathleentakes𝑌 ∼ 𝐸𝑥𝑝(1) hourstofinishhishalf.\n4\na. FindtheCDFof 𝑋\/𝑌,whichistheratiooftheirgradingcompletiontimes.\nThe random variable of interest is the ratio 𝑋\/𝑌, so the CDF, 𝐹(𝑟), in this case would be\n𝑃(𝑋 < 𝑟), where𝑟 stands for ratio and ranges from 0 to ∞. Rearranging, we are interested\n𝑌\n–4–\nincomputing 𝑃(𝑋 < 𝑟𝑌),whichcanbecomputedintermsofthePDFsfor 𝑋 and𝑌:\n𝑋\n𝐹(𝑟) = 𝑃( < 𝑟) = 𝑃(𝑋 < 𝑟𝑌)\n𝑌\n∫ ∞∫ 𝑟𝑦\n1\n=\n𝑒−1 3𝑥𝑒−1 4𝑦𝑑𝑥𝑑𝑦\n12\n0 0\n1 ∫ ∞ ∫ 𝑟𝑦\n=\n𝑒−1 4𝑦 𝑒−1 3𝑥𝑑𝑥𝑑𝑦\n12\n0 0\n1 ∫ ∞ (cid:16) (cid:17)(cid:12)𝑟𝑦\n= −\n4 0\n𝑒−1 4𝑦 𝑒− 31𝑥 (cid:12)\n(cid:12) 0\n𝑑𝑦\n1 ∫ ∞ (cid:16) (cid:17)\n= − 𝑒−1 4𝑦 𝑒− 31𝑟𝑦 −1 𝑑𝑦\n4\n0\n1 ∫ ∞ 1 ∫ ∞\n=\n𝑒−1 4𝑦𝑑𝑦\n−\n𝑒−(1 3𝑟+1 4)𝑦𝑑𝑦\n4 4\n0 0\n1 (cid:12)∞\n= 1+ 1𝑟4\n+\n1𝑒−(1 3𝑟+1 4)𝑦(cid:12)\n(cid:12)\n0\n3 4\n1 1𝑟 + 1 1\n= 1− 4 = 3 4 − 4\n1𝑟 + 1 1𝑟 + 1 1𝑟 + 1\n3 4 3 4 3 4\n1𝑟\n= 3\n1𝑟 + 1\n3 4\nFor those question why that first of two integrals vanished to 1, note that the integrand is\njustthePDFofExpo(𝜆 = 1)!\n4\nIncidentally, we can compute the probability density function from the CDF by dif-\nferentiatingwithrespectto𝑟:\n𝑑𝐹(𝑟)\n𝑓(𝑟) =\n𝑑𝑟\n𝑑 1𝑟\n= 3\n𝑑𝑟 1𝑟 + 1\n3 4\n1\n=\n12(1𝑟 + 1)2\n3 4\nb. WhatistheprobabilitythatKathleenfinishesbeforeJacobdoes?\nIn comparison, that is delightfully straightforward, because we get to plug 𝑟 = 1 into our\nresult from part a. 𝑃(𝑋 < 𝑌) = 1 · 12 = 4. That, however, is the probability that Jacob\n3 7 7\nfinishing before Kathleen, and we want to opposite. Therefore, the probability of interest\n–5–\nis really 3. Given the expected completion times of 3 and 4 hours for Jacob and Kathleen,\n7\nrespectively,thisseemsright.\n2.3 Central Limit Theorem and Sampling Calisthenics\na. Let 𝑋 , 𝑋 , 𝑋 ,..., 𝑋 beiid—thatis,independentandidenticallydistributed—suchthat\n1 2 3 1000\n𝑋 𝑖 ∼ NegBin(𝑟 = 10, 𝑝 = 0.5),andlet𝑊 = 𝑋 1 + 𝑋 2 +...+ 𝑋 1000.AccordingtotheCentral\nLimitTheorem,whatdistributiondoes𝑊 assume,andwhatareitsparameters?\nThis is classic Central Limit Theorem where the distribution of the sum is a Gaussian with\nmean1000𝐸[𝑋 𝑖] andvariance1000𝑉𝑎𝑟(𝑋 𝑖).TheformulasforaNegativeBinomial’smean\nandvariancearewell-definedandarecomputedas:\n𝑟 10\n𝐸[𝑋 𝑖] = = = 20\n𝑝 0.5\n𝑟(1− 𝑝) 10·0.5\n𝑉𝑎𝑟(𝑋 𝑖) = = = 20\n𝑝2 0.52\nHow neat is it that the mean and variance are the same? This all means that 𝑊 ∼\nN(20000,20000).\nb. Define 𝑋¯ = 101\n00\n(cid:205) 𝑖1 =0 100 𝑋 𝑖 tobethesamplemeanofour1000iidsamples.Whatisthe\nstandarddeviationoftherandomvariable 𝑋¯?\nThe Central Limit Theorem has a lot to say about the distribution of sample means as well.\nIn particular, for this problem, 𝑋¯ ∼ N(𝐸[𝑋 𝑖],𝑉𝑎𝑟[𝑋 𝑖] ). That’s more than we’re asking—all\n1000\nIneedfromyouisthat𝑉𝑎𝑟(𝑋 𝑖) = 20 = 0.02.\n1000\nc. Youcomputethevarianceofyour1000samples, 𝑋 , 𝑋 , 𝑋 ,..., 𝑋 accordingtothe\n1 2 3 1000\ntraditionaldefinitionofvariance—i.e. 101\n00\n(cid:205) 𝑖1 =0 100 (𝑋 𝑖 − 𝑋¯)2.Doyouexpectthisvarianceto,\nmoreoftenthannot,belarger,equalto,orsmallerthanthevarianceofNegBin(10,0.5).\nExplainyouranswer.\nHerethepopulationvarianceis20,sinceweknowthepopulationdistributionistheNegative\nBinomial.Recallthattheunbiasedsamplevariancedividesthesumofthedifferencessquared\nby𝑛−1,or999.Thetraditionallycomputedvariancedividesbyaslightlylargernumberof\n1000, so we expect the traditionally computed variance to, more often than not, be a little\ntoolow.\nd. ThenumberofsamplesneededfortheCentralLimitTheoremtoapplyisgenerally\nunderstoodtobe30ormore.However,theCentralLimitTheoremworkswellforaneven\nsmallernumberofsampleswhen 𝑋 𝑖 ∼ Bin(10,0.5) thanisdoeswhen\n𝑋\n𝑖\n∼ NegBin(10,0.5).Brieflyexplainwhy.\n–6–\nThe simple answer is that 𝑋 𝑖 ∼ Bin(10,0.5) is symmetric, so there are no asymmetries\nto overcome as you add samples together. In fact, you can view, say, 3 Bin(10,0.5) as 30\nBer(0.5).\ne. Recallthatsamplingtheoryallowsareasonablylargesampletostandinforthetrue\npopulationdistribution.Whenresamplingfromthesampleforbootstrappingpurposes,we\ngenerallydosowithreplacement.Whywithreplacementinsteadof without?\nWe sample with replacement because we treat the original set of samples as a probability\nmass function. If we were to sample without replacement, we’re incapable of creating\nresamples of a size larger than the original sample, and when the size of the resample is\nclosetothesizeoftheoriginal,eachresamplewouldessentiallybeareplicaoftheoriginal. <END>"}
{"prompt":"cs109_lec17_conceptcheck.txt\n\n###\n\n","completion":" Lecture 17: Concept Check  Graded\nStudent\nNeetish Sharma\nTotal Points\n2 \/ 2 pts\nQuestion 1\nQuestion 1 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 2\nRatios of probabilities 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQ1 Question 1\n1 Point\nSuppose we have two independent normal random variables, X and Y . Let\nX ∼ N(μ = 2,σ2 = 10) and Y ∼ N(μ = 4,σ2 = 1). How is X + Y\ndistributed?\nX + Y ∼ N(2, 10 + 1)\nX + Y ∼ N(2,11)\nX + Y ∼ N(3,5.5)\nX + Y ∼ N(6,11)\nX + Y ∼ Poi( 10 + 1)\nQ2 Ratios of probabilities\n1 Point\nLet X ∼ Exp(λ) with PDF given by f(x). How much more likely is P(X = 3)\ncompared to P(X = 30)?\nf(3)\nf(30)\nf(1)\nf(10)\nf(30)\nf(3)\nf(10)\nf(1)\nundefined, since P(X = 3) = 0 and P(X = 30) = 0\n1 since P(X = 3) = P(X = 30) = 0 <END>"}
{"prompt":"cs109_lec03_conceptcheck.txt\n\n###\n\n","completion":" Lecture 3: Concept Check  Graded\nStudent\nNeetish Sharma\nTotal Points\n4 \/ 4 pts\nQuestion 1\nCombinatorics III 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 2\nEvent Spaces: Dice 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 3\nCounting and Probability: Poker Hands 2 \/ 2 pts\n3.1 Counting: Poker Hands 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\n3.2 Probability: Poker Hands 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQ1 Combinatorics III\n1 Point\nHow many solutions are there to the equation x + x + x = 10, given the\n1 2 3\nconstraint that all x i need to be positive integers? Express your answer as a\nsimple integer.\n36\nQ2 Event Spaces: Dice\n1 Point\nSuppose you roll two dice. Which of the following definitions of events E and F\nare mutually exclusive? Select all that apply.\nE: you roll exactly 1 one, F: you roll exactly 1 two\nE: you roll exactly 1 one, F: you roll exactly 2 twos\nE: dice 1 outcome is even, F: dice 2 outcome is odd\nE: dice 1 outcome is even, F: dice 1 outcome is odd\nQ3 Counting and Probability: Poker Hands\n2 Points\nQ3.1 Counting: Poker Hands\n1 Point\nA 52-card deck has 4 suits (Diamond\/Club\/Heart\/Spade), each with 13 ranks (Ace,\n2, 3, …, 9, 10, Jack, Queen, King). How many unordered 5-card hands would result\nin a three of a kind in poker using a standard 52-card deck? A three of a kind\noccurs when the cards have ranks a, a, a, b, c, where a, b and c are all distinct.\nExample unordered three-of-a-kind hand: (Jack Club, Jack Spade, Jack Heart, 9\nHeart, 10 Heart).\n13 ⋅ (4 ) ⋅ (12 ) ⋅ 42 = 54,912\n3 2\n13 ⋅ (4 ) ⋅ (48 ) = 58,656\n3 2\n13 ⋅ 12 ⋅ 11 ⋅ 10 ⋅ 9 = 154,440\n12\n13 ⋅ ( ) = 858\n2\nQ3.2 Probability: Poker Hands\n1 Point\nIf you are dealt 5 cards from a 52-card deck, what is the probability of getting a\nthree of a kind? Please provide a decimal answer (e.g. 0.0034), with a leading\nzero.\n0.0211 <END>"}
{"prompt":"cs109_lec14_conceptcheck.txt\n\n###\n\n","completion":" Lecture 14: Concept Check  Graded\nStudent\nNeetish Sharma\n View or edit group\nTotal Points\n4 \/ 4 pts\nQuestion 1\nImplications of Independence 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 2\nImplications of Dependence 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 3\nConditional Expectation and Rolls of a Single Die 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 4\nConditional Variance and Rolls of a Single Die 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQ1 Implications of Independence\n1 Point\nSuppose X and Y are independent. Which of the following is true? Check all that\napply.\nE[XY ] = E[X]E[Y ]\nE[X + Y ] = E[X] + E[Y ]\nV ar(X + Y ) = V ar(X) + V ar(Y )\nQ2 Implications of Dependence\n1 Point\nSuppose X and Y are dependent. Which of the following is true for all X and Y\nthat are dependent? Check all that apply.\nE[XY ] = E[X]E[Y ]\nE[X + Y ] = E[X] + E[Y ]\nV ar(X + Y ) = V ar(X) + V ar(Y )\nQ3 Conditional Expectation and Rolls of a Single Die\n1 Point\nConsider an experiment where you repeatedly roll a single fair die until you get a\n4. Let X be the number of rolls needed until you get that 4 (e.g. X = 7 if the\nseventh roll produces the first 4), and let Y count the number of 3's you see\nalong the way. What is E[Y ∣X = x]?\nx\n6\n1 + x\n6\nx\n1 +\n5\n1(x − 1)\n5\n1(x + 1)\n5\n1(x − 1)\n6\n1(x + 1)\n6\nQ4 Conditional Variance and Rolls of a Single Die\n1 Point\nLeveraging the same experiment used in Question 3, the conditional variance is\ngiven by V ar(Y ∣X = x) = k(x − 1). What is the value of k?\nThe answer is a real number between 0 and 1, which you should enter it as a\ndecimal to two significant digits.\n0.16 <END>"}
{"prompt":"cs109_lec15_conceptcheck.txt\n\n###\n\n","completion":" Lecture 15: Concept Check  Graded\nStudent\nNeetish Sharma\nTotal Points\n2 \/ 2 pts\nQuestion 1\nBayesian Networks 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 2\nBayesian Inference 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQ1 Bayesian Networks\n1 Point\nSelect all that are true of Bayesian networks.\nThey model the conditional dependences of random variables as edges in\na graph, where the nodes represent the random variables.\n→\nIf A and B are nodes in a Bayesian network with an edge A B, then B is\nconditionally dependent on A.\nThey are useful for solving inference questions.\n→ →\nIf A and B share a parent C (with edges C A and C B), then then A\nand B are conditionally independent.\nQ2 Bayesian Inference\n1 Point\nHow do Bayesian networks help us answer inference questions?\nThey allow us to avoid the calculation of the entire joint probability table\nof all variables.\nThey help us more easily track conditional dependencies, which are\nessential for evaluating conditional probabilities.\nThey increase computation time of conditional probabilities.\nWe can implement algorithms that take advantage of the Bayesian\nnetworks to solve inference problems. <END>"}
{"prompt":"cs109_lec10_conceptcheck.txt\n\n###\n\n","completion":" Lecture 10: Concept Check  Graded\nStudent\nNeetish Sharma\nTotal Points\n4 \/ 4 pts\nQuestion 1\nStanford Student Heights 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 2\nMeal Plan Dollars 3 \/ 3 pts\n2.1 Meal Plan Dollars: Expectation 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\n2.2 Meal Plan Dollars: Variance 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\n2.3 Meal Plan Dollars: Linear Combinations of Gaussians 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQ1 Stanford Student Heights\n1 Point\nAssume the heights of all Stanford students are normally distributed with mean\n175 cm and variance 16 cm2 . Restated, if X is the height of a random Stanford\nstudent, X ∼ N(μ = 175,σ2 = 16).\nApproximately what percentage of Stanford student heights would fall within\none standard deviation of the mean, i.e., in the interval [171, 179]?\n90%\n50%\n68%\n20%\nQ2 Meal Plan Dollars\n3 Points\nNow, consider that Stanford starts instituting a plan where the number of meal\nplan dollars a student receives each quarter is Y = 3X + 1400, where X is the\nheight (in cm) of the student and follows the distribution in Question 1. (Yes, this\nis a contrived scenario and the university does not actually do this. At least yet!)\nQ2.1 Meal Plan Dollars: Expectation\n1 Point\nIf you are a Stanford student, what is the expected number of meal plan dollars\nyou receive in a quarter?\n1925\nQ2.2 Meal Plan Dollars: Variance\n1 Point\nIf you are a Stanford student, what is the variance of the meal plan dollars you\nreceive in a quarter?\n144\nQ2.3 Meal Plan Dollars: Linear Combinations of Gaussians\n1 Point\nWhat is the probability that you receive more than $1937 in meal dollars in a\nquarter?\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20 <END>"}
{"prompt":"12_independent_rvs_annotated.txt\n\n###\n\n","completion":" 12: Independent RVs\nJerry Cain\nApril 26th, 2024\nLecture Discussion on Ed\n1\nSums of\nindependent\nBinomial RVs\n2\nIndependent discrete RVs\nRecall the definition of independent\n𝑃 𝐸𝐹 = 𝑃 𝐸 𝑃 𝐹\nevents 𝐸 and 𝐹:\nTwo discrete random variables 𝑋 and 𝑌 are independent if:\nfor all 𝑥, 𝑦:\n𝑃 𝑋 = 𝑥, 𝑌 = 𝑦 = 𝑃 𝑋 = 𝑥 𝑃 𝑌 = 𝑦\nDifferent notation,\n𝑝 𝑥, 𝑦 = 𝑝 𝑥 𝑝 𝑦\n!,# ! #\nsame idea:\n• Intuitively: knowing value of 𝑋 tells us nothing about\nthe distribution of 𝑌 (and vice versa)\n• If two variables are not independent, they are termed dependent.\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 3\nSum of independent Binomials\n𝑋~Bin(𝑛 , 𝑝)\n!\n𝑋 + 𝑌 ~Bin(𝑛 + 𝑛 , 𝑝)\n𝑌~Bin(𝑛 , 𝑝)\n! \"\n\"\n𝑋, 𝑌 independent\nIntuition:\n• Each trial in 𝑋 and 𝑌 is independent and has same success probability 𝑝\n• Define 𝑍 =# successes in 𝑛 + 𝑛 independent trials, each with success\n! \"\nprobability 𝑝. 𝑍~Bin 𝑛 + 𝑛 , 𝑝 and 𝑍 = 𝑋 + 𝑌 as well.\n! \"\nHolds in general case:\n’ ’\nIf only it were\n𝑋 ~Bin(𝑛 , 𝑝) + 𝑋 ~Bin(+ 𝑛 , 𝑝)\n# # $ $ always so simple\n𝑋 independent for 𝑖 = 1, … , 𝑛\n# $%& $%&\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 4\nCoin flips\nFlip a coin with probability 𝑝 of heads a total of 𝑛 + 𝑚 times.\nLet 𝑋 = number of heads in first 𝑛 flips. 𝑋~Bin(𝑛, 𝑝)\n𝑌 = number of heads in next 𝑚 flips. 𝑌~Bin 𝑚, 𝑝\n𝑍 = total number of heads in 𝑛 + 𝑚 flips.\n1. Are 𝑋 and 𝑍 independent?\n2. Are 𝑋 and 𝑌 independent?\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 5\nCoin flips\nFlip a coin with probability 𝑝 of heads a total of 𝑛 + 𝑚 times.\nLet 𝑋 = number of heads in first 𝑛 flips. 𝑋~Bin(𝑛, 𝑝)\n𝑌 = number of heads in next 𝑚 flips. 𝑌~Bin(𝑚, 𝑝)\n𝑍 = total number of heads in 𝑛 + 𝑚 flips.\n1. Are 𝑋 and 𝑍 independent? ❌ Counterexample: What if 𝑍 = 0?\n2. Are 𝑋 and 𝑌 independent? ✅\n# of mutually exclusive 𝑛 𝑚\n∶\nfirst 𝑛 flips have 𝑥 heads outcomes in event 𝑥 𝑦\n𝑃 𝑋 = 𝑥, 𝑌 = 𝑦 = 𝑃\nand next 𝑚 flips have 𝑦 heads 𝑃 each outcome\n𝑛 𝑚 = 𝑝! 1 − 𝑝 \"#!𝑝$ 1 − 𝑝 %#$\n$ %&$ ’ (&’\n= 𝑝 1 − 𝑝 𝑝 1 − 𝑝\n𝑥 𝑦\nThis probability (found through\ncounting) is the product of the\n= 𝑃 𝑋 = 𝑥 𝑃 𝑌 = 𝑦\nmarginal PMFs.\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 6\nConvolution:\nSum of\nindependent\nPoisson RVs\n7\nConvolution: Sum of independent random variables\nFor any discrete random variables 𝑋 and 𝑌:\n𝑃 𝑋 + 𝑌 = 𝑛 = + 𝑃 𝑋 = 𝑘, 𝑌 = 𝑛 − 𝑘\n3\nIn particular, for independent discrete random variables 𝑋 and 𝑌:\n𝑃 𝑋 + 𝑌 = 𝑛 = + 𝑃 𝑋 = 𝑘 𝑃 𝑌 = 𝑛 − 𝑘\n3\nthe convolution of 𝑝 and 𝑝\n) *\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 8\nInsight into convolution\nFor independent discrete random variables 𝑋 and 𝑌:\nthe convolution\n𝑃 𝑋 + 𝑌 = 𝑛 = ’ 𝑃 𝑋 = 𝑘 𝑃 𝑌 = 𝑛 − 𝑘\nof 𝑝 and 𝑝\n) *\n!\nSuppose 𝑋 and 𝑌 are independent, both with support 0, 1, … , 𝑛, … :\n𝑋\n0 1 2 … 𝑛 𝑛 + 1 …\n• ✔: event where 𝑋 + 𝑌 = 𝑛\n0\n✔\n• Each event has probability:\n…\n…\n𝑃 𝑋 = 𝑘, 𝑌 = 𝑛 − 𝑘\n𝑛 − 2 ✔\n= 𝑃 𝑋 = 𝑘 𝑃 𝑌 = 𝑛 − 𝑘\n𝑌 𝑛 − 1\n✔\n(because 𝑋,𝑌 are independent)\n𝑛\n✔ • 𝑃 𝑋 + 𝑌 = 𝑛 = sum of\n𝑛 + 1\nmutually exclusive events\n…\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 9\nSum of 2 dice rolls\n6\/36\n5\/36\n4\/36\n3\/36\n2\/36\n1\/36\n0\n2 3 4 5 6 7 8 9 10 11 12\n% + ' = )\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 10\n)\n=\n'\n+\n%\n.\nThe distribution of a sum of\n2 dice rolls is a convolution\nof 2 PMFs.\nExample:\n𝑃 𝑋 + 𝑌 = 4 =\n𝑃 𝑋 = 1 𝑃 𝑌 = 3\n+ 𝑃 𝑋 = 2 𝑃 𝑌 = 2\n+ 𝑃 𝑋 = 3 𝑃 𝑌 = 1\nSum of 10 dice rolls (fun preview)\n0.08\nThe distribution of a sum of\n0.06\n10 dice rolls is a convolution\n0.04\n10 PMFs.\n0.02\n0\n10 20 30 40 50 60\n𝑋 + 𝑋 + ⋯ + 𝑋 = 𝑛\n! \" !3\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 11\n𝑛\n=\n𝑋\n+\n⋯\n+\n𝑋\n+\n𝑋\n𝑃\n3!\n\"\n!\nLooks kinda Normal…???\n(more on this in a few weeks)\nSum of independent Poissons\n𝑋~Poi 𝜆 , 𝑌~Poi 𝜆\n! \" 𝑋 + 𝑌 ~Poi(𝜆 + 𝜆 )\n! \"\n𝑋, 𝑌 independent\nProof (just for reference):\n𝑋 and 𝑌 independent,\n𝑃 𝑋 + 𝑌 = 𝑛 = ; 𝑃 𝑋 = 𝑘 𝑃 𝑌 = 𝑛 − 𝑘\nconvolution\n4\n% %\n4 %&4 4 %&4\n𝜆 𝜆 𝜆 𝜆\n! \" ! \"\n= ; 𝑒&6 ! 𝑒&6 \" = 𝑒&(6 !86 \") ; PMF of Poisson RVs\n𝑘! (𝑛 − 𝑘)! 𝑘! (𝑛 − 𝑘)!\n453 453\n%\nBinomial Theorem:\n& 6 86 & 6 86\n𝑒 ! \" 𝑛! 𝑒 ! \"\n!\n4 %&4 %\n= ; 𝜆 𝜆 = 𝜆 + 𝜆 𝑛\n𝑛! 𝑘! (𝑛 − 𝑘)! ! \" 𝑛! ! \" 𝑎 + 𝑏 ! = = 𝑎\"𝑏!%\"\n𝑘\n453\n\"#$\nPoi 𝜆 + 𝜆\n! \"\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 12\nSum of independent Poissons\n𝑋~Poi 𝜆 , 𝑌~Poi 𝜆\n! \" 𝑋 + 𝑌 ~Poi(𝜆 + 𝜆 )\n! \"\n𝑋, 𝑌 independent\n• 𝑛 servers with independent number of requests\/minute\n• Server 𝑖’s requests each minute can be modeled as 𝑋 ~Poi 𝜆\n\" \"\nWhat is the probability that the total number of web requests received at all\nservers in the next minute exceeds 10?\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 13\nExercises\n14\nIndependent questions\n1. Let 𝑋~Bin 30, 0.01 and 𝑌~Bin 50, 0.02 be independent RVs.\n• How do we compute 𝑃 𝑋 + 𝑌 = 2 using a Poisson approximation?\n• How do we compute 𝑃 𝑋 + 𝑌 = 2 exactly?\n2. Let 𝑁 = # of requests to a web server per day. Suppose 𝑁~Poi 𝜆 .\n• Each request independently comes from a human (prob. 𝑝), or bot (1 − 𝑝).\n• Let 𝑋 be # of human requests\/day, and 𝑌 be # of bot requests\/day.\nAre 𝑋 and 𝑌 independent? What are their marginal PMFs?\n🤔\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 15\n1. Approximating the sum of independent Binomial RVs\nLet 𝑋~Bin 30, 0.01 and 𝑌~Bin 50, 0.02 be independent RVs.\n• How do we compute 𝑃 𝑋 + 𝑌 = 2 using a Poisson approximation?\n• How do we compute 𝑃 𝑋 + 𝑌 = 2 exactly?\n\"\n𝑃 𝑋 + 𝑌 = 2 = ; 𝑃 𝑋 = 𝑘 𝑃 𝑌 = 2 − 𝑘\n453\n)\n30 50\n= : 0.01& 0.99 *(#& 0.02)#&0.98+(#()#&)≈ 0.2327\n𝑘 2 − 𝑘\n&’(\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 16\n2. Web server requests\nLet 𝑁 = # of requests to a web server per day. Suppose 𝑁~Poi 𝜆 .\n• Each request independently comes from a human (prob. 𝑝), or bot (1 − 𝑝).\n• Let 𝑋 be # of human requests\/day, and 𝑌 be # of bot requests\/day.\nAre 𝑋 and 𝑌 independent? What are their marginal PMFs?\n𝑃 𝑋 = 𝑥, 𝑌 = 𝑦 = 𝑃 𝑋 = 𝑥, 𝑌 = 𝑦 𝑁 = 𝑥 + 𝑦 𝑃 𝑁 = 𝑥 + 𝑦 Law of Total\nProbability\n+𝑃 𝑋 = 𝑥, 𝑌 = 𝑦 𝑁 ≠ 𝑥 + 𝑦 𝑃 𝑁 ≠ 𝑥 + 𝑦\n= 𝑃 𝑋 = 𝑥 𝑁 = 𝑥 + 𝑦 𝑃 𝑌 = 𝑦| 𝑋 = 𝑥, 𝑁 = 𝑥 + 𝑦 𝑃 𝑁 = 𝑥 + 𝑦 Chain Rule\n$8’\n𝜆\n𝑥 + 𝑦 Given 𝑁 = 𝑥 + 𝑦 indep. trials,\n$ ’ &6\n= 𝑝 1 − 𝑝 ⋅ 1 ⋅ 𝑒\n𝑥 𝑥 + 𝑦 ! 𝑋|𝑁 = 𝑥 + 𝑦~Bin 𝑥 + 𝑦,𝑝\n$ $\n𝑥 + 𝑦 ! 𝜆𝑝 # 𝜆 1 − 𝑝 𝜆𝑝 # 𝜆 1 − 𝑝\n= 𝑒!\" = 𝑒!\"% ⋅ 𝑒!\" &!%\n𝑥! 𝑦! 𝑥 + 𝑦 ! 𝑥! 𝑦!\nYes, 𝑋 and 𝑌 are\nwhere 𝑋~Poi 𝜆𝑝 ,𝑌~Poi 𝜆 1 − 𝑝\n= 𝑃 𝑋 = 𝑥 𝑃 𝑌 = 𝑦 independent!\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 17\nExpectation of\nCommon RVs\n18\nLinearity of Expectation: Important\n’\nExpectation is a linear mathematical operation. If 𝑋 = ∑ 𝑋 :\n$%& $\n8 8\n𝐸 𝑋 = 𝐸 . 𝑋 = . 𝐸 𝑋\n6 6\n67! 67!\n• Even if you don’t know the distribution of 𝑋 (e.g., because the joint\ndistribution of 𝑋 , … , 𝑋 is unknown), you can still compute\n& ’\nexpectation of 𝑋.\nMost common use cases:\n%\n• Problem-solving key: • 𝐸 𝑋 easy to calculate\n#\n𝑋 = ; 𝑋\nDefine 𝑋 such that # • Sum of dependent RVs\n$\n#5!\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 19\nExpectations of common RVs: Binomial Review\n𝑋~Bin(𝑛, 𝑝) # of successes in 𝑛 independent trials\n𝐸 𝑋 = 𝑛𝑝\nwith probability of success 𝑝\nRecall: Bin 1, 𝑝 = Ber 𝑝\n8\n𝑋 = . 𝑋\n6\n67!\n% % %\nLet 𝑋 = 𝑖th trial is heads\n. 𝐸 𝑋 = 𝐸 ; 𝑋 = ; 𝐸 𝑋 = ; 𝑝 = 𝑛𝑝\n# #\n𝑋 ~Ber 𝑝 ,𝐸 𝑋 = 𝑝\n. .\n#5! #5! #5!\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 20\nExpectations of common RVs: Negative Binomial\nE # of independent trials with probability\n𝑌~NegBin(𝑟, 𝑝) 𝐸 𝑌 =\nF of success 𝑝 until 𝑟 successes\nRecall: NegBin 1, 𝑝 = Geo 𝑝\n?\n1. How should we define 𝑌 ?\n$\n𝑌 = . 𝑌\n6\n2. How many terms are in our summation?\n67!\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 21\nExpectations of common RVs: Negative Binomial\nE # of independent trials with probability\n𝑌~NegBin(𝑟, 𝑝) 𝐸 𝑌 =\nF of success 𝑝 until 𝑟 successes\nRecall: NegBin 1, 𝑝 = Geo 𝑝\n?:\n𝑌 = . 𝑌\n6\n67!\nLet 𝑌 = # trials to get 𝑖th success (after\n. ? ? ?\n1 𝑟\n𝑖 − 1 th success)\n𝐸 𝑌 = 𝐸 ; 𝑌 = ; 𝐸 𝑌 = ; =\n\/ # #\n𝑌~Geo 𝑝 ,𝐸 𝑌 = 𝑝 𝑝\n. .\n0 #5! #5! #5!\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 22 <END>"}
{"prompt":"03_section.txt\n\n###\n\n","completion":" –1–\nCS109 April25,2024\nSection 3: Named Random Variables\nBefore you leave lab, make sure you click here so that you’re marked as having attended. The CA\nleadingyourdiscussionsectioncanenterthepasswordneededonceyou’vesubmitted.\n1 Gender Composition of Discussion Sections\nAmassiveonlineStanfordclasshassectionswith10studentseach.Eachstudentinourpopulation\nhas a 50% chance of identifying as female, 47% chance of identifying as male and 3% chance of\nidentifying as non-binary. Even though students are assigned randomly to sections, a few sections\nenduphavingaveryunevendistributionjustbychance.Youshouldassumethatthepopulationof\nstudents is so large that the percentages of students who identify as male \/ female \/ non-binary are\nunchanged,evenifyouselectstudentswithoutreplacement.\na. Definearandomvariableforthenumberofpeopleinasectionwhoidentifyasmale.\nb. What is the expectation and standard deviation of number of students who identify as male\ninasinglesection?\nc. Write an expression for the exact probability that a section is skewed. We defined skewed to\nbethatthesectionhas0,1,9or10peoplewhoidentifyasmale.\nd. The course has 1,200 sections. Approximate the probability that 30 or more sections will be\nskewed.\n2 Better Evaluation of Eye Disease\nWhen a patient has eye inflammation, eye doctors ”grade” the inflammation. When ”grading”\ninflammation they randomly look at a single 1 millimeter by 1 millimeter square in the patient’s\neyeandcounthowmany”cells”theysee.\nThere is uncertainty in these counts. If the true average number of cells for a given patient’s eye is\n6, the doctor could get a different count (say 4, or 5, or 7) just by chance. As of 2021, modern eye\nmedicine does not have a sense of uncertainty for their inflammation grades! In this problem we\nare going to change that. At the same time we are going to learn about poisson distributions over\nspace.\na. Explain, as if teaching, why the number of cells observed in a 1x1 square is governed by\na poisson process. Make sure to explain how a binomial distribution could approximate the\ncount of cells. Explain what 𝜆 means in this context. Note: for a given person’s eye, the\npresenceofacellinalocationisindependentofthepresenceofacellinanotherlocation.\nb. For a given patient the true average rate of cells is 5 cells per 1x1 sample. What is the\nprobabilitythatinasingle1x1samplethedoctorcounts4cells?\n–2–\nFigure 1: A 1x1mm sample used for inflammation grading. Inflammation is graded by counting\ncellsinarandomlychosen1mmby1mmsquare.Thissamplehas5cells.\n3 Continuous Random Variables\nLet 𝑋 beacontinuousrandomvariablewiththefollowingprobabilitydensityfunction:\n(cid:26) 𝑐(𝑒𝑥−1 +𝑒−𝑥) if0 ≤ 𝑥 ≤ 1\n𝑓 𝑋(𝑥) =\n0 otherwise\na. Findthevalueof 𝑐 thatmakes 𝑓 𝑋 avalidprobabilitydistribution.\nb. Whatis 𝑃(𝑋 < 0.75)?Whatis 𝑃(𝑋 < 𝑥)?\n4 Website Visits\nYouhaveawebsitewhereonlyonevisitorcanbeonthesiteatatime,butthereisaninfinitequeue\nof visitors, so that immediately after a visitor leaves, a new visitor will come onto the website. On\naverage,visitorsleaveyourwebsiteafter5minutes.Assumethatthelengthofstayisexponentially\ndistributed.Wewillcalculatewhatistheprobabilitythatauserstaysmorethan10minutes.\na. Usingtherandomvariable 𝑋 definedasabove,whatistheprobabilitythatauserstayslonger\nthan10mins?(i.e, 𝑋 > 10).\nb. Using the random variable𝑌, defined as the number of users who leave your website over a\n10-minuteinterval,whatistheprobabilitythatauserstayslongerthan10mins? <END>"}
{"prompt":"heart-readme.txt\n\n###\n\n","completion":" Task:\nYour task is to assist a doctor in predicting whether or not a patient has heart disease (specifically myocardial perfusion diagnosis). Your prediction will be based on partial diagnosis made on images of different parts of the heart.\n\nValues:\nA heart is scanned and pictures of 2D images are generated for five different parts of the heart:\nArea A: Near the heart's apex (4 ROIs)\nArea B: In middle of the \"LV\" (5 ROIs)\nArea C: Near the heart base (5 ROIs)\nArea D: In the center of the LV cavity for horizontal long axis view (4 ROIs)\nArea E: In the center of the LV cavity for vertical long axis view (4 ROIs)\nThere are 4 or 5 regions of interest (ROIs) in each image. \n\nA cardiologist makes a partial diagnoses for each of the 22 Regions of Interest (ROIs). These partial diagnosis were mechanical to generate and could be performed by a trained nurse. 0 is a negative diagnosis (healthy), 1 is a positive diagnosis (unhealthy).  \n\nColumn meaning:\nEach column represents a partial diagnosis by a cardiologist for a particular ROI:\nColumn index, Area.ROI \n1, A.1\n2, A.2\n3, A.3\n4, A.4\n5, B.1\n6, B.2\n7, B.3\n8, B.4\n9, B.5\n10, C.1\n11, C.2\n12, C.3\n13, C.4\n14, C.5\n15, D.1\n16, D.2\n17, D.3\n18, D.4\n19, E.1\n20, E.2\n21, E.3\n22, E.4\nSee figure 3 in the paper \"Knowledge discovery approach to automated cardiac SPECT diagnosis\" by Kurgan et Al for a visual.\n\nPrediction:\nThe variable you are predicting is the overall diagnosis of heart disease. The labels were generated by a team of cardiologists based on detailed analysis of each case.\n\nCredit:\nThis dataset was collected by Kurgan et al and is hosted by the UCI Machine Learning Repository:\nhttp:\/\/archive.ics.uci.edu\/ml\/datasets\/SPECT+Heart\n\n <END>"}
{"prompt":"cs109_lec13_conceptcheck.txt\n\n###\n\n","completion":" Lecture 13: Concept Check  Graded\nStudent\nNeetish Sharma\nTotal Points\n4 \/ 4 pts\nQuestion 1\nLinearity of Expectation 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 2\nCovariance 2 \/ 2 pts\n2.1 Independent RVs 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\n2.2 Dependent RVs 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 3\nZero Covariance 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQ1 Linearity of Expectation\n1 Point\nLinearity of Expectation—that is, expectation of a sum is the same as the sum of\nexpectations—holds when:\nThere is independence\nThere is mutual exclusivity\nIn general\nQ2 Covariance\n2 Points\nQ2.1 Independent RVs\n1 Point\nSuppose X and Y are independent random variables. What does this tell us\nabout their covariance?\nIt is 0\nIt is 1\nIt is 0.5\nIt tells us nothing about their covariance\nQ2.2 Dependent RVs\n1 Point\nSuppose X and Y are dependent random variables. Which of the following are\ntrue?\nCov(X,Y ) = E[X − E[X]] ⋅ E[Y − E[Y ]]\nCov(X,Y ) = E[XY ] − E[X]E[Y ]\nE[X + Y ] = E[X] + E[Y ]\nVar(X + Y ) = Var(X) + Var(Y )\nVar(X + Y ) = Var(X) + 2Cov(X,Y ) + Var(Y )\nQ3 Zero Covariance\n1 Point\nSuppose we know that for Stanford students that their height and weight are\nboth random but their covariance is zero. Can we then conclude that it must be\ntrue that height and weight are independent for Stanford students?\nYes\nNo <END>"}
{"prompt":"ancestry-readme.txt\n\n###\n\n","completion":" Task:\nYour task is to predict the ethnicity of a person who has sent in their DNA based on Single Nucleotide Polymorphisms (SNPs).\n\nValues:\nThis dataset contains the genetic variation found in people sampled by the 1000 Genomes Project which sequenced the DNA from different ethnic groups around the world. Each input vector represents the DNA at specific locations in the genome for one individual. There are 20 binary input features. 0 indicates that the user's DNA at the given location matches the human reference genome. 1 indicates that the user's DNA does not match the human reference genome. The output class value represents the super population (ethnicity) of each individual. The super populations contained in this dataset are East Asian or Ad Mixed American, encoded in binary. The training data set contains 283 data vectors, and the testing data set contains 184 data vectors.\n\nColumn meaning:\nEach column represents a particular location in the human genome. 0 indicates that the user's DNA at the given location matches the human reference genome. 1 indicates that the user's DNA does not match the human reference genome. Though the particular locations in DNA may have semantic meanings -- for this task all you know is that each column is a distinct nucleotide index.\n\nPrediction:\nThe variable you are predicting is the super population of the user.\n\nCredit:\nThanks to Jim Notwell and Gill Bejerano who is a professor in Computer Science and Genetics \n\n <END>"}
{"prompt":"cs109_lec02_conceptcheck.txt\n\n###\n\n","completion":" Lecture 2: Concept Check  Graded\nStudent\nNeetish Sharma\nTotal Points\n4 \/ 4 pts\nQuestion 1\nCounting 2 \/ 2 pts\n1.1 Counting: Minimal Constraints 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\n1.2 Counting: Added Constraints 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 2\nCombinatorics I 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 3\nCombinatorics II 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQ1 Counting\n2 Points\nThere are 5 CS109 students (Emnet, Vanessa, Anthony, Mara, and Meygan) in line\nfor office hours.\nQ1.1 Counting: Minimal Constraints\n1 Point\nHow many ways can the five students be ordered in line, from front to back?\n5\n55 = 3125\n4 ⋅ 3 ⋅ 2 ⋅ 1 = 24\n2 ⋅ (4 ⋅ 3 ⋅ 2 ⋅ 1) = 48\n5 ⋅ 4 ⋅ 3 ⋅ 2 ⋅ 1 = 120\n6 ⋅ 5 ⋅ 4 ⋅ 3 ⋅ 2 ⋅ 1 = 720\n5 ⋅ 200 = 1000\nQ1.2 Counting: Added Constraints\n1 Point\nHow many ways can they be ordered if Emnet and Mara insist on being next to\neach other in line?\n5 ⋅ 4 ⋅ 3 ⋅ 2 ⋅ 1 = 120\n6 ⋅ 5 ⋅ 4 ⋅ 3 ⋅ 2 ⋅ 1! = 720\n4 ⋅ 3 ⋅ 2 ⋅ 1 = 24\n2 ⋅ (4 ⋅ 3 ⋅ 2 ⋅ 1) = 48\n5⋅4⋅3⋅2⋅1 = 60\n2\nQ2 Combinatorics I\n1 Point\nHow many ways are there to split a dozen people into 3 teams, where one team\nhas 2 people, and the other two teams have 5 people each? Restated, how many\nways can you partition 12 distinct items into three subsets of size 2, 5, and 5.\nNote that the order of the two size-5 subsets doesn't matter!, i.e. A partition of\n{1,2,3,4,5,6,7,8,9,10,11,12} into {1,2}, {3,4,5,6,7}, and\n{8,9,10,11,12} is the same partition as {1,2}, {8,9,10,11,12}, and\n{3,4,5,6,7}, so it should only be counted once.\nExpress your answer as a simple integer.\n8316\nQ3 Combinatorics II\n1 Point\nHow many ways are there to split a dozen people into 6 teams of two people\neach? Express your answer as a simple integer. Hint: Avoid overcounting just as\nyou did for Question 2.\n10395 <END>"}
{"prompt":"simple-readme.txt\n\n###\n\n","completion":" Task:\nThis is a toy dataset. You may find it helpful for testing your code. You should be able to predict the test dataset (which is identical to the train dataset) with 100% accuracy.\n\nValues:\nThe values are all binary. \n\nColumn meaning:\nN\/A\n\nPrediction:\nN\/A\n\nCredit:\nThis dataset was developed by Mehran Sahami\n\n <END>"}
{"prompt":"cs109_lec18_conceptcheck.txt\n\n###\n\n","completion":" Lecture 18: Concept Check  Graded\nStudent\nNeetish Sharma\nTotal Points\n5 \/ 5 pts\nQuestion 1\nDefinition: iid 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 2\nSum of IID Poisson RVs 2 \/ 2 pts\n2.1 (no title) 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\n2.2 (no title) 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 3\nSum of Continuous Uniforms 2 \/ 2 pts\n3.1 Sum of Uniforms 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\n3.2 Approximate Probability 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQ1 Definition: iid\n1 Point\nAre X 1, X 2, ... X n independent and identically distributed (iid) with the following\ndistributions? Select all that apply.\nX ∼ Poi(λ)\ni\nX ∼ Ber(p )\ni i\nLet X i be an indicator variable which takes a 1 if it is raining on the i'th\nday of the year\nImagine you toss a normal 6-sided die n times. Let X i be the outcome of\nthe i'th die toss.\nQ2 Sum of IID Poisson RVs\n2 Points\nLet X 1, X 2, ... X 10 be iid RV with X i ∼ Poi(10). Select all that apply.\nQ2.1\n1 Point\n10\nWhat is the distribution of their sum, ∑ X ?\ni=1 i\nN(100, 100)\nPoi(100)\nPoi(10)\nBinomial(100000, 0.001)\nQ2.2\n1 Point\nWhat is P(∑ i1 =0 1 X i ≤ 110), approximately or exactly? Select all that apply.\nΦ(110.5−100) = Φ(1.05) = 0.8531\n10\nΦ(110−100) = Φ(1) = 0.841\n10\n∑110 100ie−100\n= 0.8529\ni=0 i!\nQ3 Sum of Continuous Uniforms\n2 Points\nLet X 1, X 2, X 3, ..., X 95, and X 96 all be iid such that X i ∼ Uni(0,1), and let\n96\nX = ∑ X . Recall that the mean and variance of a single continuous\nk=1 k\nUni(a,b)\nare\na+b\nand\n(a−b)2\n, respectively.\n2 12\nQ3.1 Sum of Uniforms\n1 Point\nThe Central Limit Theorem tells us that X is distributed as something very close\nto a Gaussian. What is the sum of the mean and variance of this Gaussian? Enter\nyour answer to the nearest integer.\n56\nQ3.2 Approximate Probability\n1 Point\nWhat is the approximate probability that X < 64. Enter your answer to three\ndecimal places.\nHint: you can make an educated guess without bothering to do the math. We'll\nshow you the math after you enter the correct value.\n1.000 <END>"}
{"prompt":"06_section.txt\n\n###\n\n","completion":" –1–\nCS109 May16,2024\nContinuous Joint Distributions, Central Limit Theorem\nBeforeyouleavelab,makesureyouclickheresothatyou’remarkedashavingattendedthis\nweek’ssection.TheCAleadingyourdiscussionsectioncanenterthepasswordneededonce\nyou’vesubmitted.\n1 Warmups\n1.1 Food for Thought\nKarelthedogeatsanunpredictableamountoffood.Everyday,thedogisequallylikelytoeat\nbetweenacontinuousamountintherange100to300g.HowmuchKareleatsisindependentofall\notherdays.Youonlyhave6.5kgoffoodforthenext30days.Whatistheprobabilitythat6.5kg\nwillbeenoughforthenext30days?\n1.2 Sample and Population Mean\nComputingthesamplemeanissimilartothepopulationmean:sumallavailablepointsanddivide\nbythenumberofpoints.However,samplevarianceisslightlydifferentfrompopulationvariance.\n1. Considertheequationforpopulationvariance,andananalogousequationforsample\nvariance.\n𝑁\n1 ∑︁\n𝜎2 = (𝑥 𝑖 − 𝜇)2\n𝑁\n𝑖=1\n𝑛\n1 ∑︁\n𝑆2 𝑏𝑖𝑎𝑠𝑒𝑑 = 𝑛 (𝑋 𝑖 − 𝑋¯)2\n𝑖=1\n𝑆2 isarandomvariabletoestimatetheconstant 𝜎2.Becauseitisbiased,\n𝑏𝑖𝑎𝑠𝑒𝑑\n𝐸[𝑆2 ] ≠ 𝜎2.Is 𝐸[𝑆2 ] greaterorlessthan 𝜎2?\n𝑏𝑖𝑎𝑠𝑒𝑑 𝑏𝑖𝑎𝑠𝑒𝑑\n2. ConsideranalternativeRandomVariable, 𝑆2 (knownsimplyas 𝑆2 inclass).The\n𝑢𝑛𝑏𝑖𝑎𝑠𝑒𝑑\ntechniqueofun-biasingvarianceisknownasBessel’scorrection.Writethe 𝑆2\n𝑢𝑛𝑏𝑖𝑎𝑠𝑒𝑑\nequation.\n2 Problems\n2.1 Sum of Two Exponentials\nConsidertwoindependentrandomvariables 𝑋 and𝑌,eachExponentialswithdifferent\nparameters—specifically,let 𝑋 ∼ 𝐸𝑥𝑝(1) and𝑌 ∼ 𝐸𝑥𝑝(1).Assuming𝑇 = 𝑋 +𝑌,deriveand\n2 3\npresenttheprobabilitydensityfunction 𝑓 𝑇(𝑡) byevaluatingtherelevantconvolution.Onceyou\narriveatyour 𝑓 𝑇(𝑡),verifyyouranswerbycalculating 𝑓 𝑇(2) outtothreedecimalplaces.\n–2–\n2.2 Grading Exams\nJacobandKathleenareplanningtogradeProblem1onyourWeek7exam,andthey’lleachgrade\ntheirhalfindependentlyoftheother.Jacobtakes 𝑋 ∼ 𝐸𝑥𝑝(1) hourstofinishhishalfwhile\n3\nKathleentakes𝑌 ∼ 𝐸𝑥𝑝(1) hourstofinishhishalf.\n4\na. FindtheCDFof 𝑋\/𝑌,whichistheratiooftheirgradingcompletiontimes.\nb. WhatistheprobabilitythatKathleenfinishesbeforeJacobdoes?\n2.3 Central Limit Theorem and Sampling Calisthenics\na. Let 𝑋 , 𝑋 , 𝑋 ,..., 𝑋 beiid—thatis,independentandidenticallydistributed—suchthat\n1 2 3 1000\n𝑋 𝑖 ∼ NegBin(𝑟 = 10, 𝑝 = 0.5),andlet𝑊 = 𝑋 1 + 𝑋 2 +...+ 𝑋 1000.AccordingtotheCentral\nLimitTheorem,whatdistributiondoes𝑊 assume,andwhatareitsparameters?\nb. Define 𝑋¯ = 101\n00\n(cid:205) 𝑖1 =0 100 𝑋 𝑖 tobethesamplemeanofour1000iidsamples.Whatisthe\nstandarddeviationoftherandomvariable 𝑋¯?\nc. Youcomputethevarianceofyour1000samples, 𝑋 , 𝑋 , 𝑋 ,..., 𝑋 accordingtothe\n1 2 3 1000\ntraditionaldefinitionofvariance—i.e. 101\n00\n(cid:205) 𝑖1 =0 100 (𝑋 𝑖 − 𝑋¯)2.Doyouexpectthisvarianceto,\nmoreoftenthannot,belarger,equalto,orsmallerthanthevarianceofNegBin(10,0.5).\nExplainyouranswer.\nd. ThenumberofsamplesneededfortheCentralLimitTheoremtoapplyisgenerally\nunderstoodtobe30ormore.However,theCentralLimitTheoremworkswellforaneven\nsmallernumberofsampleswhen 𝑋 𝑖 ∼ Bin(10,0.5) thanisdoeswhen\n𝑋\n𝑖\n∼ NegBin(10,0.5).Brieflyexplainwhy.\ne. Recallthatsamplingtheoryallowsareasonablylargesampletostandinforthetrue\npopulationdistribution.Whenresamplingfromthesampleforbootstrappingpurposes,we\ngenerallydosowithreplacement.Whywithreplacementinsteadof without? <END>"}
{"prompt":"cs109_lec16_conceptcheck.txt\n\n###\n\n","completion":" Lecture 16: Concept Check  Graded\nStudent\nNeetish Sharma\nTotal Points\n5 \/ 5 pts\nQuestion 1\nJoint Continuous X and Y 4 \/ 4 pts\n1.1 Finding the full PDF 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\n1.2 Probabilities 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\n1.3 Inferences 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\n1.4 Further Constraint 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 2\nExpected Distance 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQ1 Joint Continuous X and Y\n4 Points\nSuppose X and Y are jointly continuous random variables with joint PDF\nf X,Y(x,y) = cxy where 0 ≤ x,y ≤ 1.\nQ1.1 Finding the full PDF\n1 Point\nWhat is the value for constant c? Your answer should be a whole integer.\n4\nQ1.2 Probabilities\n1 Point\nFor your value of c, what is P(X ≤ Y )?\nDepends on c\n0.5\n0\n1\nQ1.3 Inferences\n1 Point\nFrom above, which of the following statements can we infer to be true?\nX and Y are independent\nX and Y are correlated\nWe can infer nothing\nQ1.4 Further Constraint\n1 Point\nSuppose now we learn some more information about the relationship between X\nand Y and update the joint PDF f X,Y(x,y) = 8xy where 0 ≤ x ≤ y ≤ 1.\nWhich of the following statements can we infer to be true?\nX and Y are independent\nX and Y are dependent\nQ2 Expected Distance\n1 Point\nAssume that X and Y are each Uni(0,1) random variables, so that the joint\nPDF is simply f(x,y) = 1 for 0 < x,y < 1.\nWhat is the expected value of the distance between them? Restated, what is\nE[abs(Y − X)]? Express your answer to three decimal places.\n0.333 <END>"}
{"prompt":"03_section_soln.txt\n\n###\n\n","completion":" –1–\nCS109 April25,2024\nSection 3: Named Random Variables\nBefore you leave lab, make sure you click here so that you’re marked as having attended. The CA\nleadingyourdiscussionsectioncanenterthepasswordneededonceyou’vesubmitted.\n1 Gender Composition of Discussion Sections\nAmassiveonlineStanfordclasshassectionswith10studentseach.Eachstudentinourpopulation\nhas a 50% chance of identifying as female, 47% chance of identifying as male and 3% chance of\nidentifying as non-binary. Even though students are assigned randomly to sections, a few sections\nenduphavingaveryunevendistributionjustbychance.Youshouldassumethatthepopulationof\nstudents is so large that the percentages of students who identify as male \/ female \/ non-binary are\nunchanged,evenifyouselectstudentswithoutreplacement.\na. Definearandomvariableforthenumberofpeopleinasectionwhoidentifyasmale.\nLet 𝑋 denotethenumberofpeopleinasectionwhoidentifyasmale.\n𝑋 ∼ Bin(𝑛 = 10, 𝑝 = 0.47)\nb. What is the expectation and standard deviation of number of students who identify as male\ninasinglesection?\n𝐸[𝑋] = 𝑛· 𝑝 = 10·0.47 = 4.7\n√\n√︁ √︁\nStd(𝑋) = Var(𝑋) = 𝑛· 𝑝 · (1− 𝑝) = 10·0.47·0.53 ≈ 1.58\nc. Write an expression for the exact probability that a section is skewed. We defined skewed to\nbethatthesectionhas0,1,9or10peoplewhoidentifyasmale.\nRecallthat 𝑝 = 0.47.\n𝑃(skewed) = 𝑃(𝑋 = 0) + 𝑃(𝑋 = 1) + 𝑃(𝑋 = 9) + 𝑃(𝑋 = 10)\n(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)\n10 10 10 10\n= (1− 𝑝)10 + 𝑝(1− 𝑝)9 + 𝑝9(1− 𝑝) + 𝑝10\n0 1 9 10\n≈ 0.024\n$ python3\n>>> import scipy.stats as st\n–2–\n>>> import numpy as np\n>>> st.binom(10, 0.47).pmf(np.array([0,1,9,10])).sum()\n0.023715146414928143\nd. The course has 1,200 sections. Approximate the probability that 30 or more sections will be\nskewed.\nThe exact probability of number of skewed sections is 𝑆 ∼ Bin(𝑛 = 1200, 𝑝 = 0.024). To\nsimplify the math, we can approximate the number of skewed sections using a Poisson\napproximation.Let𝑌 bethePoissonapproximationof 𝑆.\n𝑌 ∼ Poi(𝜆 = 28.8) since 𝑛𝑝 = 1200·0.024 = 28.8.\n𝑃(𝑌 ≥ 30) = 1− 𝑃(𝑌 < 30)\n29\n∑︁\n= 1− 𝑃(𝑌 = 𝑘)\n𝑘=0\n≈ 0.436\n$ python3\n>>> import scipy.stats as st\n>>> import numpy as np\n>>> lamb = 28.8\n>>> 1 - st.poisson(lamb).pmf(range(0, 30)).sum()\n0.43605869062536795\n2 Better Evaluation of Eye Disease\nWhen a patient has eye inflammation, eye doctors ”grade” the inflammation. When ”grading”\ninflammation they randomly look at a single 1 millimeter by 1 millimeter square in the patient’s\neyeandcounthowmany”cells”theysee.\nThere is uncertainty in these counts. If the true average number of cells for a given patient’s eye is\n6, the doctor could get a different count (say 4, or 5, or 7) just by chance. As of 2021, modern eye\nmedicine does not have a sense of uncertainty for their inflammation grades! In this problem we\nare going to change that. At the same time we are going to learn about poisson distributions over\nspace.\na. Explain, as if teaching, why the number of cells observed in a 1x1 square is governed by\na poisson process. Make sure to explain how a binomial distribution could approximate the\n–3–\nFigure 1: A 1x1mm sample used for inflammation grading. Inflammation is graded by counting\ncellsinarandomlychosen1mmby1mmsquare.Thissamplehas5cells.\ncount of cells. Explain what 𝜆 means in this context. Note: for a given person’s eye, the\npresenceofacellinalocationisindependentofthepresenceofacellinanotherlocation.\nWe can approximate a distribution for the count by discretizing the square into a fixed\nnumber of equal sized buckets. Each bucket either has a cell or not. Therefore, the count\nof cells in the 1x1 square is a sum of Bernoulli random variables with equal 𝑝, and as\nsuch can be modeled as a binomial random variable. This is an approximation because it\ndoesn’t allow for two cells in one bucket. Just like with time, if we make the size of each\nbucket infinitely small, this limitation goes away and we converge on the true distribution\nof counts. The binomial in the limit, i.e. a binomial as 𝑛 → ∞, is truly represented by a\nPoisson random variable. In this context, 𝜆 represents the average number of cells per 1×1\nsample.SeeFigure2.\nFigure 2: 𝑋 is counts of events in discrete buckets. In the limit, as 𝑛 (number of buckets) → ∞, 𝑋\nbecomesaPoisson.\nb. For a given patient the true average rate of cells is 5 cells per 1x1 sample. What is the\nprobabilitythatinasingle1x1samplethedoctorcounts4cells?\n–4–\nLet 𝑋 denote the number of cells in the 1x1 sample. We note that 𝑋 ∼ 𝑃𝑜𝑖(5). We want to\nfind 𝑃(𝑋 = 4).\n54𝑒−5\n𝑃(𝑋 = 4) = ≈ 0.175\n4!\nInadditiontoprovidinganexpressionabove,\npleasecomputeanumericanswer: 0.175\n3 Continuous Random Variables\nLet 𝑋 beacontinuousrandomvariablewiththefollowingprobabilitydensityfunction:\n(cid:26) 𝑐(𝑒𝑥−1 +𝑒−𝑥) if0 ≤ 𝑥 ≤ 1\n𝑓 𝑋(𝑥) =\n0 otherwise\na. Findthevalueof 𝑐 thatmakes 𝑓 𝑋 avalidprobabilitydistribution.\nWeneed∫ ∞ 𝑓 𝑋(𝑥)𝑑𝑥 = 1ifthisistobeavalidprobabilitydensityfunction!\n−∞\n∫ ∞ ∫ 1\n𝑓 𝑋(𝑥)𝑑𝑥 = 𝑐(𝑒𝑥−1 +𝑒−𝑥 )𝑑𝑥\n−∞ 0\n1 = 𝑐 (cid:2)𝑒𝑥−1 −𝑒−𝑥(cid:3)1\n𝑥=0\n1 = 𝑐(𝑒1−1 −𝑒−1 − (𝑒0−1 −𝑒−0))\n1 1\n𝑐 = =\n1−𝑒−1 − (𝑒−1 −1) 2− 2\n𝑒\nb. Whatis 𝑃(𝑋 < 0.75)?Whatis 𝑃(𝑋 < 𝑥)?\n∫ 0.75\n𝑃(𝑋 < 0.75) = 𝑐(𝑒𝑥−1 +𝑒−𝑥 )𝑑𝑥\n0\n= 𝑐 (cid:2)𝑒𝑥−1 −𝑒−𝑥(cid:3)0.75\n𝑥=0\n(cid:16) (cid:17)\n= 𝑐 (𝑒0.75−1 −𝑒−0.75) − (𝑒0−1 −𝑒−0)\n∫ 𝑥\n𝑃(𝑋 < 𝑥) = 𝑐(𝑒𝑦−1 +𝑒−𝑦 )𝑑𝑦\n0\n= 𝑐 (cid:2)𝑒𝑦−1 −𝑒−𝑦(cid:3)𝑥\n𝑥=0\n(cid:16) (cid:17)\n= 𝑐 (𝑒𝑥−1 −𝑒−𝑥 ) − (𝑒0−1 −𝑒−0)\n–5–\n4 Website Visits\nYouhaveawebsitewhereonlyonevisitorcanbeonthesiteatatime,butthereisaninfinitequeue\nof visitors, so that immediately after a visitor leaves, a new visitor will come onto the website. On\naverage,visitorsleaveyourwebsiteafter5minutes.Assumethatthelengthofstayisexponentially\ndistributed.Wewillcalculatewhatistheprobabilitythatauserstaysmorethan10minutes.\na. Usingtherandomvariable 𝑋 definedasabove,whatistheprobabilitythatauserstayslonger\nthan10mins?(i.e, 𝑋 > 10).\n𝑃(𝑋 > 10) = 1− 𝐹 𝑋(10) = 1− (1−𝑒−10𝜆 ) = 𝑒−2 ≈ 0.1353\nb. Using the random variable𝑌, defined as the number of users who leave your website over a\n10-minuteinterval,whatistheprobabilitythatauserstayslongerthan10mins?\nIf this problem doesn’t convince you that the Poisson and Exponential RVs are coupled,\nthenI’mnotsurewhatwill!Asdefinedabove, 𝑋 ∼ Exp(𝜆 = 1).\n5\nAlternatively, we have that 𝑌 is the number of users leaving on the website in the\nnext 10 minutes. The average number of users leaving is 2 users per 10 minutes.\n𝑌 ∼ Poi(𝜆 = 2).\n20𝑒−2\n𝑃(𝑌 = 0) =\n0!\n= 𝑒−2 ≈ 0.1353 <END>"}
{"prompt":"cs109_lec09_conceptcheck.txt\n\n###\n\n","completion":" Lecture 9: Concept Check  Graded\nStudent\nNeetish Sharma\nTotal Points\n3 \/ 3 pts\nQuestion 1\nContinuous Distributions 2 \/ 2 pts\n1.1 Continuous Distributions: Specific Outcome 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\n1.2 Continuous Distributions: Expectation 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 2\nMore Continuous Distributions 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQ1 Continuous Distributions\n2 Points\nA continuous variable X is distributed according to the probability density\nfunction (PDF): f(x) = 1 for 1 ≤ x ≤ 3, and f(x) = 0 otherwise. Notice that\n2\nX ∼ Uni(1,3).\nQ1.1 Continuous Distributions: Specific Outcome\n1 Point\nWhat is the probability that X = 1?\nf(1)\n∫1 f(x)dx = 1\n0 2\n0\nnot enough info\nQ1.2 Continuous Distributions: Expectation\n1 Point\nWhat is the expectation of X, E[X]?\n∞\n∫ f(x)dx = 1\n−∞\n∞\n∫ xf(x)dx = 2\n−∞\nf(2) = 1\n2\nf(1) + f(3) = 1\nQ2 More Continuous Distributions\n1 Point\nLet Y ∼ Uni(0,10), with PDF f(x). Which of the following are correct (select all\nthat apply).\nP(Y ≤ 2) = 1\n5\nP(4 ≤ Y ≤ 5) = P(9 ≤ Y ≤ 10)\nf(5) = 1\n10\nE[Y ] = 5\nE[Y ] = 10 <END>"}
{"prompt":"cs109_lec23_conceptcheck.txt\n\n###\n\n","completion":" Lecture 23: Concept Check  Graded\nStudent\nNeetish Sharma\nTotal Points\n3 \/ 3 pts\nQuestion 1\nNaïve Bayes: Definitions 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 2\nNaive Bayes: Why? 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 3\nMovie Suggestions Revisited 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQ1 Naïve Bayes: Definitions\n1 Point\nWhich of the following directly follow from the Naïve Bayes assumption? Select\none or more.\nP^ (X∣Y ) = ∏m P^ (X ∣Y )\nj=1 j\nY^ = argmax P^ (Y = y∣X)\ny\nP^ (X ,X ,X ∣Y ) = P^ (X ∣Y )P^ (X ∣Y )P^ (X ∣Y )\n1 2 3 1 2 3\nlog[P^ (X∣Y )] = ∑m log[P^ (X ∣Y )]\nj=1 j\nThe assumption that the features X are conditionally independent given Y\nQ2 Naive Bayes: Why?\n1 Point\nWhy might we use the Naïve Bayes assumption?\nGreatly reduces the number of parameters of the model\nThe features X are always conditionally independent\nMany times the features are roughly conditionally independent\nMakes our model more robust and flexible\nQ3 Movie Suggestions Revisited\n1 Point\nRecall this example from today's lectre, which predicted that\nY^\n= 1 for an input\nvector of X = (1,0). Suppose instead that X = (0,1). Compute the two\nprobabilities that need to be compared, and return the absolute value of their\ndifference out to two significant digits.\n0.02 <END>"}
{"prompt":"07_section_soln.txt\n\n###\n\n","completion":" –1–\nCS109 May23,2024\nSampling and Bootstrapping, MLE, Beta\nBeforeyouleavelab,makesureyouclickheresothatyou’remarkedashavingattendedthisweek’s\nsection.TheCAleadingyourdiscussionsectioncanenterthepasswordneededonceyou’ve\nsubmitted.\n1 Warmups\n1.1 MLE\nSuppose𝑥 1,...,𝑥 𝑛 areiid(independentandidenticallydistributed)valuessampledfromsome\ndistributionwithdensityfunction 𝑓(𝑥|𝜃),where𝜃 isunknown.Recallthatthelikelihoodofthedatais\n𝑛\n(cid:214)\n𝐿(𝜃) = 𝑓(𝑥 1,𝑥 2,...,𝑥 𝑛|𝜃) = 𝑓(𝑥 𝑖|𝜃)\n𝑖=1\nRecallwesolveanoptimizationproblemtofind 𝜃ˆwhichmaximizes 𝐿(𝜃),i.e., 𝜃ˆ = argmax 𝐿(𝜃).\n𝜃\na. Writeanexpressionforthelog-likelihood, 𝐿𝐿(𝜃) = log𝐿(𝜃).\nb. Whycanweoptimize 𝐿𝐿(𝜃) ratherthan 𝐿(𝜃)?\nc. Whymight weoptimize 𝐿𝐿(𝜃) ratherthan 𝐿(𝜃)?\na. (cid:205) 𝑖𝑛 =1log 𝑓(𝑥 𝑖|𝜃)\nb. Logarithmsarestrictlyincreasingfunctions.Foranystrictlyincreasingfunction 𝑓 andany\nfunction 𝑔,thefollowingholds:argmax𝑔(𝑥) = argmax 𝑓(𝑔(𝑥))\nc. Findingthemaxofafunctionrequiresyoutakederivatives.Theoriginalexpressionconsistsof\naproductofmanyfunctionsof 𝜃.Thisleadstoadifficultderivationbecauseofthechainrule\nofcalculus.\nBytakingthelog,weconverttheproductintosums,andthederivativeofthesumofmany\nfunctionsof 𝜃 ismucheasiertocompute.\nThatis, 𝜕𝜕\n𝜃\n(cid:205)\n𝑖\n𝑓 𝑖(𝜃) = (cid:205)\n𝑖\n𝜕𝜕\n𝜃\n𝑓 𝑖(𝜃) whichiseasy.\nHowever, 𝜕𝜕\n𝜃\n(cid:206)\n𝑖\n𝑓 𝑖(𝜃) = acomplicatedexpression\n–2–\n1.2 Beta\na. Supposeyouhaveacoinwhereyouhavenopriorbeliefonitstrueprobabilityofheads 𝑝.How\ncanyoumodelthisbeliefasaBetadistribution?\nb. Supposeyouhaveacoinwhichyoubelieveisfair,with”strength” 𝛼.Thatis,pretendyou’ve\nseen 𝛼 headsand 𝛼 tails.HowcanyoumodelthisbeliefasaBetadistribution?\nc. Nowsupposeyoutakethecoinfromthepreviouspartandflipit10times.Yousee8headsand\n2tails.Howcanyoumodelyourposteriorbeliefofthecoin’sprobabilityofheads?\na. Beta(1,1) isauniformprior,meaningthatpriortoseeingtheexperiment,allprobabilitiesof\nheadsareequallylikely.\nb. Beta(𝛼+1,𝛼+1).Thisisourpriorbeliefaboutthedistribution.\nc. Beta(𝛼+9,𝛼+3)\n1.3 Beta Sum\nWhatisthedistributionofthesumof100iidBetas?Let 𝑋 bethesum\n100\n∑︁\n𝑋 = 𝑋 𝑖 whereeach 𝑋 𝑖 ∼ Beta(𝑎 = 3,𝑏 = 4)\n𝑖=1\nNotethevarianceofaBeta:\n𝑎𝑏\nVar(𝑋 𝑖) = where 𝑋 𝑖 ∼ Beta(𝑎,𝑏)\n(𝑎 +𝑏)2(𝑎 +𝑏 +1)\nBytheCentralLimitTheorem,thesumofequallyweightedIIDrandomvariableswillbeNormally\ndistributed.Wecalculatetheexpectationandvarianceof 𝑋 𝑖 usingthebetaformulas:\n𝑎\n𝐸(𝑋 𝑖) = ExpectationofaBeta\n𝑎 +𝑏\n3\n= ≈ 0.43\n7\n𝑎𝑏\nVar(𝑋 𝑖) = VarianceofaBeta\n(𝑎 +𝑏)2(𝑎 +𝑏 +1)\n3·4\n=\n(3+4)2(3+4+1)\n12\n= ≈ 0.03\n49·8\n–3–\n𝑋 ∼ 𝑁(𝜇 = 𝑛· 𝐸[𝑋 𝑖],𝜎2 = 𝑛·Var(𝑋 𝑖))\n∼ 𝑁(𝜇 = 43,𝜎2 = 3)\n2 Problems\n2.1 Variance of Hemoglobin Levels\nAmedicalresearchertreatspatientswithdangerouslylowhemoglobinlevels.Shehasformulatedtwo\nslightlydifferentdrugsandisnowtestingthemonpatients.First,sheadministereddrugAtoone\ngroupof50patientsanddrugBtoaseparategroupof50patients.Then,shemeasuredallthe\npatients’hemoglobinlevelspost-treatment.Forsimplicity,assumethatallvariationinthepatient\noutcomesisduetotheirdifferentreactionstotreatment.\nTheresearchernotesthatthesamplemeanissimilarbetweenthetwogroups:bothhavemean\nhemoglobinlevelsaround10g\/dL.However,drugB’sgrouphasasamplevariancethatis3(g\/dL)2\ngreaterthandrugA’sgroup.TheresearcherthinksthatpatientsrespondtodrugsAandBdifferently.\nSpecifically,shewantstomakethescientificclaimthatdrugA’spatientswillendupwitha\nsignificantlydifferentspreadofhemoglobinlevelscomparedtodrugB’s.\nYouareskeptical.Itispossiblethatthetwodrugshavepracticallyidenticaleffectsandthatthe\nobserveddifferentinvariancewasaresultofchanceandasmallsamplesize,i.e.thenull\nhypothesis.Calculatetheprobabilityofthenullhypothesisusingbootstrapping.Hereisthedata.\nEachnumberisthelevelofanindependentlysampledpatient:\nHemoglobinLevelsofDrugA’sGroup(𝑆2 =6.0):13,12,7,16,9,11,7,10,9,8,9,7,16,7,9,8,\n13,10,11,9,13,13,10,10,9,7,7,6,7,8,12,13,9,6,9,11,10,8,12,10,9,10,8,14,13,13,10,\n11,12,9\nHemoglobinLevelsofDrugB’sGroup(𝑆2 =9.1):8,8,16,16,9,13,14,13,10,12,10,6,14,8,\n13,14,7,13,7,8,4,11,7,12,8,9,12,8,11,10,12,6,10,15,11,12,3,8,11,10,10,8,12,8,11,6,\n7,10,8,5\nCompletethepromptsinthisColabnotebooktoinvestigatethisquestionusingbootstrapping.\nYoucanvisitthisnotebooktoseehowwefleshedoutallofthedifferentfunctionsexpectedofyou\nforthisproblem.\n–4–\n2.2 Parameter Estimation and Wealth Distribution\nThebroaderfieldofeconomicsalsoreliesonlikelihoodestimationandparameterestimation.One\ncontinuousprobabilitydistribution—onewithalongtailas𝑥 approachesinfinity—isusedtomodel\nwealthinequalityandthesocioeconomicproblemsthatstemfromit.Thisprobabilitydistributionis\ngivenas:\n𝜔3𝜔\n𝑓(𝑥|𝜔) = , where𝑥 ≥ 3,𝜔 > 1\n𝑥𝜔+1\nAssumethatyou’veobservedasampleofiidrandomvariables (𝑋 1,𝑋 2,𝑋 3,...,𝑋 𝑛),whereeachofthe\n𝑋 𝑖 ismodeledaccordingtotheaboveprobabilitydistributionfunction.\na. Whatisthelog-likelihoodfunction 𝐿𝐿(𝜔) ofthesample (𝑋 1,𝑋 2,𝑋 3,...,𝑋 𝑛)?Simplifyusing\npropertiesoflogarithmswhereverpossible.\n(cid:214)𝑛 𝜔3𝜔\n𝐿(𝜔) =\n𝑥𝜔+1\n𝑖=1 𝑖\n∑︁𝑛 𝜔3𝜔\n𝐿𝐿(𝜔) = log\n𝑥𝜔+1\n𝑖=1 𝑖\n𝑛 𝑛 𝑛\n∑︁ ∑︁ ∑︁\n= log𝜔+ log3𝜔 − log𝑥𝜔+1\n𝑖\n𝑖=1 𝑖=1 𝑖=1\n𝑛 𝑛 𝑛 𝑛\n∑︁ ∑︁ ∑︁ ∑︁\n= log𝜔 1+𝜔 log3−𝜔 log𝑥 𝑖 − log𝑥 𝑖\n𝑖=1 𝑖=1 𝑖=1 𝑖=1\n𝑛 𝑛\n∑︁ ∑︁\n= 𝑛log𝜔+𝑛𝜔log3−𝜔 log𝑥 𝑖 − log𝑥 𝑖\n𝑖=1 𝑖=1\nb. Setuptheequationthatwouldneedtobesolvedinordertocompute𝜔ˆ𝑀𝐿𝐸.Onceyouarriveat\ntheequationandhaveworkedthroughanycalculus,youcanstopandsimplypresentthe\nequationthatcanbesolvedviasimplealgebraicmanipulation.\nComputing𝜔ˆ𝑀𝐿𝐸 amountstofindingthevalueof𝜔 thatsolvestheequation 𝛿𝐿 𝛿𝐿 𝜔(𝜔) = 0.\nFortunately,thederivativeofinterestiseasilycomputed,becauseour 𝐿𝐿(𝜔) isasumof\n–5–\nlogarithmsandlinearterms.\n𝑛 𝑛\n∑︁ ∑︁\n𝐿𝐿(𝜔) = 𝑛log𝜔+𝑛𝜔log3−𝜔 log𝑥 𝑖 − log𝑥 𝑖\n𝑖=1 𝑖=1\n𝑛\n𝛿𝐿𝐿(𝜔) 𝑛\n∑︁\n= +𝑛log3− log𝑥 𝑖 = 0\n𝛿𝜔 𝜔\n𝑖=1\nYouwerewelcometostopattheaboveequation,sinceitcanbesolvedviaalgebraic\nmanipulationyouneedn’tshow.Wedo,however,presentthesolutionbelowforcompleteness,\nsincethisisaarealprobabilitydistributionusedinmanyfieldsofstatisticalanalysis.\n𝑛\n𝑛\n∑︁\n𝜔ˆ𝑀𝐿𝐸\n= 𝑆 log𝑋 −𝑛log3, where 𝑆 log𝑋 = log𝑥 𝑖\n𝑖=1\n𝑛\n𝜔ˆ𝑀𝐿𝐸 =\n𝑆\nlog𝑋\n−𝑛log3\n2.3 Dirk\/Evan Showdown\nDirkandEvanaresettocompeteinamatchofsevengames,andtheplayerwinningthemostwillbe\nrewardedwithamedalfromtheRulerofallofBayestopia,QueenDoris.Theoutcomesofeachof\nthesevengamesareindependentofoneanother,andDirkwinseachgamewithaprobability 𝑝 (so\nthatEvanwinswithprobability1− 𝑝).Unfortunately, 𝑝 isunknown,soyoudecidetomodel 𝑝 itself\nasaBetarandomvariablesuchthat 𝑝 ∼ 𝐵𝑒𝑡𝑎(1,1).\nTolearnmoreabout 𝑝,youreaduponalloftheirpreviousgames,findthatthey’vealreadycompeted\n12times,andlearnthatEvanhaswon7ofthose12gamesinthisorder:WLLLWWLWWLWW.\nEvenifEvan’sedgeiseversoslight,heappearstobetheapriorifavorite.\na. Findtheposteriordistributionof 𝑝 givenDirkandEvan’spriorhistorycompetingagainstone\nanother.\nb. RecallthatthePDFofa 𝐵𝑒𝑡𝑎 distributiononintegerparameters 𝑎 and 𝑏 isgivenas:\n1\n𝑓(𝑥|𝑎,𝑏) = 𝑥𝑎−1(1−𝑥)𝑏−1,where 𝐵(𝑎,𝑏) isanormalizationconstant\n𝐵(𝑎,𝑏)\nAssumingourhyperparameters 𝑎 and 𝑏 arepositiveintegers,explainwhytheexpectedvalue\nof\n𝑝𝑚(1− 𝑝)𝑛\n—thatis,\n𝐸[𝑝𝑚(1− 𝑝)𝑛]—isgivenby\n–6–\n𝐵(𝑎 +𝑚,𝑏 +𝑛)\n𝐸[𝑝𝑚\n(1−\n𝑝)𝑛\n] =\n𝐵(𝑎,𝑏)\nYoucanassumethat 𝑚 and 𝑛 arenonnegativeintegersaswell.\nc. Relyingonlyontheposteriordistributionof 𝑝 thatyoucomputedforpartaandtheresultfrom\npartb,computetheexpectedprobabilitythatthebest-of-sevenmatchbetweenDirkandEvan\nisn’tsettledafterthefirstsixgamesandthereforerequirestheseventhbeplayed?\na. Becausetheprioris 𝑝 ∼ 𝐵𝑒𝑡𝑎(1,1),andbecausethelikelihoodfunctionisaBinomialwith\n𝑛 = 12andunknownp,ourposteriorisalsoaBeta,butwithparametersof 𝑎 = 1+5 = 6and\n𝑏 = 1+7 = 8.Morecompactly, 𝑝|data ∼ 𝐵𝑒𝑡𝑎(6,8).\nb. LOTUStellsusthattheexpectedvalueofanarbitraryfunction 𝑓(𝑥) is:\n1 ∫ 1\n𝐸[𝑓(𝑥)] = 𝑓(𝑥)𝑥𝑎−1(1−𝑥)𝑏−1.\n𝐵(𝑎,𝑏)\n0\n𝑓(𝑥) = 𝑥𝑚(1−𝑥)𝑛 blendsbeautifullywiththePDFofourBetaposterior:\n1 ∫ 1\n𝐸[𝑓(𝑝)] = 𝐸[𝑝𝑚 (1− 𝑝)𝑛 ] = 𝑥𝑚 (1−𝑥)𝑛𝑥𝑎−1(1−𝑥)𝑏−1\n𝐵(𝑎,𝑏)\n0\n1 ∫ 1 𝐵(𝑎 +𝑚,𝑏 +𝑛)\n=\n𝑥𝑚+𝑎−1(1−𝑥)𝑛+𝑏−1\n=\n𝐵(𝑎,𝑏) 𝐵(𝑎,𝑏)\n0\nc. Theprobabilitythataseventhgameisplayedis (cid:0)6(cid:1)𝑝3(1− 𝑝)3.Sotheexpectedvalueis\n3\n(cid:0)6(cid:1)𝑝3(1− 𝑝)3 wouldbe:\n3\n(cid:18) 6(cid:19)𝐵(9,11)\n.\n3 𝐵(6,8) <END>"}
{"prompt":"cs109_lec05_conceptcheck.txt\n\n###\n\n","completion":" Lecture 5: Concept Check  Graded\nStudent\nNeetish Sharma\nTotal Points\n3 \/ 3 pts\nQuestion 1\nIndependence 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 2\nChain Rule and Independence 2 \/ 2 pts\n2.1 Chain Rule: Possible Dependence 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\n2.2 Chain Rule: Some Independence 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQ1 Independence\n1 Point\nSuppose we roll two fair N-sided dice to produce values of D and D . Define\n1 2\nthe event E to be D = 1, the event F to be that D = N, and the event G to be\n1 2\nthe sum D + D = k. Suppose we know E, F, and G are pairwise independent\n1 2\n—that is, that E and F are independent, F and G are independent, and E and G\nare independent. Suppose further that E, F, and G are not three-way\nindependent. What is k?\nN\n7\nN + 1\n2N\nN2\nQ2 Chain Rule and Independence\n2 Points\nLet E, F, and G be events with nonzero probabilities.\nQ2.1 Chain Rule: Possible Dependence\n1 Point\nWhat is an equivalent expression for P(EFG)?\nP(F∣EG)P(G∣EF)P(E∣FG)\nP(F∣E)P(G∣E)P(E∣FG)\nP(F)P(G∣F)P(E∣FG)\nP(F)P(G)P(E∣FG)\nP(E)P(F)P(G)\nQ2.2 Chain Rule: Some Independence\n1 Point\nSuppose that F and G are independent. Using the property of independence,\nwhat is an equivalent expression for P(EFG)?\nP(F∣EG)P(G∣EF)P(E∣FG)\nP(F∣E)P(G∣E)P(E∣FG)\nP(F)P(G)P(E∣FG)\nP(E)P(F)P(G) <END>"}
{"prompt":"07_section.txt\n\n###\n\n","completion":" –1–\nCS109 May23,2024\nSampling and Bootstrapping, MLE, Beta\nBeforeyouleavelab,makesureyouclickheresothatyou’remarkedashavingattendedthisweek’s\nsection.TheCAleadingyourdiscussionsectioncanenterthepasswordneededonceyou’ve\nsubmitted.\n1 Warmups\n1.1 MLE\nSuppose𝑥 1,...,𝑥 𝑛 areiid(independentandidenticallydistributed)valuessampledfromsome\ndistributionwithdensityfunction 𝑓(𝑥|𝜃),where𝜃 isunknown.Recallthatthelikelihoodofthedatais\n𝑛\n(cid:214)\n𝐿(𝜃) = 𝑓(𝑥 1,𝑥 2,...,𝑥 𝑛|𝜃) = 𝑓(𝑥 𝑖|𝜃)\n𝑖=1\nRecallwesolveanoptimizationproblemtofind 𝜃ˆwhichmaximizes 𝐿(𝜃),i.e., 𝜃ˆ = argmax 𝐿(𝜃).\n𝜃\na. Writeanexpressionforthelog-likelihood, 𝐿𝐿(𝜃) = log𝐿(𝜃).\nb. Whycanweoptimize 𝐿𝐿(𝜃) ratherthan 𝐿(𝜃)?\nc. Whymight weoptimize 𝐿𝐿(𝜃) ratherthan 𝐿(𝜃)?\n1.2 Beta\na. Supposeyouhaveacoinwhereyouhavenopriorbeliefonitstrueprobabilityofheads 𝑝.How\ncanyoumodelthisbeliefasaBetadistribution?\nb. Supposeyouhaveacoinwhichyoubelieveisfair,with”strength” 𝛼.Thatis,pretendyou’ve\nseen 𝛼 headsand 𝛼 tails.HowcanyoumodelthisbeliefasaBetadistribution?\nc. Nowsupposeyoutakethecoinfromthepreviouspartandflipit10times.Yousee8headsand\n2tails.Howcanyoumodelyourposteriorbeliefofthecoin’sprobabilityofheads?\n1.3 Beta Sum\nWhatisthedistributionofthesumof100iidBetas?Let 𝑋 bethesum\n100\n∑︁\n𝑋 = 𝑋 𝑖 whereeach 𝑋 𝑖 ∼ Beta(𝑎 = 3,𝑏 = 4)\n𝑖=1\nNotethevarianceofaBeta:\n𝑎𝑏\nVar(𝑋 𝑖) = where 𝑋 𝑖 ∼ Beta(𝑎,𝑏)\n(𝑎 +𝑏)2(𝑎 +𝑏 +1)\n–2–\n2 Problems\n2.1 Variance of Hemoglobin Levels\nAmedicalresearchertreatspatientswithdangerouslylowhemoglobinlevels.Shehasformulatedtwo\nslightlydifferentdrugsandisnowtestingthemonpatients.First,sheadministereddrugAtoone\ngroupof50patientsanddrugBtoaseparategroupof50patients.Then,shemeasuredallthe\npatients’hemoglobinlevelspost-treatment.Forsimplicity,assumethatallvariationinthepatient\noutcomesisduetotheirdifferentreactionstotreatment.\nTheresearchernotesthatthesamplemeanissimilarbetweenthetwogroups:bothhavemean\nhemoglobinlevelsaround10g\/dL.However,drugB’sgrouphasasamplevariancethatis3(g\/dL)2\ngreaterthandrugA’sgroup.TheresearcherthinksthatpatientsrespondtodrugsAandBdifferently.\nSpecifically,shewantstomakethescientificclaimthatdrugA’spatientswillendupwitha\nsignificantlydifferentspreadofhemoglobinlevelscomparedtodrugB’s.\nYouareskeptical.Itispossiblethatthetwodrugshavepracticallyidenticaleffectsandthatthe\nobserveddifferentinvariancewasaresultofchanceandasmallsamplesize,i.e.thenull\nhypothesis.Calculatetheprobabilityofthenullhypothesisusingbootstrapping.Hereisthedata.\nEachnumberisthelevelofanindependentlysampledpatient:\nHemoglobinLevelsofDrugA’sGroup(𝑆2 =6.0):13,12,7,16,9,11,7,10,9,8,9,7,16,7,9,8,\n13,10,11,9,13,13,10,10,9,7,7,6,7,8,12,13,9,6,9,11,10,8,12,10,9,10,8,14,13,13,10,\n11,12,9\nHemoglobinLevelsofDrugB’sGroup(𝑆2 =9.1):8,8,16,16,9,13,14,13,10,12,10,6,14,8,\n13,14,7,13,7,8,4,11,7,12,8,9,12,8,11,10,12,6,10,15,11,12,3,8,11,10,10,8,12,8,11,6,\n7,10,8,5\nCompletethepromptsinthisColabnotebooktoinvestigatethisquestionusingbootstrapping.\n2.2 Parameter Estimation and Wealth Distribution\nThebroaderfieldofeconomicsalsoreliesonlikelihoodestimationandparameterestimation.One\ncontinuousprobabilitydistribution—onewithalongtailas𝑥 approachesinfinity—isusedtomodel\nwealthinequalityandthesocioeconomicproblemsthatstemfromit.Thisprobabilitydistributionis\ngivenas:\n𝜔3𝜔\n𝑓(𝑥|𝜔) = , where𝑥 ≥ 3,𝜔 > 1\n𝑥𝜔+1\nAssumethatyou’veobservedasampleofiidrandomvariables (𝑋 1,𝑋 2,𝑋 3,...,𝑋 𝑛),whereeachofthe\n𝑋 𝑖 ismodeledaccordingtotheaboveprobabilitydistributionfunction.\na. Whatisthelog-likelihoodfunction 𝐿𝐿(𝜔) ofthesample (𝑋 1,𝑋 2,𝑋 3,...,𝑋 𝑛)?Simplifyusing\npropertiesoflogarithmswhereverpossible.\nb. Setuptheequationthatwouldneedtobesolvedinordertocompute𝜔ˆ𝑀𝐿𝐸.Onceyouarriveat\ntheequationandhaveworkedthroughanycalculus,youcanstopandsimplypresentthe\nequationthatcanbesolvedviasimplealgebraicmanipulation.\n–3–\n2.3 Dirk\/Evan Showdown\nDirkandEvanaresettocompeteinamatchofsevengames,andtheplayerwinningthemostwillbe\nrewardedwithamedalfromtheRulerofallofBayestopia,QueenDoris.Theoutcomesofeachof\nthesevengamesareindependentofoneanother,andDirkwinseachgamewithaprobability 𝑝 (so\nthatEvanwinswithprobability1− 𝑝).Unfortunately, 𝑝 isunknown,soyoudecidetomodel 𝑝 itself\nasaBetarandomvariablesuchthat 𝑝 ∼ 𝐵𝑒𝑡𝑎(1,1).\nTolearnmoreabout 𝑝,youreaduponalloftheirpreviousgames,findthatthey’vealreadycompeted\n12times,andlearnthatEvanhaswon7ofthose12gamesinthisorder:WLLLWWLWWLWW.\nEvenifEvan’sedgeiseversoslight,heappearstobetheapriorifavorite.\na. Findtheposteriordistributionof 𝑝 givenDirkandEvan’spriorhistorycompetingagainstone\nanother.\nb. RecallthatthePDFofa 𝐵𝑒𝑡𝑎 distributiononintegerparameters 𝑎 and 𝑏 isgivenas:\n1\n𝑓(𝑥|𝑎,𝑏) = 𝑥𝑎−1(1−𝑥)𝑏−1,where 𝐵(𝑎,𝑏) isanormalizationconstant\n𝐵(𝑎,𝑏)\nAssumingourhyperparameters 𝑎 and 𝑏 arepositiveintegers,explainwhytheexpectedvalue\nof\n𝑝𝑚(1− 𝑝)𝑛\n—thatis,\n𝐸[𝑝𝑚(1− 𝑝)𝑛]—isgivenby\n𝐵(𝑎 +𝑚,𝑏 +𝑛)\n𝐸[𝑝𝑚\n(1−\n𝑝)𝑛\n] =\n𝐵(𝑎,𝑏)\nYoucanassumethat 𝑚 and 𝑛 arenonnegativeintegersaswell.\nc. Relyingonlyontheposteriordistributionof 𝑝 thatyoucomputedforpartaandtheresultfrom\npartb,computetheexpectedprobabilitythatthebest-of-sevenmatchbetweenDirkandEvan\nisn’tsettledafterthefirstsixgamesandthereforerequirestheseventhbeplayed? <END>"}
{"prompt":"02_section_soln.txt\n\n###\n\n","completion":" CS109 April18,2024\nSection 2: Conditional Probability and Bayes\nChrisPiech,MehranSahami,JerryCain,LisaYan,andnumerousCS109CA’s.\nOverview of Section Materials\nThewarm-upquestionsprovidedwillhelpstudentspracticeconceptsintroducedinlectures.Thesectionprob-\nlemsaremeanttoapplytheseconceptsinmorecomplexscenariossimilartowhatyouwillseeinproblemsets\nandexams.Infact,manyofthemareoldexamquestions.\nBeforeyouleavelab,makesureyouclickheresothatyou’remarkedashavingattendedthisweek’ssection.The\nCAleadingyourdiscussionsectioncanenterthepasswordneededonceyou’vesubmitted.\nWarm-ups\n1. Definitions:CiteBayes’Theorem.Canyouexplainwhy 𝑃(𝐴|𝐵) isdifferentthan 𝑃(𝐵|𝐴)?\n2. TrueorFalse.NotethattruemeanstrueforALLcases.\n(a) Ingeneral, 𝑃(𝐴𝐵|𝐶) = 𝑃(𝐵|𝐶)𝑃(𝐴|𝐵𝐶)\n(b) If 𝐴 and 𝐵 areindependent,soare 𝐴 and 𝐵𝐶 .\n1. Bayes’Theorem: 𝑃(𝐸|𝐹) = 𝑃(𝐹|𝐸)𝑃(𝐸) .\n𝑃(𝐹)\nAnd 𝑃(𝐴|𝐵) isdifferentthan 𝑃(𝐴|𝐵) becausethefirstcomputestheprobabilitythat 𝐴 occurswiththeun-\nderstandthat 𝐵 hasoccurred,andthesecondcomputestheprobabilitythat 𝐵 occurswiththeunderstand\nthat 𝐴 hasoccurred.Anexample: 𝑃(𝐴|𝐵) mightbetheprobabilitythatyouhavetheflubecauseyou’re\nrunningahighfever,whereas 𝑃(𝐵|𝐴) wouldthenbetheprobabilityyougetahighfeverasaresultof\ncomingdownwiththeflu.Thefirstmightbeasmallprobability,becausetherecanbeallkindsofreasons\nyouhaveahighfever(bacterialinfection,overexertion,heatstroke,orasideeffectofsomemedication)\nwhereasthesecondcouldbeveryhigh(i.e.,perhapsit’salmostalwaysthecasethattheflupresentsitself\nwithahighfever).\n2. (a) True\n𝑃(𝐴𝐵|𝐶) Leftside\n𝑃(𝐴𝐵𝐶)\nDef’nCond’nProb\n𝑃(𝐶)\n𝑃(𝐴|𝐵𝐶)𝑃(𝐵𝐶)\nChainRule\n𝑃(𝐶)\n𝑃(𝐴|𝐵𝐶)𝑃(𝐵|𝐶)𝑃(𝐶)\nChainRule\n𝑃(𝐶)\n𝑃(𝐴|𝐵𝐶)𝑃(𝐵|𝐶) Cancellation\n𝑃(𝐵|𝐶)𝑃(𝐴|𝐵𝐶) ■\n(b) True\nStartfromLawofTotalProbability(thisisagoodcandidateforastartingpointbecauseitrelates 𝐴,\n𝐵 and 𝐵∁ ).Wewillemploytheassumptionthat 𝐴 ⊥ 𝐵 (i.e. 𝐴 isindependentof 𝐵)somewhere,and\nthentrytoseeifwecanarriveattheequation 𝑃(𝐴𝐵∁) = 𝑃(𝐴)𝑃(𝐵∁) (thisiswhatitmeansfor 𝐴 and\n𝐵∁\ntobeindependentmathematically).\n𝑃(𝐴) = 𝑃(𝐴𝐵) + 𝑃(𝐴𝐵∁ ) LOTP\n𝑃(𝐴) = 𝑃(𝐴)𝑃(𝐵) + 𝑃(𝐴𝐵∁ ) 𝐴 ⊥ 𝐵\n𝑃(𝐴) − 𝑃(𝐴)𝑃(𝐵) = 𝑃(𝐴𝐵∁ ) Subtract 𝑃(𝐴)𝑃(𝐵)\n𝑃(𝐴)(1− 𝑃(𝐵)) = 𝑃(𝐴𝐵∁ ) Factor 𝑃(𝐴)\n𝑃(𝐴)𝑃(𝐵∁ ) = 𝑃(𝐴𝐵∁ ) 1− 𝑃(𝐵) := 𝑃(𝐵∁ )\n𝑃(𝐴𝐵∁ ) = 𝑃(𝐴)𝑃(𝐵∁ ) ■\n1 Taking Expectation: Breaking Vegas\nPreamble:Whenarandomvariablefitsneatlyintoafamilywe’veseenbefore(e.g.Binomial),wegetitsexpec-\ntationforfree.Whenitdoesnot,wehavetousethedefinitionofexpectation.\nProblem:Ifyoubeton“Red”inRoulette,thereis 𝑝 = 18\/38thatyouwithwin$Yanda (1−𝑝) probabilitythat\nyoulose$Y.Considerthisalgorithmforaseriesofbets:\nLetY=$1.FirstyoubetY.Ifyouwin,thenstop.Ifyoulose,thensetYtobe2Yandrepeat.\nWhatareyourexpectedwinningswhenyoustop?Itwillhelptorecallthatthesumofageometricseries 𝑎0+𝑎1+\n𝑎2 +··· = 1 if0 < 𝑎 < 1.Vegasbreaksyou:Whydoesn’teveryonedothis?\n1−𝑎\nLetXbethenumberofdollarsthatyourearn.\nThepossiblevaluesofxarefromtheoutcomesof:winningonyourfirstbet,winningonyoursecondbet,andso\non.\n18 2018 (cid:16)20(cid:17)218\n𝐸[𝑋] = + (2−1) + (4−2−1) +...\n38 3838 38 38\n∑︁∞ (cid:16)20(cid:17)𝑖(cid:16)18(cid:17)(cid:16) ∑︁𝑖−1 (cid:17)\n𝑖 𝑗\n= 2 − 2\n38 38\n𝑖=0 𝑗=0\n(cid:16)18(cid:17)∑︁∞ (cid:16)20(cid:17)𝑖\n=\n38 38\n𝑖=0\n(cid:16)18(cid:17) 1\n= = 1\n38 1− 20\n38\nRealgameshavemaximumbetamounts.Youhavefinitemoneyandcasinoscankickyouout.But,ifyouhadno\nbettinglimitsandinfinitemoney,thengoforit!(andtellmewhichplanetyouarelivingon).\n2 Conditional Probabilities: Missing Not at Random\nPreamble:Wehavethreebigtoolsformanipulatingconditionalprobabilities:\n• Definitionofconditionalprobability: 𝑃(𝐸𝐹) = 𝑃(𝐸|𝐹)𝑃(𝐹)\n• LawofTotalProbability: 𝑃(𝐸) = 𝑃(𝐸𝐹) + 𝑃(𝐸𝐹𝐶) = 𝑃(𝐸|𝐹)𝑃(𝐹) + 𝑃(𝐸|𝐹𝐶)𝑃(𝐹𝐶)\n• BayesRule: 𝑃(𝐸|𝐹) = 𝑃(𝐹|𝐸)𝑃(𝐸) = 𝑃(𝐹|𝐸)𝑃(𝐸)\n𝑃(𝐹) 𝑃(𝐹|𝐸)𝑃(𝐸)+𝑃(𝐹|𝐸𝐶)𝑃(𝐸𝐶)\nThisisagoodtimetocommitthesethreetomemoryandstartthinkingaboutwheneachofthemisuseful.\nProblem:YoucollectdataonwhetherornotpeopleintendtovoteforAyesha,acandidateinanupcomingelec-\ntion.Yousendanelectronicpollto100randomlychosenpeople.Youassumeall100responsesareindependent\nandidenticallydistributed.\nUserResponse Count\nRespondedthattheywillvoteforAyesha 40\nRespondedthattheywillnotvoteforAyesha 45\nDidnotrespond 15\nLet 𝐴 betheeventthatapersonwillvoteforAyesha.Let 𝑀 betheeventthatauserdidnotrespondtothepoll.\nWeareinterestedinestimating 𝑃(𝐴),thoughcomputingthatestimateisdifficult,giventhat15usersdidn’tactu-\nallyrespond.\na. WhatistheprobabilitythatausersaidtheywillvoteforAyeshaandthattheyrespondedtothepoll 𝑃(𝐴 and 𝑀𝐶)?\nb. Whichformulafromclasswouldyouusetocalculate 𝑃(𝐴)?Yourformulashouldrelyonthecontextthat\nvotersforAyeshaareinoneoftwo(mutuallyexclusive)groups:thosethatmissedthepoll,andthosethat\ndidnot.\nc. Calculatethe 𝑃(𝐴).Youestimatethattheprobabilitythatavoterismissing,giventhattheyweregoingto\nvoteforAyeshais 𝑃(𝑀|𝐴) = 1.\n5\na. 𝑃(𝐴 and 𝑀𝐶) = 40 .The 𝑀𝐶 partisredundant.\n100\nb. Thelawoftotalprobability.Itbreaksdown 𝑃(𝐴) intotwoparts,thepartwhichintersectswith 𝑀 andthe\npartthatintersectionswith\n𝑀𝐶\n.\n𝑃(𝐴) = 𝑃(𝐴 and 𝑀) + 𝑃(𝐴 and 𝑀𝐶 )\nc.\n𝑃(𝐴) = 𝑃(𝐴 and 𝑀𝐶 ) + 𝑃(𝐴 and 𝑀) Lawoftotalprobability\n40\n= + 𝑃(𝐴 and 𝑀) Fromparta\n100\n40\n= + 𝑃(𝑀|𝐴)𝑃(𝐴) Chainrule\n100\n40\n𝑃(𝐴) − 𝑃(𝑀|𝐴)𝑃(𝐴) = Therestisalgebra\n100\n40\n𝑃(𝐴) · [1− 𝑃(𝑀|𝐴)] =\n100\n4 40\n𝑃(𝐴) · =\n5 100\n40 5\n𝑃(𝐴) = ·\n100 4\n1\n𝑃(𝐴) =\n2\n3 Sending Bits to Space\nPreamble: Whensendingbinarydatatosatellites(orreallyoveranynoisychannel),thebitscanbeflippedwith\nhighprobability.In1947,RichardHammingdevelopedasystemtomorereliablysenddata.ByusingErrorCor-\nrectingHammingCodes,youcansendastreamof4bitsalongwith3redundantbits.Ifzerooroneoftheseven\nbitsarecorrupted,usingerrorcorrectingcodes,areceivercanidentifytheoriginal4bits.\nProblem: Letsconsiderthecaseofsendingasignaltoasatellitewhereeachbitisindependentlyflippedwith\nprobability 𝑝 = 0.1.\na. Ifyousend4bits,whatistheprobabilitythatthecorrectmessagewasreceived(i.e.noneofthebitsare\nflipped).\nb. Ifyousend4bits,with3Hammingerrorcorrectingbits,whatistheprobabilitythataninterpretablemes-\nsage(i.e.amessagewithzerooroneerrors)wasreceived?\nc. InsteadofusingHammingcodes,youdecidetosend100copiesofeachofthefourbits.Ifforeverysingle\nbit,morethan50ofthecopiesarenotflipped,thesignalwillbecorrectable.Whatistheprobabilitythata\ncorrectablemessagewasreceived?\nHammingcodesaresuperinteresting.It’sworthlookingupifyouhaven’tseenthembefore!\na. LetYbethenumberof4bitscorrupted.Then 𝑃(𝑌 = 𝑘) isgivenas:\n(cid:18) (cid:19)\n4\n𝑃(𝑌 = 0) = (0.1)0(0.9)4 = 0.656\n0\nb. LetZbethenumberof7bitscorrupted.Acorrectablemessageisreceivedif 𝑍 equals0or1:\n𝑃(correctable) = 𝑃(𝑍 = 0) + 𝑃(𝑍 = 1)\n(cid:18) (cid:19) (cid:18) (cid:19)\n7 7\n= (0.1)0(0.9)7 + (0.1)1(0.9)6 = 0.850\n0 1\nThatisa30%improvement!\nc. Let 𝑋 𝑖 bethenumberofcopiesofbit𝑖 whicharenotcorrupted.Wecanrepresenteachasarandomvari-\nableaswedidinpartsaandb.\n4\n(cid:214)\n𝑃(correctable) = 𝑃(𝑋 𝑖 > 50)\n𝑖=1\n4 100\n(cid:214) ∑︁\n= 𝑃(𝑋 𝑖 = 𝑗)\n𝑖=1 𝑗=51\n4 100 (cid:18) (cid:19)\n(cid:214) ∑︁ 100\n=\n(0.9)𝑗 (0.1)100−𝑗\n𝑗\n𝑖=1 𝑗=51\n100 (cid:18) (cid:19)\n(cid:16) ∑︁ 100 (cid:17)4\n= (0.9)𝑗 (0.1)100−𝑗 > 0.999\n𝑗\n𝑗=51\nButnowyouneedtosend400bits,insteadofthe7requiredbyhammingcodes:-).\nExtra:Explanationofthe”Hamming(7,4)”technique\nIfwearetryingtotransmit4bits,wecansendanadditional3”parity”bitsthatwecanusetocorrectourorig-\ninalmessageifabitgetsflippedduetoanerrorintransmission.Considerthediagram.Thedatabitsare 𝑑\n1\nthrough 𝑑 .The”parity”bitsare 𝑝 through 𝑝 .Aparitybitissettowhatevervaluewouldmakeit’slargecir-\n4 1 3\nclehaveanevennumberofbits.Forexample,thegreencircleconsistsof 𝑝 , 𝑑 , 𝑑 ,and 𝑑 .If 𝑑 = 1, 𝑑 = 1,\n1 1 2 4 1 2\nand 𝑑 = 1,then 𝑝 wouldbesetto1inordertoensurethereareanevennumberofbitsinthatcircle(inthis\n4 1\ncase,4bits).\nConvinceyourselfthatasingleerrorwhichappearedinanybitcouldbeidentifiedandcorrected!Forexample,\nif 𝑑 isflipped,itwouldthrowofftheparityforthegreenandredcircles.Therefore,flipping 𝑑 backistheonly\n2 2\nwaytocorrecttheparity.Asanotherexample,if 𝑝 isflipped,thenonlythebluecirclewouldhaveaparityis-\n2\nsue,andflipping 𝑝 backistheuniquesolutiontofixingtheparity.\n2 <END>"}
{"prompt":"cs109_lec04_conceptcheck.txt\n\n###\n\n","completion":" Lecture 4: Concept Check  Graded\nStudent\nNeetish Sharma\nTotal Points\n5 \/ 5 pts\nQuestion 1\nBayes' Theorem: Terminology 3 \/ 3 pts\n1.1 Prior 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\n1.2 Likelihood 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\n1.3 Posterior 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 2\nConditional Probability: Rolling Fair Dice 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 3\nBayes' Theorem: Diagnosing An Illness 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQ1 Bayes' Theorem: Terminology\n3 Points\nP(B∣A)P(A)\nConsider Bayes' Theorem written as follows: P(A∣B) = .\nP(B)\nQ1.1 Prior\n1 Point\nWhich of these is the known as the prior?\nP(A)\nP(B∣A)\nP(A∣B)\nP(B)\nQ1.2 Likelihood\n1 Point\nWhich of these is the known as the likelihood?\nP(A)\nP(A∣B)\nP(B∣A)\nP(B)\nQ1.3 Posterior\n1 Point\nWhich of these is the known as the posterior?\nP(B∣A)\nP(A∣B)\nP(A)\nP(B)\nQ2 Conditional Probability: Rolling Fair Dice\n1 Point\nAssume that two fair, six-sided dice are rolled, and let D and D be the values\n1 2\nof the first and second die rolls, respectively. What is the probability that D = 3\n1\ngiven that D + D < 7?\n1 2\n1\n6\n1\n5\n3\n7\n4\n7\n7\n9\nQ3 Bayes' Theorem: Diagnosing An Illness\n1 Point\nConsider the following hypothetical scenario regarding an illness. Consider that\n8% of all people have the illness and further that there has been a test developed\nfor the illness with a 95% true positive rate and a 7% false positive rate ).\nAssuming someone tests positive for the illness, what is the probability that they\nactually have the disease? Please round your answer to two decimal places, e.g.,\n0.17, with the leading zero.\n0.54 <END>"}
{"prompt":"cs109_lec22_conceptcheck.txt\n\n###\n\n","completion":" Lecture 22: Concept Check  Graded\nStudent\nNeetish Sharma\nTotal Points\n4 \/ 4 pts\nQuestion 1\nMAP vs MLE: Intuition 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 2\nBiased Coin 3 \/ 3 pts\n2.1 MLE Refresh 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\n2.2 MAP Part 1 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\n2.3 MAP Part 2 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQ1 MAP vs MLE: Intuition\n1 Point\nWhich of these statements correctly identifies the main conceptual difference\nbetween MAP and MLE?\nMLE will choose θ to maximize the data likelihood and so will MAP.\nMLE will choose θ to maximize the data likelihood and MAP will choose θ\nto minimize mean squared error.\nMAP will choose θ to maximize the data likelihood and MLE will choose θ\nto minimize mean squared error.\nMLE will choose θ to maximize the data likelihood and MAP will choose θ\nto maximize the product of the data likelihood and the prior belief on θ.\nQ2 Biased Coin\n3 Points\nConsider that we have a coin from an alien land which flips heads with\nprobability p. We flip the coin 10 times and see the following sequence of\noutcomes: H,H,T,T,H,T,T,T,H,T\nQ2.1 MLE Refresh\n1 Point\nWhat is the maximum likelihood estimate of p? Please write your answer as a\ndecimal to 2 decimal places with the leading zero (e.g., 0.12).\n0.40\nQ2.2 MAP Part 1\n1 Point\nIf I impose a Uniform prior on p (i.e., p ∼ Uni(0,1), what is my MAP estimate of\np? Please write your answer as a decimal to 2 decimal places with the leading\nzero (e.g., 0.12).\n0.40\nQ2.3 MAP Part 2\n1 Point\nIf I impose a prior on p, namely that p ∼ Beta(10,10). What is my MAP\nestimate of p? Please write your answer as a decimal to 2 decimal places with the\nleading zero (e.g., 0.12).\n0.46 <END>"}
{"prompt":"cs109_lec11_conceptcheck.txt\n\n###\n\n","completion":" Lecture 11: Concept Check  Graded\nStudent\nNeetish Sharma\nTotal Points\n2 \/ 2 pts\nQuestion 1\nMarginal Probability of Joint PMFs 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 2\nJoint PMF Facts 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQ1 Marginal Probability of Joint PMFs\n1 Point\nSuppose we have two random variables, X and Y , whose joint probability mass\nfunction, P X,Y(a,b) = P(X = a,Y = b), is defined by the table below.\nWhat is the marginal probability distribution function of the random variable X?\nP(X = 1) = 0.4\nP(X = 1) = 0.5,P(X = 2) = 0.4,P(X = 3) = 0.3\nP(Y = 7) = 0.6,P(Y = 9) = 0.4\nP(X = 1) = 0.4,P(X = 2) = 0.4,P(X = 3) = 0.2\nQ2 Joint PMF Facts\n1 Point\nSelect all of the following that are true about joint PMFs.\n∑ ∑ P(X = a,Y = b) = 1\na b\n∑ P(X = a,Y = b) = P(Y = b)\na\n∑ P(X = a,Y = b) = P(X = b)\na\n∑ P(X = a,Y = b) = P(X = a)\na\n∑ ∑ P(X = a,Y = b) = ∑ P(X = a) = 1\na b a <END>"}
{"prompt":"cs109_lec20_conceptcheck.txt\n\n###\n\n","completion":" Lecture 20: Concept Check  Graded\nStudent\nNeetish Sharma\nTotal Points\n3 \/ 3 pts\nQuestion 1\nQuestion 1: Key Definitions 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 2\nQuestion 2: Why Prefer LL over L? 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 3\nQuestion 3: Computer Failture Rates 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQ1 Question 1: Key Definitions\n1 Point\nWhat is the general definition of the likelihood L(θ) if you have a sample of i.i.d.\nrandom variables X 1,X 2,…,X n?\nn\nL(θ) = ∏ f(X ∣θ)\ni=1 i\nn\nL(θ) = ∑ f(X ∣θ)\ni=1 i\nL(θ) = f(X ∣θ)\ni\nQ2 Question 2: Why Prefer LL over L?\n1 Point\nWhen we maximize the likelihood function L(θ), it's generally better maximize\nthe log of the likelihood function—we call it LL(θ)—instead. Why do we\ngenerally work with LL(θ)?\nNumerical stability.\nDerivatives of sums are easier to manipulate than derivatives of products.\nQ3 Question 3: Computer Failture Rates\n1 Point\nSuppose you observe the following durations—recorded in minutes—in between\nisolated computer failures at a datacenter:\n1.2,8.5,0.8,2.2,6.1,3.4,1.9,0.5,1.7,5.4,2.8,0.9,3.3,1.5,4.8\nAssume all durations are generated, and therefore nicely modeled by, an\nExponential distribution. What's λ MLE? (Note that the 15 numbers in your\ndataset add up to 45.)\nHint: You can certainly maximize the relevant LL(λ) by differentiating it, setting\nit to zero, and then solving for λ. But it's easier to compute the average duration\nand assume that average is an unbiased estimate of the Exponential's true mean\nvalue. Enter your answer so that it's accurate to three decimal places.\n0.333 <END>"}
{"prompt":"cs109_lec19_conceptcheck.txt\n\n###\n\n","completion":" Lecture 19: Concept Check  Graded\nStudent\nNeetish Sharma\nTotal Points\n5 \/ 5 pts\nQuestion 1\nMeans and Sampling 2 \/ 2 pts\n1.1 Definitions 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\n1.2 Randomness 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQuestion 2\nCentral Limit Theorem 2 \/ 2 pts\n + 2 pts Correct\n+ 0 pts Incorrect\nQuestion 3\nBootstrapping 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQ1 Means and Sampling\n2 Points\nXˉ and μ are both terms that come up in sampling and statistics.\nQ1.1 Definitions\n1 Point\nWhich of the following statements correctly define Xˉ and μ in the context of\nsampling from a distribution in statistics?\nThey are different notations for the same quantity (the population mean)\nThey are different notations for the same quantity (the sample mean)\nμ is the population mean and Xˉ is the sample mean\nμ is the sample mean and Xˉ is the population mean\nQ1.2 Randomness\n1 Point\nSampling implicitly introduces randomness when drawing from a distribution.\nHow do we describe the randomness of Xˉ and μ?\nOnly\nXˉ\nis a random variable\nOnly μ is a random variable\nBoth quantities are random variables\nQ2 Central Limit Theorem\n2 Points\nThe Central Limit Theorem (CLT) is one of the most important results in\nprobability, statistics, and data analysis, and it greatly informs why sampling and\nbootstrapping are valid data analysis techniques. Relying on what you learned\nabout the Central Limit Theorem last lecture and what you learned about\nsampling during today's lecture, select all of the statements below that are true.\nVar(Xˉ\n), the variance of the sample mean, is inversely proportional to\nthe size of the sample\nVar(Xˉ\n), the variance of the sample mean, grows linearly with the size of\nthe sample\nXˉ\n, the sample mean, grows linearly with the size of the sample\nThe population mean μ drawn, from any type of distribution is\ndistributed, normally\nXˉ\n, the mean of a sample drawn iid. from any type of distribution, is\ndistributed normally\n∑\ni\nX i, the sum of a sample drawn iid. from any type of distribution, is\ndistributed normally\nQ3 Bootstrapping\n1 Point\nBootstrapping relies on the following intuition:\nYour population distribution can be approximated as a Gaussian\nYour sample distribution is Gaussian\nYour sample distribution approximates your population distribution. <END>"}
{"prompt":"05_independence_annotated.txt\n\n###\n\n","completion":" 05: Independence\nJerry Cain\nApril 10th, 2024\nLecture Discussion on Ed\n1\nIndependence I\n2\nIndependence\nTwo events 𝐸 and 𝐹 are defined as independent if:\n𝑃 𝐸𝐹 = 𝑃 𝐸 𝑃(𝐹)\nOtherwise 𝐸 and 𝐹 are called dependent events.\nIf 𝐸 and 𝐹 are independent, then:\n𝑃 𝐸 𝐹 = 𝑃(𝐸)\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 3\nIndependent\nIntuition through proof 𝑃 𝐸𝐹 = 𝑃 𝐸 𝑃(𝐹)\nevents 𝐸 and 𝐹\nStatement:\nIf 𝐸 and 𝐹 are independent, then 𝑃 𝐸 𝐹 = 𝑃 𝐸 .\nProof:\n𝑃 𝐸𝐹\nDefinition of\n𝑃 𝐸 𝐹 =\n𝑃(𝐹)\nconditional probability\n𝑃 𝐸 𝑃 𝐹\nIndependence of 𝐸 and 𝐹\n=\n𝑃(𝐹)\n= 𝑃(𝐸 ) Taking the bus to cancellation city\nKnowing that 𝐹 happened does not\nchange our belief that 𝐸 happened.\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 4\nIndependent 𝑃 𝐸𝐹 = 𝑃 𝐸 𝑃(𝐹)\nDice, our misunderstood friends\nevents 𝐸 and 𝐹 𝑃 𝐸|𝐹 = 𝑃(𝐸)\n• Roll two 6-sided dice, yielding values 𝐷 and 𝐷 .\n# $\n• Let event 𝐸: 𝐷 = 1\n#\nevent 𝐹: 𝐷 = 6\n$\nevent 𝐺: 𝐷 + 𝐷 = 5\n# $ 𝐺 = { 1,4 , 2,3 , 3,2 , 4,1 }\n1. Are 𝐸 and 𝐹 independent? 2. Are 𝐸 and 𝐺 independent?\n𝑃 𝐸 = 1\/6 𝑃 𝐸 = 1\/6\n𝑃 𝐹 = 1\/6 𝑃 𝐺 = 4\/36 = 1\/9\n𝑃 𝐸𝐹 = 1\/36 𝑃 𝐸𝐺 = 1\/36 ≠ 𝑃 𝐸 𝑃(𝐺)\n✅ independent ❌ dependent\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 5\nGeneralizing independence\n𝑃 𝐸𝐹𝐺 = 𝑃 𝐸 𝑃 𝐹 𝑃 𝐺 , and\nThree events 𝐸, 𝐹, and 𝐺\n𝑃 𝐸𝐹 = 𝑃 𝐸 𝑃(𝐹), and\nare independent if:\n𝑃 𝐸𝐺 = 𝑃 𝐸 𝑃(𝐺), and\n𝑃 𝐹𝐺 = 𝑃 𝐹 𝑃(𝐺)\nfor 𝑟 = 1, … , 𝑛:\n𝑛 events 𝐸 , 𝐸 , … , 𝐸 are\n# $ % for every subset 𝐸 , 𝐸 , … , 𝐸 :\n( ) *\nindependent if:\n𝑃 𝐸 𝐸 … 𝐸 = 𝑃 𝐸 𝑃 𝐸 ⋯ 𝑃 𝐸\n( ) * ( ) *\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 6\nDice, increasingly misunderstood (still our friends)\n• Each roll of a 6-sided die is an independent trial.\n• Two rolls: 𝐷 and 𝐷 .\n# $\n• Let event 𝐸: 𝐷 = 1\n#\nevent 𝐹: 𝐷 = 6\n$\nevent 𝐺: 𝐷 + 𝐷 = 7 𝐺 = { 1,6 , 2,5 , 3,4 , 4,3 , 5,2 , 6,1 }\n# $\n1. Are 𝐸 and 𝐹 2. Are 𝐸 and 𝐺 3. Are 𝐹 and 𝐺 4. Are 𝐸, 𝐹, 𝐺\nindependent? independent? independent? independent?\n✅\n𝑃 𝐸𝐹 = 1\/36\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 7\nDice, increasingly misunderstood (still our friends)\n• Each roll of a 6-sided die is an independent trial.\n• Two rolls: 𝐷 and 𝐷 .\n# $\n• Let event 𝐸: 𝐷 = 1\n#\nevent 𝐹: 𝐷 = 6\n$\nevent 𝐺: 𝐷 + 𝐷 = 7 𝐺 = { 1,6 , 2,5 , 3,4 , 4,3 , 5,2 , 6,1 }\n# $\n1. Are 𝐸 and 𝐹 2. Are 𝐸 and 𝐺 3. Are 𝐹 and 𝐺 4. Are 𝐸, 𝐹, 𝐺\nindependent? independent? independent? independent?\n✅ ✅ ✅ ❌\n𝑃 𝐸𝐹 = 1\/36\nPairwise independence is not sufficient to prove independence of 3 or more events!\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 8\nIndependence II\n9\nIndependent trials\nWe often are interested in experiments consisting of 𝑛 independent trials.\n• 𝑛 trials, each with the same set of possible outcomes\n• 𝑛-way independence: an event in one subset of trials is independent of\nevents in other subsets of trials\nExamples:\n• Flip a coin 𝑛 times\n• Roll a die 𝑛 times\n• Send a multiple-choice survey to 𝑛 people\n• Send 𝑛 web requests to 𝑘 different servers\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 10\nNetwork reliability\nConsider the following parallel network: 𝑝\n(\n• 𝑛 independent routers, each with 𝑝\n)\nprobability 𝑝 of functioning (where 1 ≤ 𝑖 ≤ 𝑛)\n+\n• 𝐸 = functional path from A to B exists.\n𝑝\n,\nWhat is 𝑃 𝐸 ?\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 11\n…\n𝐴 𝐵\nNetwork reliability\nConsider the following parallel network: 𝑝\n(\n• 𝑛 independent routers, each with 𝑝\n)\nprobability 𝑝 of functioning (where 1 ≤ 𝑖 ≤ 𝑛)\n+\n• 𝐸 = functional path from A to B exists.\n𝑝\n,\nWhat is 𝑃 𝐸 ?\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 12\n…\n𝐴 𝐵\n𝑃 𝐸 = 𝑃 ≥ 1 one router works\n= 1 − 𝑃 all routers fail\n= 1 − 1 − 𝑝 1 − 𝑝 ⋯ 1 − 𝑝\n# $ %\n%\n= 1 − @ 1 − 𝑝 ≥ 1 with independent trials:\n&\ntake complement\n&’#\nExercises\n13\nIndependent 𝑃 𝐸𝐹 = 𝑃 𝐸 𝑃(𝐹)\nIndependence?\nevents 𝐸 and 𝐹 𝑃 𝐸|𝐹 = 𝑃(𝐸)\n1. True or False? Two events 𝐸 and 𝐹 are independent if:\nA. Knowing that 𝐹 happens means that 𝐸 can’t happen.\nB. Knowing that 𝐹 happens doesn’t change probability that 𝐸 happened.\n2. Are 𝐸 and 𝐹 independent in the following pictures?\nA. B.\nE\nE\n1\/4 2\/9 4\/9\n1\/4\nF\nS F 1\/9 S\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 14\nIndependent 𝑃 𝐸𝐹 = 𝑃 𝐸 𝑃(𝐹)\nIndependence?\nevents 𝐸 and 𝐹 𝑃 𝐸|𝐹 = 𝑃(𝐸)\n1. True or False? Two events 𝐸 and 𝐹 are independent if:\nA. Knowing that 𝐹 happens means that 𝐸 can’t happen.\nB. Knowing that 𝐹 happens doesn’t change probability that 𝐸 happened.\n2. Are 𝐸 and 𝐹 independent in the following pictures?\nA. B.\nE\nE\n1\/4 2\/9 4\/9\n1\/4\nF\nS F 1\/9 S\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 15\nCoin Flips\nSuppose we flip a coin 𝑛 times. Each coin flip is an independent trial with\nprobability 𝑝 of coming up heads. Write an expression for the following:\n1. 𝑃(𝑛 heads on 𝑛 coin flips)\n2. 𝑃(𝑛 tails on 𝑛 coin flips)\n3. 𝑃(first 𝑘 heads, then 𝑛 − 𝑘 tails)\n4. 𝑃(exactly 𝑘 heads on 𝑛 coin flips)\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 16\nCoin Flips\nSuppose we flip a coin 𝑛 times. Each coin flip is an independent trial with\nprobability 𝑝 of coming up heads. Write an expression for the following:\n1. 𝑃(𝑛 heads on 𝑛 coin flips)\n2. 𝑃(𝑛 tails on 𝑛 coin flips)\n3. 𝑃(first 𝑘 heads, then 𝑛 − 𝑘 tails)\n4. 𝑃(exactly 𝑘 heads on 𝑛 coin flips)\n𝑛\n( %)(\n𝑝 1 − 𝑝\n𝑘\n# of mutually 𝑃(a particular outcome’s\nexclusive 𝑘 heads on 𝑛 coin flips)\noutcomes\nMake sure you understand #4! It will come up again.\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 17\nProbability of events\nE or F E and F\n𝑃 𝐸 ∪ 𝐹 𝑃 𝐸𝐹\nMutually\nexclusive? Independent?\nInclusion-\nJust add! Just multiply! Chain Rule\nExclusion\nPrinciple\n𝑃 𝐸 + 𝑃(𝐹) 𝑃 𝐸 + 𝑃 𝐹 − 𝑃(𝐸 ∩ 𝐹)\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 18\nProbability of events\nE or F E and F\n𝑃 𝐸 ∪ 𝐹 𝑃 𝐸𝐹\nMutually\nexclusive? Independent?\nInclusion-\nJust add! Just multiply! Chain Rule\nExclusion\n𝑃 𝐸 𝑃 𝐹 𝐸\nPrinciple\nor\n𝑃 𝐸 + 𝑃(𝐹) 𝑃 𝐸 + 𝑃 𝐹 − 𝑃(𝐸 ∩ 𝐹) 𝑃 𝐸 𝑃 𝐹\n𝑃 𝐹 𝑃(𝐸|𝐹)\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 19\nProbability of events\nDe Morgan’s\nE or F E and F\n𝑃 𝐸 ∪ 𝐹 𝑃 𝐸𝐹\nMutually\nexclusive? Independent?\nInclusion-\nJust add! Exclusion Just multiply! Chain Rule\nPrinciple\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 20\nDe Morgan’s Laws De Morgan’s lets you switch between AND and OR.\n𝐸 ∩ 𝐹 - = 𝐸- ∪ 𝐹- In probability:\nS\n, - , 𝑃 𝐸 𝐸 ⋯ 𝐸\n# $ %\nE F\n-\n; 𝐸 = < 𝐸\n+ + = 1 − 𝑃 𝐸 𝐸 ⋯ 𝐸 -\n# $ %\n+.( +.(\n- . .\n= 1 − 𝑃 𝐸 ∪ 𝐸 ∪ ⋯ ∪ 𝐸\n# $ %\n-\nGreat if 𝐸 mutually exclusive!\n+\nIn probability:\n- - -\n𝐸 ∪ 𝐹 = 𝐸 ∩ 𝐹\nS\n𝑃 𝐸 ∪ 𝐸 ∪ ⋯ ∪ 𝐸\n, - , # $ %\nE F\n-\n< 𝐸 = ; 𝐸 -\n+ + = 1 − 𝑃 𝐸 ∪ 𝐸 ∪ ⋯ ∪ 𝐸\n# $ %\n+.( +.(\n. . .\n= 1 − 𝑃 𝐸 𝐸 ⋯ 𝐸\n# $ %\nGreat if 𝐸 independent!\n+\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 21\nHash table fun\n• 𝑚 strings are hashed (not uniformly) into a hash table with 𝑛 buckets.\n• Each string hashed is an independent trial w.p. 𝑝 of getting hashed into bucket 𝑖.\n+\nWhat is 𝑃 𝐸 if\n1. 𝐸 = bucket 1 has ≥ 1 string hashed into it?\n2. 𝐸 = at least 1 of buckets 1 to 𝑘 has ≥ 1 string hashed into it?\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 22\nHash table fun\n• 𝑚 strings are hashed (not uniformly) into a hash table with 𝑛 buckets.\n• Each string hashed is an independent trial w.p. 𝑝 of getting hashed into bucket 𝑖.\n+\nWhat is 𝑃 𝐸 if\n1. 𝐸 = bucket 1 has ≥ 1 string hashed into it?\nDefine 𝑆 = string 𝑖 is\n+\nhashed into bucket 1\n-\n𝑆 = string 𝑖 is not\n+\nhashed into bucket 1\n𝑃 𝑆 = 𝑝\n! \"\n#\n𝑃 𝑆 = 1 − 𝑝\n! \"\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 23\nHash table fun\n• 𝑚 strings are hashed (not uniformly) into a hash table with 𝑛 buckets.\n• Each string hashed is an independent trial w.p. 𝑝 of getting hashed into bucket 𝑖.\n+\nWhat is 𝑃 𝐸 if\n1. 𝐸 = bucket 1 has ≥ 1 string hashed into it?\nDefine 𝑆 = string 𝑖 is\n+\nhashed into bucket 1\nWTF (not-real acronym for Want To Find):\n-\n𝑆 = string 𝑖 is not\n+\n𝑃 𝐸 = 𝑃(𝑆 ∪ 𝑆 ∪ ⋯ ∪ 𝑆 ) hashed into bucket 1\n# $ 4\n-\n= 1 − 𝑃 𝑆 ∪ 𝑆 ∪ ⋯ ∪ 𝑆 Complement\n# $ 4\n𝑃 𝑆 = 𝑝\n! \"\n= 1 − 𝑃 𝑆- 𝑆- ⋯ 𝑆- De Morgan’s Law 𝑃 𝑆# = 1 − 𝑝\n# $ 4 ! \"\n4\n- - - -\n= 1 − 𝑃 𝑆 𝑃 𝑆 ⋯ 𝑃 𝑆 = 1 − 𝑃 𝑆 𝑆 independent trials\n# $ 4 # +\n4\n= 1 − (1 − 𝑝 )\n#\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 24\nMore hash table fun: Possible approach?\n• 𝑚 strings are hashed (not uniformly) into a hash table with 𝑛 buckets.\n• Each string hashed is an independent trial w.p. 𝑝 of getting hashed into bucket 𝑖.\n+\nWhat is 𝑃 𝐸 if\n1. 𝐸 = bucket 1 has ≥ 1 string hashed into it?\n2. 𝐸 = at least 1 of buckets 1 to 𝑘 has ≥ 1 string hashed into it?\nDefine 𝐹 = bucket 𝑖 has at\n𝑃 𝐸 = 𝑃 𝐹 ∪ 𝐹 ∪ ⋯ ∪ 𝐹 +\n# $ (\nleast one string in it\n-\n= 1 − 𝑃 𝐹 ∪ 𝐹 ∪ ⋯ ∪ 𝐹\n# $ (\n- - -\n= 1 − 𝑃 𝐹 𝐹 ⋯ 𝐹\n# $ (\n- - .\n? = 1 − 𝑃 𝐹 𝑃 𝐹 ⋯ 𝑃 𝐹\n# $ (\n⚠\n𝐹 bucket events are dependent!\n+\nSo we cannot approach with complement.\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 25\nMore hash table fun\n• 𝑚 strings are hashed (not uniformly) into a hash table with 𝑛 buckets.\n• Each string hashed is an independent trial w.p. 𝑝 of getting hashed into bucket 𝑖.\n+\nWhat is 𝑃 𝐸 if\n1. 𝐸 = bucket 1 has ≥ 1 string hashed into it?\n2. 𝐸 = at least 1 of buckets 1 to 𝑘 has ≥ 1 string hashed into it?\nDefine 𝐹 = bucket 𝑖 has at\n𝑃 𝐸 = 𝑃 𝐹 ∪ 𝐹 ∪ ⋯ ∪ 𝐹 +\n# $ (\nleast one string in it\n-\n= 1 − 𝑃 𝐹 ∪ 𝐹 ∪ ⋯ ∪ 𝐹\n# $ (\n- - -\n= 1 − 𝑃 𝐹 𝐹 ⋯ 𝐹 = 𝑃 buckets 1 to 𝑘 𝐚𝐥𝐥 𝐝𝐞𝐧𝐢𝐞𝐝 𝐬𝐭𝐫𝐢𝐧𝐠𝐬\n# $ (\n$\n= 𝑃 each string hashes to 𝑘 + 1 or higher\n= (1 − 𝑝 − 𝑝 …– 𝑝 )$\n\" % &\n4\n= 1 − (1 − 𝑝 − 𝑝 …– 𝑝 )\n# $ (\nLisa Yan, Chris Piech, Mehran Sahami, and Jerry Cain, CS109, Spring 2024 26 <END>"}
{"prompt":"cs109_lec01_conceptcheck.txt\n\n###\n\n","completion":" Lecture 1: Concept Check  Graded\nStudent\nNeetish Sharma\nTotal Points\n1 \/ 1 pts\nQuestion 1\nHow Concept Checks Work 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQ1 How Concept Checks Work\n1 Point\nThese concept checks are designed to help you verify your understanding of the\nmaterial in each lecture. You should expect a concept check to be posted after\nlecture has ended, and in an ideal world, you'd complete the concept check as\nsoon as possible to confirm you've understood that day's material.\nHowever, we understand that many of you can't always attend lecture and might\neven be taking another class and the same time. Because we know from past\nquarters that many of you often need the weekend to catch up, we'll set the due\ndates for an entire week's worth of concept checks to fall due the Tuesday of the\nfollowing week at noon.\nYou always have an unlimited number of submissions for each concept\ncheck question, and only your most recent submission is retained. Each\ntime you click \"Save Answer\" on a question, an \"Explanation\" box will pop up if\nyou supplied the correct answer. Otherwise the box will not appear. To try it out,\njust keep guessing a number between 16 and 20 and hitting \"Save Answer\" until\nyou get the \"right answer\":\n16\n17\n18\n19\n20 <END>"}
{"prompt":"cs109_lec24_conceptcheck.txt\n\n###\n\n","completion":" Lecture 24: Concept Check  Graded\nStudent\nNeetish Sharma\nTotal Points\n6 \/ 6 pts\nQuestion 1\nGradient Ascent 6 \/ 6 pts\n1.1 Gradient Ascent: Take I 2 \/ 2 pts\n + 2 pts Correct\n+ 0 pts Incorrect\n1.2 Gradient Ascent: Convergence 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\n1.3 Gradient Ascent: Take II 2 \/ 2 pts\n + 2 pts Correct\n+ 0 pts Incorrect\n1.4 Gradient Ascent: Convergence 1 \/ 1 pt\n + 1 pt Correct\n+ 0 pts Incorrect\nQ1 Gradient Ascent\n6 Points\nWhen using gradient ascent in practice, it's important to choose the correct step\nsize η. Consider that we are trying to maximize f(x) = 5 − (x − 1)2 with\nrespect to a single variable x. For this problem please write numerical answers\nto the nearest integer without using decimals. E.g. please answer 1 instead of\n1.0.\nQ1.1 Gradient Ascent: Take I\n2 Points\nConsider that we choose our starting point x = 0 and our step size η = 1, what\n0\nis x ? Restated, what is the value of x after running 2 iterations of gradient\n2\nascent?\n0\nQ1.2 Gradient Ascent: Convergence\n1 Point\nWill gradient ascent converge when we choose our starting point to be x = 0\n0\nwith a step size of η = 1?\nYes\nNo\nQ1.3 Gradient Ascent: Take II\n2 Points\nAssume we choose our starting point to be x = 0 and a learning rate of η =\n0\n0.5. What is x now?\n2\n1\nQ1.4 Gradient Ascent: Convergence\n1 Point\nWill gradient ascent converge when we choose our starting point to be x = 0\n0\nwith a step size of η = 0.5?\nYes\nNo <END>"}
{"prompt":"08_section.txt\n\n###\n\n","completion":" –1–\nCS109 May30,2024\nMaximum A Posteriori and Na¨ıve Bayes Estimation\nBeforeyouleavelab,makesureyouclickheresothatyou’remarkedashavingattendedthis\nweek’ssection.TheCAleadingyourdiscussionsectioncanenterthepasswordneededonce\nyou’vesubmitted.\n1 Warmups\n1.1 Maximum A Posteriori\na. Intuitively,whatisMAP?Whatproblemisittryingtosolve?HowdoesitdifferfromMLE?\nb. Givena6-sideddie(possiblyunfair),yourollthedie 𝑁 timesandobservethecountsfor\neachofthe6outcomesas 𝑛 ,...,𝑛 .Whatisthemaximumaposterioriestimateofthis\n1 6\ndistribution,usingLaplacesmoothing?Recallthatthedierollsthemselvesfollowa\nmultinomialdistribution.\n1.2 Naive Bayes Review\nRecalltheclassificationsetting:wehavedatavectorsoftheform 𝑋 = (𝑋 1,...,𝑋 𝑚) andwewant\ntopredictalabel𝑌 ∈ {0,1}.\na. RecallinNaiveBayes,givenadatapoint𝑥,wecompute 𝑃(𝑌 = 1|𝑋 = 𝑥) andpredict𝑌 = 1\nprovidedthisquantityis ≥ 0.5,andotherwisewepredict𝑌 = 0.Decompose\n𝑃(𝑌 = 1|𝑋 = 𝑥) intosmallerterms,andstatewheretheNaiveBayesassumptionisused.\nb. Supposewearegivenexamplevectorswithlabelsprovided.Giveaformulatoestimate\n(usingmaximumlikelihood)eachquantity 𝑃(𝑋 𝑖 = 𝑥 𝑖|𝑌 = 𝑦) above,for𝑖 ∈ {1,...,𝑚} and\n𝑦 ∈ {0,1}.Youcanassumethereisafunctioncountwhichtakesinanynumberofboolean\nconditionsandreturnsacountoverthedataofthenumberofexamplesinwhichtheyare\ntrue.Forexample,count(𝑋 = 2,𝑋 = 7) returnsthenumberofexampleswhere 𝑋 = 2and\n3 5 3\n𝑋 = 7.\n5\n2 Problems\n2.1 Why Boba Cares About MAP\nYoudon’tunderstandwhythere’snobobaplacewithinwalkingdistancearoundcampus,soyou\ndecidetostartone.Inordertoestimatetheamountofingredientsneededandthetimeyouwill\nspendinthebusiness(youstillneedtostudy),youwanttoestimatehowmanyordersyouwill\nreceiveperhour.AftertakingCS109,youareprettyconfidentthatincomingorderscanbe\nconsideredasindependenteventsandtheprocesscanbemodeledwithaPoisson.\nNowthequestionis-whatisthe𝜆 parameterofthePoisson?Inthefirsthourofyoursoftopening,\nyouarevisitedby4curiousstudents,eachofwhommadeanorder.Youhaveapriorbeliefthat\n𝑓(Λ = 𝜆) = 𝐾 ·𝜆 ·𝑒−𝜆 2.WhatistheMLEestimate?Whatisinferenceof𝜆 giventheobservation?\nWhatistheMaximumaPosteriori(MAP)estimateof𝜆?Throughyourprocesstrytoidentify\nwhatisapoint-estimate,andwhatisadistribution.\n–2–\n2.2 Multiclass Bayes\nInthisproblemwearegoingtoexplorehowtowriteNaiveBayesformultipleoutputclasses.We\nwanttopredictasingleoutputvariableYwhichrepresentshowauserfeelsaboutabook.Unlike\ninyourhomework,theoutputvariableYcantakeononeofthefour valuesintheset\n{Like,Love,Haha,Sad}.Wewillbaseourpredictionsoffofthreebinaryfeaturevariables\n𝑋 1,𝑋 2, and 𝑋 3 whichareindicatorsoftheuser’staste.Allvalues 𝑋 𝑖 ∈ {0,1}.\nWehaveaccesstoadatasetwith10,000users.Eachuserinthedatasethasavaluefor 𝑋 ,𝑋 ,𝑋\n1 2 3\nand𝑌.Youcanuseaspecialquerymethodcountthatreturnsthenumberofusersinthedataset\nwiththegivenequalityconstraints(andonlyequalityconstraints).Herearesomeexampleusages\nof count:\ncount(𝑋 = 1,𝑌 = Haha) returnsthenumberofuserswhere 𝑋 = 1and𝑌 = Haha.\n1 1\ncount(𝑌 = Love) returnsthenumberofuserswhere𝑌 = Love.\ncount(𝑋 = 0,𝑋 = 0) returnsthenumberofuserswhere 𝑋 = 0,and 𝑋 = 0.\n1 3 1 3\nYouaregivenanewuserwith 𝑋 = 1, 𝑋 = 1, 𝑋 = 0.Whatisthebestpredictionforhowtheuser\n1 2 3\nwillfeelaboutthebook(𝑌)?Youmayleaveyouranswerintermsofanargmaxfunction.You\nshouldexplainhowyouwouldcalculateallprobabilitiesusedinyourexpression.UseLaplace\nestimationwhencalculatingprobabilities.\n2.3 Gaussian Na¨ıve Bayes\nTheversionofNa¨ıveBayesthatweusedinclassworkedgreatwhenthefeaturevalueswereall\nbinary.Ifinsteadtheyarecontinuous,wearegoingtohavetorethinkhowweestimateofthe\nprobabilityofthe𝑖thfeaturegiventhelabel, 𝑃(𝑋 𝑖|𝑌).Theubiquitoussolutionistomakethe\nGaussianInputAssumptionthat:\nIf𝑌 = 0,then 𝑋 𝑖 ∼ 𝑁(𝜇 𝑖,0,𝜎 𝑖2 ,0)\nIf𝑌 = 1,then 𝑋 𝑖 ∼ 𝑁(𝜇 𝑖,1,𝜎 𝑖2 ,1)\nForeachfeature,thereare4parameters(meanandvarianceforbothclasslabels).Thereisafinal\nparameter, 𝑝,whichistheestimateof 𝑃(𝑌 = 1).Assumethatyouhavetrainedondatawithtwo\ninputfeaturesandhavealreadyestimatedall9parametervalues,includingthat 𝑝 = 0.6:\nFeature𝑖 𝜇\n𝑖,0\n𝜇\n𝑖,1\n𝜎 𝑖2\n,0\n𝜎 𝑖2\n,1\n1 5 0 1 1\n2 0 3 1 4\nWriteaninequalitytopredictwhether𝑌 = 1forinput [𝑋 = 5,𝑋 = 3].UsetheNa¨ıveBayes\n1 2\nassumptionandtheGaussianInputAssumption.Yourexpressionshouldbeintermsofthe\nlearnedparameters(eitherusingnumbersorsymbolsisfine).\n–3–\n3 Ethics and Beta Distribution\nWhiletherewon’tbeanyethicsmaterialonthefinalexam,we’reincludingaproblemthatwillnot\nonlyexercisesomeprobability,buthopefullyprovokeyoutothinkabouttheimpactthat\nprobability-anddata-drivendecisionshaveonsociety.\nTheEconomistusedabetadistributiontoforecastresultsforthe2020U.S.presidentialelection.1\nFigure1:UpdatedpredictionofDemocraticvoteshareis”Posterior”prediction.\n1. Whyisthebetadistributionappropriateformodelingapresidentialelection?\n2. ReadthepollingreportpublishedbyTheEconomist.Whatshouldbeconsideredwhen\nusingthismodelandreleasingitselectionpredictions?\n1Gelman,A.,&Heidemanns,M.(2020).Howtheeconomistpresidentialforecastworks.TheEconomist. <END>"}
{"prompt":"01_section.txt\n\n###\n\n","completion":" CS109 April11,2024\nSection 1: Combinatorics and Probability\nChrisPiech,MehranSahami,JerryCain,LisaYan,andnumerousCS109CA’s.\nOverview of Section Materials\nThewarm-upquestionsprovidedwillhelpstudentspracticeconceptsintroducedinlectures.Thesectionprob-\nlemsaremeanttoapplytheseconceptsinmorecomplexscenariossimilartowhatyouwillseeinproblemsets\nandexams.Infact,manyofthemareoldexamquestions.\nBeforeyouleavelab,makesureyouclickheresothatyou’remarkedashavingattendedthisweek’ssection.The\nCAleadingyourdiscussionsectioncanenterthepasswordneededonceyou’vesubmitted.\nWarm-ups\n1. Equality versus Inequality\nShowthatforanyevents 𝐴 and 𝐵 that\n𝑃(𝐴) + 𝑃(𝐵) −1 ≤ 𝑃(𝐴∩ 𝐵) ≤ 𝑃(𝐴∪ 𝐵) ≤ 𝑃(𝐴) + 𝑃(𝐵)\nForeachofthethreeinequalities,describesets 𝐴 and 𝐵 thatwouldresultinequality.\n2. Fish Pond\nSupposethereare7bluefish,4redfish,and8greenfishinalargefishingtank.Youdropanetintoitandendup\nwith2fish.Whatistheprobabilityyouget2bluefish?\nProblems\n3. Rolling Fair Dice\nConsideranexperimentwherewerollafair,six-sideddiemultipletimes.\na. Whatistheprobabilitythatatleastone3appearswhenyourollthesamefairdie10times?\nb. Whatistheprobabilitythatatleasttwo3’sappearwhenyourollthesamefairdie20times?Youmay\nleaveyouranswerintermsofoneormorechooseterms.\nc. Whatistheprobabilitythatatleast 𝑛 3’sappearwhenyourollthesamefairdie10𝑛 times?Youranswer\nwillcertainlyinvolveasumofmanycombinatorialterms,andyouneedn’tsimplifyprovidedweunder-\nstandthestructureofyouranswer.\nd. Doyouexpecttheprobabilityfrompartctoincreaseordecreaseas 𝑛 increases?Providesomeintuitionas\ntowhyyouexpecttheincreaseordecrease.\n4. Baking Cookies\nThefollowingproblemisbasedontrueevents.Itwasalsoatake-homeexamquestionseveralyearsago.\nJerryandtheCS109coursestaffarebakingM&McookiesonarainySaturdaymorning,buttheyonlyhave\nenoughflourtomake6cookies.Theyhave15M&M’s,allofwhicharedifferentcolors.Forallsub-parts,as-\nsumethatwedon’tdistinguishbetweendifferentarrangementsofM&M’sonthesamecookie.Thatis,M&M’s\nhavenoorderingonacookie.\na. Howmanywayscanthe15M&M’sbedistributedacrossthesixcookies?Forthissubpart,assumethe\ncookiesthemselvesAREdistinguishable,andtheM&M’sAREdistinguishable.Itispossibleforacookie\ntohavenoM&M’s,anditispossibleforacookietohaveallofthem.\nb. Howmanywayscanthe15M&M’sbedistributedacrossthesixcookies?Forthissubpart,assumethe\ncookiesthemselvesaredistinguishable,andtheM&M’sareindistinguishable.Itispossibleforacookieto\nhavenoM&M’s,anditispossibleforacookietohaveallofthem.\nc. What’stheprobabilitythateachofthesixcookiesendsupwithadifferentnumberofM&M’s?Notethat\nthiswouldrequirethateachofthesixcookiesget0,1,2,3,4,and5M&M’sinsomeorder.Forthissub-\npart,assumethecookiesthemselvesaredistinguishable,andtheM&M’saredistinguishable.Assumefur-\ntherthatanM&Misequallylikelytoappearonanycookie.\nd. IfwenolongerrequireallM&M’sbeused,what’stheprobabilityallcookiesendupwiththesamenum-\nberofM&M’s?Forthissubpart,assumethecookiesthemselvesaredistinguishable,andtheM&M’s\naredistinguishable.AssumefurtherthatanM&Misequallylikelytoappearonanycookie,andthatwe\nshouldincludethepossibilitythatnoneofthecookiesgetM&M’s.Concretely,treat“nocookie”asan-\nothercookiewithequallikelihoodtoothercookies.\n5. The Birthday Problem\nWhensolvingacountingproblem,itcanoftenbeusefultocomeupwithagenerativeprocess,aseriesofsteps\nthat“generates”examples.Acorrectgenerativeprocesstocounttheelementsofset 𝐴 will(1)generateeveryel-\nementof 𝐴 and(2)notgenerateanyelementof 𝐴 morethanonce.Ifourprocesshastheaddedpropertythat(3)\nanygivenstepalwayshasthesamenumberofpossibleoutcomes,thenwecanusetheproductruleofcounting.\nProblem:Assumethatbirthdayshappenonanyofthe365daysoftheyearwithequallikelihood(we’llignore\nleapyears).\na. Whatistheprobabilitythatofthe 𝑛 peopleinclass,atleasttwopeoplesharethesamebirthday?\nb. Whatistheprobabilitythatthisclasscontainsexactlyonepairofpeoplewhoshareabirthday?\n6. Flipping Coins\nOnethingthatstudentsoftenfindtrickywhenlearningcombinatoricsishowtofigureoutwhenaproblemin-\nvolvespermutationsandwhenitinvolvescombinations.Naturally,wewilllookataproblemthatcanbesolved\nwithbothapproaches.Payattentiontowhatpartsofyoursolutionrepresentdistinctobjectsandwhatpartsrep-\nresentindistinctobjects.\nProblem:Weflipafaircoin 𝑛 times,hoping(forsomereason)toget 𝑘 heads.\na. Howmanywaysaretheretogetexactly 𝑘 heads?CharacterizeyouranswerasapermutationofH’sand\nT’s.\nb. Forwhat𝑥 and 𝑦 isyouranswertopart(a)equalto (cid:0)𝑥(cid:1) ?Whydoesthiscombinationmakesenseasanan-\n𝑦\nswer?\nc. Whatistheprobabilitythatwegetexactly 𝑘 heads?\n7. Combinatorial Proofs\nShowthat\n(cid:0)𝑚+𝑛(cid:1)\n=\n(cid:205)𝑘 (cid:0)𝑚(cid:1)(cid:0) 𝑛 (cid:1)\nviaacombinatorialproof.\n𝑘 𝑗=0 𝑗 𝑘−𝑗 <END>"}
